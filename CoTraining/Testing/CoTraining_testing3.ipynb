{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.datasets as datasets\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mfeat_fac = pd.read_csv(\"../../../multi-feature-dataset/mfeat-fac.csv\")\n",
    "mfeat_fou = pd.read_csv(\"../../../multi-feature-dataset/mfeat-fou.csv\")\n",
    "mfeat_kar = pd.read_csv(\"../../../multi-feature-dataset/mfeat-kar.csv\")\n",
    "mfeat_mor = pd.read_csv(\"../../../multi-feature-dataset/mfeat-mor.csv\")\n",
    "mfeat_pix = pd.read_csv(\"../../../multi-feature-dataset/mfeat-pix.csv\")\n",
    "mfeat_zer = pd.read_csv(\"../../../multi-feature-dataset/mfeat-zer.csv\")\n",
    "labels_pd = mfeat_zer.label\n",
    "labels_pd.drop(np.arange(400,labels_pd.shape[0]), inplace=True)\n",
    "dataset_names = ['mfeat_fac','mfeat_fou','mfeat_kar','mfeat_mor','mfeat_pix','mfeat_zer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop all other than 0's and 1's\n",
    "for dataset in [mfeat_fac, mfeat_fou, mfeat_kar, mfeat_mor, mfeat_pix, mfeat_zer]:\n",
    "    dataset.drop(np.arange(400,dataset.shape[0]), inplace=True)\n",
    "    dataset.drop(['label'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>98</td>\n",
       "      <td>236</td>\n",
       "      <td>531</td>\n",
       "      <td>673</td>\n",
       "      <td>607</td>\n",
       "      <td>647</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>474</td>\n",
       "      <td>536</td>\n",
       "      <td>628</td>\n",
       "      <td>632</td>\n",
       "      <td>18</td>\n",
       "      <td>36</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>121</td>\n",
       "      <td>193</td>\n",
       "      <td>607</td>\n",
       "      <td>611</td>\n",
       "      <td>585</td>\n",
       "      <td>665</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>520</td>\n",
       "      <td>458</td>\n",
       "      <td>570</td>\n",
       "      <td>634</td>\n",
       "      <td>15</td>\n",
       "      <td>32</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>115</td>\n",
       "      <td>141</td>\n",
       "      <td>590</td>\n",
       "      <td>605</td>\n",
       "      <td>557</td>\n",
       "      <td>627</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>535</td>\n",
       "      <td>498</td>\n",
       "      <td>572</td>\n",
       "      <td>656</td>\n",
       "      <td>20</td>\n",
       "      <td>35</td>\n",
       "      <td>16</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90</td>\n",
       "      <td>122</td>\n",
       "      <td>627</td>\n",
       "      <td>692</td>\n",
       "      <td>607</td>\n",
       "      <td>642</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>576</td>\n",
       "      <td>549</td>\n",
       "      <td>628</td>\n",
       "      <td>621</td>\n",
       "      <td>16</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>157</td>\n",
       "      <td>167</td>\n",
       "      <td>681</td>\n",
       "      <td>666</td>\n",
       "      <td>587</td>\n",
       "      <td>666</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>594</td>\n",
       "      <td>525</td>\n",
       "      <td>568</td>\n",
       "      <td>653</td>\n",
       "      <td>16</td>\n",
       "      <td>35</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 216 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5   6  7  8  9 ...   206  207  208  209  210  \\\n",
       "0   98  236  531  673  607  647   2  9  3  6 ...   474  536  628  632   18   \n",
       "1  121  193  607  611  585  665   7  9  2  4 ...   520  458  570  634   15   \n",
       "2  115  141  590  605  557  627  12  6  3  3 ...   535  498  572  656   20   \n",
       "3   90  122  627  692  607  642   0  6  4  5 ...   576  549  628  621   16   \n",
       "4  157  167  681  666  587  666   8  6  1  4 ...   594  525  568  653   16   \n",
       "\n",
       "   211  212  213  214  215  \n",
       "0   36    8   15   12   13  \n",
       "1   32   11   13   15   11  \n",
       "2   35   16   14   13    6  \n",
       "3   35    7   12   15    9  \n",
       "4   35   10   15   13   13  \n",
       "\n",
       "[5 rows x 216 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfeat_fac.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataArrays = []\n",
    "for dataset in [mfeat_fac, mfeat_fou, mfeat_kar, mfeat_mor, mfeat_pix, mfeat_zer]:\n",
    "    dataArrays.append(dataset.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "[[0 3 4 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 1 ..., 2 1 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 4 3 0]\n",
      " [0 0 0 ..., 4 1 0]]\n"
     ]
    }
   ],
   "source": [
    "labels = labels_pd.values\n",
    "print(labels)\n",
    "print(dataArrays[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 216)\n",
      "(100, 216)\n",
      "[1 1 1 0 0 1 0 0 1 1 0 0 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 0 0 0 0 1 1 1 0\n",
      " 1 1 0 0 1 1 0 0 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1\n",
      " 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 1 1 0 0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "trainData = []\n",
    "testData = []\n",
    "for dataset in dataArrays:\n",
    "    temp_train, temp_test, y_train, y_test = train_test_split(dataset, labels, test_size = 0.25, random_state = 12)\n",
    "    trainData.append(temp_train)\n",
    "    testData.append(temp_test)\n",
    "    \n",
    "print(trainData[0].shape)\n",
    "print(testData[0].shape)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test of Sufficiency\n",
      "mfeat_fac accuracy: 1.0\n",
      "mfeat_fou accuracy: 0.99\n",
      "mfeat_kar accuracy: 1.0\n",
      "mfeat_mor accuracy: 0.99\n",
      "mfeat_pix accuracy: 0.99\n",
      "mfeat_zer accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "# Make sure each view is sufficient\n",
    "print(\"Test of Sufficiency\")\n",
    "for i in range(len(trainData)): \n",
    "    hh = GaussianNB()\n",
    "    hh.fit(trainData[i], y_train)\n",
    "    y_pred = hh.predict(testData[i])\n",
    "    print(dataset_names[i] + \" accuracy: \" + str(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]]\n"
     ]
    }
   ],
   "source": [
    "# set certain labels to -1\n",
    "# seed 8 Lsize 6\n",
    "# for last figure, seed 5, Lsize 6, test_size=0.25, random_state=12\n",
    "random.seed(5)\n",
    "Lsize = 6\n",
    "labels_train = y_train.copy()\n",
    "minus1 = np.arange(0,len(labels_train))\n",
    "random.shuffle(minus1)\n",
    "minus1 = minus1[:-Lsize]\n",
    "labels_train = labels_train.astype(float)\n",
    "labels_train[minus1] = np.nan\n",
    "\n",
    "print(labels_train[np.argwhere(~np.isnan(labels_train))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  4.64940000e-02   1.25060000e+00   1.44860000e+01   6.18350000e+01\n",
      "    9.47000000e+01   3.86450000e+02   2.84000000e+02   2.25340000e-02\n",
      "    6.39980000e-01   5.37530000e+00   2.32260000e+01   4.59380000e+01\n",
      "    9.20320000e+01   1.45020000e-01   3.99200000e+00   3.25960000e+01\n",
      "    5.57780000e+01   1.47380000e+02   4.15590000e+02   7.47940000e-02\n",
      "    1.47700000e+00   1.03710000e+01   2.20540000e+01   5.88240000e+01\n",
      "    5.36810000e-01   1.11190000e+01   5.74220000e+01   1.12560000e+02\n",
      "    3.36150000e+02   2.02680000e-01   3.33490000e+00   1.37960000e+01\n",
      "    2.59140000e+01   1.83340000e+00   3.00000000e+01   2.13910000e+02\n",
      "    6.17240000e+00   5.55310000e-01   7.28990000e+00   2.16730000e+01\n",
      "    6.74840000e+00   1.51040000e+02   3.97650000e+02   1.80490000e+00\n",
      "    1.85210000e+01   4.43930000e+01   5.16640000e+02]\n",
      " [  1.96440000e-01   7.47890000e-01   3.02210000e+01   9.84300000e+01\n",
      "    7.06790000e+01   1.73450000e+02   7.15460000e+01   2.48860000e-01\n",
      "    5.98300000e+00   1.04970000e+01   1.91880000e+02   1.83230000e+02\n",
      "    7.12260000e+01   9.52430000e-02   9.64360000e+00   6.67440000e+01\n",
      "    4.87890000e+01   9.36430000e+01   9.09630000e+01   8.34320000e-01\n",
      "    3.04690000e+00   1.30700000e+02   2.97310000e+02   7.38460000e+01\n",
      "    1.43630000e+00   2.61930000e+01   2.26840000e+01   1.94900000e+02\n",
      "    1.27800000e+02   4.42360000e-01   5.33220000e+01   2.77260000e+02\n",
      "    8.93710000e+01   4.72470000e+00   7.14770000e+00   2.55900000e+02\n",
      "    3.17710000e+02   1.01830000e+01   1.50720000e+02   1.40120000e+02\n",
      "    1.30240000e+00   1.73760000e+02   5.77510000e+02   3.74070000e+01\n",
      "    1.22840000e+02   5.12070000e+01   6.19140000e+02]\n",
      " [  8.88690000e-02   2.16910000e+00   2.17160000e+01   7.41520000e+01\n",
      "    2.14520000e+01   2.20260000e+02   1.21620000e+02   1.19940000e-01\n",
      "    2.56920000e+00   2.56470000e+01   1.64760000e+02   2.00030000e+02\n",
      "    1.04390000e+02   2.66570000e-01   6.35010000e+00   3.96050000e+01\n",
      "    2.44800000e+01   1.77140000e+02   1.19480000e+02   3.38750000e-01\n",
      "    8.39920000e+00   1.02110000e+02   2.64760000e+02   1.50270000e+02\n",
      "    8.91630000e-01   1.34890000e+01   2.14880000e+01   2.14870000e+02\n",
      "    2.13640000e+01   1.30390000e+00   3.92220000e+01   2.17220000e+02\n",
      "    1.80560000e+02   2.22440000e+00   1.09580000e+01   2.22960000e+02\n",
      "    2.24320000e+02   7.19410000e+00   1.08570000e+02   1.73820000e+02\n",
      "    2.51020000e+00   1.38720000e+02   4.82950000e+02   2.55390000e+01\n",
      "    1.16480000e+02   3.91940000e+01   5.16880000e+02]\n",
      " [  1.14390000e-01   2.56180000e+00   3.28650000e+01   8.25320000e+01\n",
      "    1.31860000e+01   2.29300000e+02   3.15210000e+01   2.80230000e-01\n",
      "    3.94270000e+00   2.68570000e+01   1.66570000e+02   2.76900000e+02\n",
      "    1.06890000e+02   3.25780000e-01   1.01550000e+01   5.20800000e+01\n",
      "    1.12950000e+01   8.74510000e+01   2.97480000e+01   5.52130000e-01\n",
      "    8.83310000e+00   1.12640000e+02   3.28990000e+02   8.33490000e+01\n",
      "    1.48130000e+00   1.97240000e+01   1.86150000e+01   8.78780000e+01\n",
      "    2.05930000e+01   1.39190000e+00   4.57170000e+01   2.65730000e+02\n",
      "    1.38200000e+02   3.48720000e+00   1.16160000e+01   1.69550000e+02\n",
      "    1.54210000e+02   8.69960000e+00   1.33920000e+02   1.76800000e+02\n",
      "    2.93110000e+00   1.24240000e+02   3.95360000e+02   3.18860000e+01\n",
      "    1.34870000e+02   3.75530000e+01   4.56640000e+02]\n",
      " [  3.55180000e-02   6.29230000e-01   1.87120000e+01   7.36980000e+01\n",
      "    6.88180000e+01   4.29500000e+02   1.94850000e+02   1.23220000e-01\n",
      "    3.60040000e+00   2.27240000e+01   2.62790000e+01   7.97300000e+01\n",
      "    1.76090000e+02   7.22050000e-02   5.24080000e+00   3.79470000e+01\n",
      "    6.75470000e+00   1.90280000e+02   3.96940000e+02   4.47100000e-01\n",
      "    6.53440000e+00   1.77240000e+01   1.99350000e+01   1.52080000e+02\n",
      "    7.12060000e-01   1.25740000e+01   1.41890000e+01   9.23090000e+01\n",
      "    3.99910000e+02   9.27440000e-01   8.59840000e+00   5.68570000e+01\n",
      "    7.86510000e+01   2.03050000e+00   9.58680000e+00   2.18490000e+02\n",
      "    8.46700000e+01   1.81460000e+00   3.83000000e+01   9.56410000e+00\n",
      "    2.30450000e+00   1.60760000e+02   3.43860000e+02   1.02310000e+01\n",
      "    2.34360000e+01   4.80440000e+01   5.10960000e+02]\n",
      " [  1.52190000e-02   6.10290000e-01   8.37530000e+00   3.98950000e+01\n",
      "    2.85280000e+01   3.41840000e+02   2.78600000e+02   2.23290000e-02\n",
      "    6.17260000e-01   6.24500000e+00   3.57890000e+01   4.17090000e+01\n",
      "    1.20680000e+02   6.89390000e-02   2.27070000e+00   2.24130000e+01\n",
      "    8.34520000e+01   1.01950000e+02   3.84950000e+02   8.23700000e-02\n",
      "    2.28270000e+00   2.78730000e+01   1.00110000e+02   3.48390000e+01\n",
      "    3.01200000e-01   7.78970000e+00   7.37220000e+01   1.39650000e+02\n",
      "    2.86070000e+02   3.72010000e-01   1.16730000e+01   9.22790000e+01\n",
      "    7.27870000e+01   1.29010000e+00   3.47530000e+01   2.26210000e+02\n",
      "    5.67560000e+01   2.22290000e+00   4.76540000e+01   1.17380000e+02\n",
      "    7.41700000e+00   1.54140000e+02   4.32390000e+02   1.13060000e+01\n",
      "    9.27490000e+01   4.46730000e+01   5.32530000e+02]]\n",
      "[[ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(trainData[i][np.argwhere(~np.isnan(labels_train)),:].squeeze())\n",
    "print(labels_train[np.argwhere(~np.isnan(labels_train))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mfeat_fac semi-supervised accuracy: 0.58\n",
      "mfeat_fou semi-supervised accuracy: 0.73\n",
      "mfeat_kar semi-supervised accuracy: 0.69\n",
      "mfeat_mor semi-supervised accuracy: 0.99\n",
      "mfeat_pix semi-supervised accuracy: 0.99\n",
      "mfeat_zer semi-supervised accuracy: 0.77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gavin\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# Test Accuracy in Semi-supervised setting\n",
    "for i in range(len(trainData)):\n",
    "    hh = GaussianNB()\n",
    "    hh.fit(trainData[i][np.argwhere(~np.isnan(labels_train)),:].squeeze(), labels_train[np.argwhere(~np.isnan(labels_train))])\n",
    "    y_pred = hh.predict(testData[i])\n",
    "    print(dataset_names[i] + \" semi-supervised accuracy: \" + str(accuracy_score(y_test, y_pred)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of set\n",
      "2\n",
      "1\n",
      "1\n",
      "U is\n",
      "L is\n",
      "6\n",
      "agree: 42\n",
      "disagree: 58\n",
      "agree: 139\n",
      "disagree: 161\n",
      "10\n",
      "agree: 64\n",
      "disagree: 36\n",
      "agree: 210\n",
      "disagree: 90\n",
      "14\n",
      "agree: 84\n",
      "disagree: 16\n",
      "agree: 270\n",
      "disagree: 30\n",
      "18\n",
      "agree: 85\n",
      "disagree: 15\n",
      "agree: 276\n",
      "disagree: 24\n",
      "22\n",
      "agree: 84\n",
      "disagree: 16\n",
      "agree: 272\n",
      "disagree: 28\n",
      "26\n",
      "agree: 84\n",
      "disagree: 16\n",
      "agree: 269\n",
      "disagree: 31\n",
      "30\n",
      "agree: 84\n",
      "disagree: 16\n",
      "agree: 270\n",
      "disagree: 30\n",
      "34\n",
      "agree: 85\n",
      "disagree: 15\n",
      "agree: 271\n",
      "disagree: 29\n",
      "38\n",
      "agree: 87\n",
      "disagree: 13\n",
      "agree: 278\n",
      "disagree: 22\n",
      "42\n",
      "agree: 87\n",
      "disagree: 13\n",
      "agree: 275\n",
      "disagree: 25\n",
      "46\n",
      "agree: 86\n",
      "disagree: 14\n",
      "agree: 276\n",
      "disagree: 24\n",
      "50\n",
      "agree: 87\n",
      "disagree: 13\n",
      "agree: 274\n",
      "disagree: 26\n",
      "54\n",
      "agree: 87\n",
      "disagree: 13\n",
      "agree: 275\n",
      "disagree: 25\n",
      "58\n",
      "agree: 86\n",
      "disagree: 14\n",
      "agree: 272\n",
      "disagree: 28\n",
      "62\n",
      "agree: 86\n",
      "disagree: 14\n",
      "agree: 272\n",
      "disagree: 28\n",
      "66\n",
      "agree: 87\n",
      "disagree: 13\n",
      "agree: 275\n",
      "disagree: 25\n",
      "70\n",
      "agree: 95\n",
      "disagree: 5\n",
      "agree: 282\n",
      "disagree: 18\n",
      "74\n",
      "agree: 95\n",
      "disagree: 5\n",
      "agree: 287\n",
      "disagree: 13\n",
      "78\n",
      "agree: 98\n",
      "disagree: 2\n",
      "agree: 287\n",
      "disagree: 13\n",
      "82\n",
      "agree: 98\n",
      "disagree: 2\n",
      "agree: 288\n",
      "disagree: 12\n",
      "86\n",
      "agree: 98\n",
      "disagree: 2\n",
      "agree: 286\n",
      "disagree: 14\n",
      "90\n",
      "agree: 98\n",
      "disagree: 2\n",
      "agree: 289\n",
      "disagree: 11\n",
      "94\n",
      "agree: 98\n",
      "disagree: 2\n",
      "agree: 289\n",
      "disagree: 11\n",
      "98\n",
      "agree: 98\n",
      "disagree: 2\n",
      "agree: 288\n",
      "disagree: 12\n",
      "102\n",
      "agree: 98\n",
      "disagree: 2\n",
      "agree: 288\n",
      "disagree: 12\n",
      "106\n",
      "agree: 98\n",
      "disagree: 2\n",
      "agree: 288\n",
      "disagree: 12\n",
      "110\n",
      "agree: 98\n",
      "disagree: 2\n",
      "agree: 288\n",
      "disagree: 12\n",
      "114\n",
      "agree: 99\n",
      "disagree: 1\n",
      "agree: 289\n",
      "disagree: 11\n",
      "118\n",
      "agree: 99\n",
      "disagree: 1\n",
      "agree: 289\n",
      "disagree: 11\n",
      "122\n",
      "agree: 99\n",
      "disagree: 1\n",
      "agree: 289\n",
      "disagree: 11\n",
      "126\n",
      "agree: 99\n",
      "disagree: 1\n",
      "agree: 289\n",
      "disagree: 11\n",
      "130\n",
      "agree: 99\n",
      "disagree: 1\n",
      "agree: 289\n",
      "disagree: 11\n",
      "134\n",
      "agree: 99\n",
      "disagree: 1\n",
      "agree: 289\n",
      "disagree: 11\n",
      "138\n",
      "agree: 99\n",
      "disagree: 1\n",
      "agree: 290\n",
      "disagree: 10\n",
      "142\n",
      "agree: 99\n",
      "disagree: 1\n",
      "agree: 290\n",
      "disagree: 10\n",
      "146\n",
      "agree: 99\n",
      "disagree: 1\n",
      "agree: 290\n",
      "disagree: 10\n",
      "150\n",
      "agree: 99\n",
      "disagree: 1\n",
      "agree: 290\n",
      "disagree: 10\n",
      "154\n",
      "agree: 99\n",
      "disagree: 1\n",
      "agree: 290\n",
      "disagree: 10\n",
      "158\n",
      "agree: 99\n",
      "disagree: 1\n",
      "agree: 290\n",
      "disagree: 10\n",
      "162\n",
      "agree: 99\n",
      "disagree: 1\n",
      "agree: 290\n",
      "disagree: 10\n",
      "166\n",
      "agree: 99\n",
      "disagree: 1\n",
      "agree: 291\n",
      "disagree: 9\n",
      "170\n",
      "agree: 99\n",
      "disagree: 1\n",
      "agree: 290\n",
      "disagree: 10\n",
      "174\n",
      "agree: 99\n",
      "disagree: 1\n",
      "agree: 289\n",
      "disagree: 11\n",
      "178\n",
      "agree: 99\n",
      "disagree: 1\n",
      "agree: 290\n",
      "disagree: 10\n",
      "182\n",
      "agree: 99\n",
      "disagree: 1\n",
      "agree: 290\n",
      "disagree: 10\n",
      "186\n",
      "agree: 99\n",
      "disagree: 1\n",
      "agree: 290\n",
      "disagree: 10\n",
      "190\n",
      "agree: 99\n",
      "disagree: 1\n",
      "agree: 290\n",
      "disagree: 10\n",
      "194\n",
      "agree: 99\n",
      "disagree: 1\n",
      "agree: 291\n",
      "disagree: 9\n",
      "198\n",
      "agree: 99\n",
      "disagree: 1\n",
      "agree: 291\n",
      "disagree: 9\n",
      "202\n",
      "agree: 99\n",
      "disagree: 1\n",
      "agree: 292\n",
      "disagree: 8\n",
      "206\n",
      "agree: 99\n",
      "disagree: 1\n",
      "agree: 291\n",
      "disagree: 9\n",
      "210\n",
      "agree: 99\n",
      "disagree: 1\n",
      "agree: 292\n",
      "disagree: 8\n",
      "214\n",
      "agree: 99\n",
      "disagree: 1\n",
      "agree: 292\n",
      "disagree: 8\n",
      "218\n",
      "agree: 99\n",
      "disagree: 1\n",
      "agree: 292\n",
      "disagree: 8\n",
      "222\n",
      "agree: 99\n",
      "disagree: 1\n",
      "agree: 293\n",
      "disagree: 7\n"
     ]
    }
   ],
   "source": [
    "view1 = 1\n",
    "view2 = 2\n",
    "gnb1 = GaussianNB()\n",
    "gnb2 = GaussianNB()\n",
    "clf = CoTrainingClassifier(gnb1, gnb2, n=1, p=1, k=100, u=75)\n",
    "errors_train, errors = clf.fit(trainData[view1], trainData[view2], labels_train, y_train, testData[view1], testData[view2], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYyrc/wecnO0QWutF6QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x25fac818e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datetime\n",
    "time = str(datetime.datetime.now())\n",
    "time = time.replace(\":\",\"_\")\n",
    "time = time.replace(\".\",\"_\")\n",
    "\n",
    "fig, ax = plt.subplots(1,2, sharey=True)\n",
    "ax[0].plot(100*np.array(errors_train))\n",
    "ax[0].set_title(\"Error on Training Data\")\n",
    "ax[1].plot(100*np.array(errors))\n",
    "ax[1].set_title(\"Error on Testing Data\")\n",
    "ax[0].set_xlabel(\"Co-training iterations\")\n",
    "ax[0].set_ylabel(\"Classification Error (%)\")\n",
    "ax[1].set_xlabel(\"Co-training iterations\")\n",
    "ax[1].set_ylabel(\"Classification Error (%)\")\n",
    "plt.savefig(\"Multiview data error \" + dataset_names[view1] + \"_and_\" + dataset_names[view2] + time + \".png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Update documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#import copy\n",
    "\n",
    "class CoTrainingClassifier(object):\n",
    "    \"\"\"\n",
    "    Co-Training Classifier\n",
    "    \n",
    "    This class implements the co-training classifier similar to as described in [1]\n",
    "    This is meant to be used on 2 views of the input data which satisfy the 2 conditions\n",
    "    \n",
    "    Organization from https://github.com/jjrob13/sklearn_cotraining\n",
    "    Algorithm based on \"Combining Labeled and Unlabeled Data with Co-Training\", Blum and Mitchell, 1998 \n",
    "    \n",
    "    Parameters:\n",
    "    clf - The classifier that will be used in the cotraining algorithm on the view 1 feature set\n",
    "        (If clf2 is not specified, then the same type of classifier will be used on the second view).\n",
    "\n",
    "    clf2 - (Optional) A different classifier type can be specified to be used on the X2 feature set\n",
    "         if desired.\n",
    "\n",
    "    p - (Optional) The number of positive examples that will be 'labeled' by each classifier during each iteration\n",
    "        The default is the is determined by the smallest integer ratio of positive to negative samples in L (from paper)\n",
    "\n",
    "    n - (Optional) The number of negative examples that will be 'labeled' by each classifier during each iteration\n",
    "    The default is the is determined by the smallest integer ratio of positive to negative samples in L (from paper)\n",
    "\n",
    "    k - (Optional) The number of iterations\n",
    "        The default is 30 (from paper)\n",
    "\n",
    "    u - (Optional) The size of the pool of unlabeled samples from which the classifier can choose\n",
    "    Default - 75 (from paper)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, clf, clf2, p=-1, n=-1, k=30, u=75):\n",
    "        \n",
    "        self.clf1_ = clf\n",
    "\n",
    "#         if clf2 == None:\n",
    "#             self.clf2_ = copy.copy(clf)\n",
    "#         else:\n",
    "        self.clf2_ = clf2\n",
    "\n",
    "        #if user only specifies one of n or p, raise an exception\n",
    "        if (p == -1 and n != -1) or (p != -1 and n == -1):\n",
    "            raise ValueError('Must supply either both p and n, or neither')\n",
    "\n",
    "        self.p_ = p\n",
    "        self.n_ = n\n",
    "        self.k_ = k\n",
    "        self.u_ = u\n",
    "\n",
    "        random.seed(10)\n",
    "        \n",
    "        # for testing\n",
    "        self.partial_error_ = []\n",
    "        # for testing with training data\n",
    "        self.partial_train_error_ = []\n",
    "\n",
    "\n",
    "    def fit(self, X1, X2, y, y_train_full=None, X1_test=None, X2_test=None, y_test=None):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        fits the classifiers on the partially labeled data, y.\n",
    "\n",
    "        Parameters:\n",
    "        X1 - array-like (n_samples, n_features_1): first set of features for samples\n",
    "        X2 - array-like (n_samples, n_features_2): second set of features for samples\n",
    "        y - array-like (n_samples): labels for samples, -1 indicates unlabeled\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # convert to numpy array\n",
    "        y = np.asarray(y)\n",
    "        print(\"length of set\")\n",
    "        print(len(set(y[~np.isnan(y)])))\n",
    "\n",
    "        #set the n and p parameters if we need to\n",
    "        if self.p_ == -1 and self.n_ == -1:\n",
    "            num_pos = sum(1 for y_i in y if y_i == 1)\n",
    "            num_neg = sum(1 for y_i in y if y_i == 0)\n",
    "\n",
    "            n_p_ratio = num_neg / float(num_pos)\n",
    "\n",
    "            if n_p_ratio > 1:\n",
    "                self.p_ = 1\n",
    "                self.n_ = round(self.p_*n_p_ratio)\n",
    "\n",
    "            else:\n",
    "                self.n_ = 1\n",
    "                self.p_ = round(self.n_/n_p_ratio)\n",
    "        print(self.n_)\n",
    "        print(self.p_)\n",
    "\n",
    "        assert(self.p_ > 0 and self.n_ > 0 and self.k_ > 0 and self.u_ > 0)\n",
    "\n",
    "        #the set of unlabeled samples\n",
    "        U = [i for i, y_i in enumerate(y) if np.isnan(y_i)]\n",
    "        print(\"U is\")\n",
    "\n",
    "        #we randomize here, and then just take from the back so we don't have to sample every time\n",
    "        np.random.seed(10)\n",
    "        np.random.shuffle(U)\n",
    "        \n",
    "        #this is U' in paper\n",
    "        U_ = U[-min(len(U), self.u_):]\n",
    "\n",
    "        #the samples that are initially labeled\n",
    "        L = [i for i, y_i in enumerate(y) if ~np.isnan(y_i)]\n",
    "        print(\"L is\")\n",
    "\n",
    "        #remove the samples in U_ from U\n",
    "        U = U[:-len(U_)]\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        it = 0 #number of cotraining iterations we've done so far\n",
    "\n",
    "        #loop until we have assigned labels to everything in U or we hit our iteration break condition\n",
    "        while it != self.k_ and len(U) > 0:\n",
    "            it += 1\n",
    "\n",
    "            \n",
    "            self.clf1_.fit(X1[L], y[L])\n",
    "            self.clf2_.fit(X2[L], y[L])\n",
    "            print(len(L))\n",
    "            ###y_test_new = y_test[U_]\n",
    "\n",
    "            y1_prob = self.clf1_.predict_log_proba(X1[U_])\n",
    "            y2_prob = self.clf2_.predict_log_proba(X2[U_])\n",
    "            \n",
    "            \n",
    "#             print(y1_prob)\n",
    "#             print(y2_prob)\n",
    "            \n",
    "            n, p = [], []\n",
    "            accurate_guesses_h1 = 0\n",
    "            accurate_guesses_h2 = 0\n",
    "            wrong_guesses_h1 = 0\n",
    "            wrong_guesses_h2 = 0\n",
    "            \n",
    "            \n",
    "            #print([np.sort(y1_prob)[:5]])\n",
    "            for i in (y1_prob[:,0].argsort())[-self.n_:]:\n",
    "                if y1_prob[i,0] > np.log(0.5):\n",
    "                    n.append(i)\n",
    "#                     if y_test_new[i] == 0:\n",
    "#                         accurate_guesses_h1 += 1\n",
    "#                         print(\"h1 correct class 0\")\n",
    "#                     else:\n",
    "#                         wrong_guesses_h1 += 1\n",
    "#                         print(\"h1 guessed 0 actually \" + str(y_test_new[i]))\n",
    "                 \n",
    "            #print([(np.sort(y1_prob))[-5:]])\n",
    "            for i in (y1_prob[:,1].argsort())[-self.p_:]:\n",
    "                if y1_prob[i,1] > np.log(0.5):\n",
    "                    p.append(i)\n",
    "#                     if y_test_new[i] == 1:\n",
    "#                         accurate_guesses_h1 += 1\n",
    "#                         print(\"h1 correct class 1\")\n",
    "#                     else:\n",
    "#                         wrong_guesses_h1 += 1\n",
    "#                         print(\"h1 guessed 1 actually \" + str(y_test_new[i]))\n",
    "\n",
    "            #print([(np.sort(y2_prob))[:5]])\n",
    "            for i in (y2_prob[:,0].argsort())[-self.n_:]:\n",
    "                if y2_prob[i,0] > np.log(0.5):\n",
    "                    n.append(i)\n",
    "#                     if y_test_new[i] == 0:\n",
    "#                         accurate_guesses_h2 += 1\n",
    "#                         print(\"h2 correct class 0\")\n",
    "#                     else:\n",
    "#                         wrong_guesses_h2 += 1\n",
    "#                         print(\"h2 guessed 0 actually \" + str(y_test_new[i]))\n",
    "                    \n",
    "            #print([(np.sort(y2_prob))[-5:]])\n",
    "            for i in (y2_prob[:,1].argsort())[-self.p_:]:\n",
    "                if y2_prob[i,1] > np.log(0.5):\n",
    "                    p.append(i)\n",
    "#                     if y_test_new[i] == 1:\n",
    "#                         accurate_guesses_h2 += 1\n",
    "#                         print(\"h2 correct class 1\")\n",
    "#                     else:\n",
    "#                         wrong_guesses_h2 += 1\n",
    "#                         print(\"h2 guessed 1 actually \" + str(y_test_new[i]))\n",
    "\n",
    "\n",
    "                        \n",
    "#             print(\"accurate guesses h1 \" + str(accurate_guesses_h1))\n",
    "#             print(\"wrong guesses h1\" + str(wrong_guesses_h1))\n",
    "#             print(\"accurate guesses h2 \" + str(accurate_guesses_h2))\n",
    "#             print(\"wrong guesses h2\" + str(wrong_guesses_h2))\n",
    "            \n",
    "\n",
    "            #label the samples and remove the newly added samples from U_\n",
    "            y[[U_[x] for x in p]] = 1\n",
    "            y[[U_[x] for x in n]] = 0\n",
    "\n",
    "            L.extend([U_[x] for x in p])\n",
    "            L.extend([U_[x] for x in n])\n",
    "\n",
    "            U_ = [elem for elem in U_ if not (elem in p or elem in n)]\n",
    "\n",
    "            #add new elements to U_\n",
    "            add_counter = 0 #number we have added from U to U_\n",
    "            num_to_add = len(p) + len(n)\n",
    "            while add_counter != num_to_add and U:\n",
    "                add_counter += 1\n",
    "                U_.append(U.pop())\n",
    "                \n",
    "            \n",
    "            # if input testing data as well, find the incrememtal update on accuracy\n",
    "            if X1_test is not None and X2_test is not None and y_test is not None:\n",
    "                y_pred = self.predict(X1_test, X2_test)\n",
    "                self.partial_error_.append(1-accuracy_score(y_test, y_pred))\n",
    "                y_pred = self.predict(X1, X2)\n",
    "                self.partial_train_error_.append(1-accuracy_score(y_train_full, y_pred))\n",
    "\n",
    "\n",
    "            #TODO: Handle the case where the classifiers fail to agree on any of the samples (i.e. both n and p are empty)\n",
    "\n",
    "\n",
    "        #fit the final model\n",
    "        self.clf1_.fit(X1[L], y[L])\n",
    "        self.clf2_.fit(X2[L], y[L])\n",
    "        \n",
    "        return (self.partial_train_error_, self.partial_error_)\n",
    "\n",
    "\n",
    "    #TODO: Move this outside of the class into a util file.\n",
    "    def supports_proba(self, clf, x):\n",
    "        \"\"\"Checks if a given classifier supports the 'predict_proba' method, given a single vector x\"\"\"\n",
    "        try:\n",
    "            clf.predict_proba([x])\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def predict(self, X1, X2):\n",
    "        \"\"\"\n",
    "        Predict the classes of the samples represented by the features in X1 and X2.\n",
    "\n",
    "        Parameters:\n",
    "        X1 - array-like (n_samples, n_features1)\n",
    "        X2 - array-like (n_samples, n_features2)\n",
    "\n",
    "        \n",
    "        Output:\n",
    "        y - array-like (n_samples)\n",
    "            These are the predicted classes of each of the samples.  If the two classifiers, don't agree, we try\n",
    "            to use predict_proba and take the classifier with the highest confidence and if predict_proba is not implemented, then we randomly\n",
    "            assign either 0 or 1.  We hope to improve this in future releases.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        y1 = self.clf1_.predict(X1)\n",
    "        y2 = self.clf2_.predict(X2)\n",
    "\n",
    "        proba_supported = self.supports_proba(self.clf1_, X1[0]) and self.supports_proba(self.clf2_, X2[0])\n",
    "\n",
    "        #fill y_pred with -1 so we can identify the samples in which the classifiers failed to agree\n",
    "        y_pred = np.asarray([-1] * X1.shape[0])\n",
    "        num_disagree = 0\n",
    "        num_agree = 0\n",
    "\n",
    "        for i, (y1_i, y2_i) in enumerate(zip(y1, y2)):\n",
    "            if y1_i == y2_i:\n",
    "                y_pred[i] = y1_i\n",
    "                num_agree += 1\n",
    "            elif proba_supported:\n",
    "                y1_probs = self.clf1_.predict_proba([X1[i]])[0]\n",
    "                y2_probs = self.clf2_.predict_proba([X2[i]])[0]\n",
    "                sum_y_probs = [prob1 + prob2 for (prob1, prob2) in zip(y1_probs, y2_probs)]\n",
    "                max_sum_prob = max(sum_y_probs)\n",
    "                y_pred[i] = sum_y_probs.index(max_sum_prob)\n",
    "                num_disagree += 1\n",
    "            else:\n",
    "                #the classifiers disagree and don't support probability, so we guess\n",
    "                y_pred[i] = random.randint(0, 1)\n",
    "                \n",
    "        print(\"agree: \" + str(num_agree))\n",
    "        print(\"disagree: \" + str(num_disagree))\n",
    "\n",
    "\n",
    "        #check that we did everything right\n",
    "        assert not (-1 in y_pred)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "    def predict_proba(self, X1, X2):\n",
    "        \"\"\"Predict the probability of the samples belonging to each class.\"\"\"\n",
    "        y_proba = np.full((X1.shape[0], 2), -1)\n",
    "\n",
    "        y1_proba = self.clf1_.predict_proba(X1)\n",
    "        y2_proba = self.clf2_.predict_proba(X2)\n",
    "\n",
    "        for i, (y1_i_dist, y2_i_dist) in enumerate(zip(y1_proba, y2_proba)):\n",
    "            y_proba[i][0] = (y1_i_dist[0] + y2_i_dist[0]) / 2\n",
    "            y_proba[i][1] = (y1_i_dist[1] + y2_i_dist[1]) / 2\n",
    "\n",
    "        _epsilon = 0.0001\n",
    "        assert all(abs(sum(y_dist) - 1) <= _epsilon for y_dist in y_proba)\n",
    "        return y_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "class CoTrainingClassifier(object):\n",
    "    \"\"\"\n",
    "    Organization from https://github.com/jjrob13/sklearn_cotraining\n",
    "    Algorithm based on \"Combining Labeled and Unlabeled Data with Co-Training\", Blum and Mitchell, 1998 \n",
    "    \n",
    "    Parameters:\n",
    "    clf - The classifier that will be used in the cotraining algorithm on the view 1 feature set\n",
    "        (If clf2 is not specified, then the same type of classifier will be used on the second view).\n",
    "\n",
    "    clf2 - (Optional) A different classifier type can be specified to be used on the X2 feature set\n",
    "         if desired.\n",
    "\n",
    "    p - (Optional) The number of positive examples that will be 'labeled' by each classifier during each iteration\n",
    "        The default is the is determined by the smallest integer ratio of positive to negative samples in L (from paper)\n",
    "\n",
    "    n - (Optional) The number of negative examples that will be 'labeled' by each classifier during each iteration\n",
    "    The default is the is determined by the smallest integer ratio of positive to negative samples in L (from paper)\n",
    "\n",
    "    k - (Optional) The number of iterations\n",
    "        The default is 30 (from paper)\n",
    "\n",
    "    u - (Optional) The size of the pool of unlabeled samples from which the classifier can choose\n",
    "    Default - 75 (from paper)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, clf, clf2=None, p=-1, n=-1, k=30, u=75):\n",
    "        \n",
    "        self.clf1_ = clf\n",
    "\n",
    "        if clf2 == None:\n",
    "            self.clf2_ = copy.copy(clf)\n",
    "        else:\n",
    "            self.clf2_ = clf2\n",
    "\n",
    "        #if user only specifies one of n or p, raise an exception\n",
    "        if (p == -1 and n != -1) or (p != -1 and n == -1):\n",
    "            raise ValueError('Must supply either both p and n, or neither')\n",
    "\n",
    "        self.p_ = p\n",
    "        self.n_ = n\n",
    "        self.k_ = k\n",
    "        self.u_ = u\n",
    "\n",
    "        random.seed(10)\n",
    "        \n",
    "        # for testing\n",
    "        self.partial_error_ = []\n",
    "        # for testing with training data\n",
    "        self.partial_train_error_ = []\n",
    "\n",
    "\n",
    "    def fit(self, X1, X2, y, y_train_full=None, X1_test=None, X2_test=None, y_test=None):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        fits the classifiers on the partially labeled data, y.\n",
    "\n",
    "        Parameters:\n",
    "        X1 - array-like (n_samples, n_features_1): first set of features for samples\n",
    "        X2 - array-like (n_samples, n_features_2): second set of features for samples\n",
    "        y - array-like (n_samples): labels for samples, -1 indicates unlabeled\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # convert to numpy array\n",
    "        y = np.asarray(y)\n",
    "\n",
    "        #set the n and p parameters if we need to\n",
    "        if self.p_ == -1 and self.n_ == -1:\n",
    "            num_pos = sum(1 for y_i in y if y_i == 1)\n",
    "            num_neg = sum(1 for y_i in y if y_i == 0)\n",
    "\n",
    "            n_p_ratio = num_neg / float(num_pos)\n",
    "\n",
    "            if n_p_ratio > 1:\n",
    "                self.p_ = 1\n",
    "                self.n_ = round(self.p_*n_p_ratio)\n",
    "\n",
    "            else:\n",
    "                self.n_ = 1\n",
    "                self.p_ = round(self.n_/n_p_ratio)\n",
    "        print(self.n_)\n",
    "        print(self.p_)\n",
    "\n",
    "        assert(self.p_ > 0 and self.n_ > 0 and self.k_ > 0 and self.u_ > 0)\n",
    "\n",
    "        #the set of unlabeled samples\n",
    "        U = [i for i, y_i in enumerate(y) if y_i == -1]\n",
    "\n",
    "        #we randomize here, and then just take from the back so we don't have to sample every time\n",
    "        np.random.seed(10)\n",
    "        np.random.shuffle(U)\n",
    "        \n",
    "        #this is U' in paper\n",
    "        U_ = U[-min(len(U), self.u_):]\n",
    "\n",
    "        #the samples that are initially labeled\n",
    "        L = [i for i, y_i in enumerate(y) if y_i != -1]\n",
    "\n",
    "        #remove the samples in U_ from U\n",
    "        U = U[:-len(U_)]\n",
    "\n",
    "\n",
    "        it = 0 #number of cotraining iterations we've done so far\n",
    "\n",
    "        #loop until we have assigned labels to everything in U or we hit our iteration break condition\n",
    "        while it != self.k_ and U:\n",
    "            it += 1\n",
    "\n",
    "            \n",
    "            self.clf1_.fit(X1[L], y[L])\n",
    "            self.clf2_.fit(X2[L], y[L])\n",
    "            print(len(L))\n",
    "            ###y_test_new = y_test[U_]\n",
    "\n",
    "            y1_prob = self.clf1_.predict_log_proba(X1[U_])\n",
    "            y2_prob = self.clf2_.predict_log_proba(X2[U_])\n",
    "            \n",
    "            \n",
    "#             print(y1_prob)\n",
    "#             print(y2_prob)\n",
    "            \n",
    "            n, p = [], []\n",
    "            accurate_guesses_h1 = 0\n",
    "            accurate_guesses_h2 = 0\n",
    "            wrong_guesses_h1 = 0\n",
    "            wrong_guesses_h2 = 0\n",
    "            \n",
    "            \n",
    "            #print([np.sort(y1_prob)[:5]])\n",
    "            for i in (y1_prob[:,0].argsort())[-self.n_:]:\n",
    "                #if y1_prob[i,0] > 0.5:\n",
    "                    n.append(i)\n",
    "#                     if y_test_new[i] == 0:\n",
    "#                         accurate_guesses_h1 += 1\n",
    "#                         print(\"h1 correct class 0\")\n",
    "#                     else:\n",
    "#                         wrong_guesses_h1 += 1\n",
    "#                         print(\"h1 guessed 0 actually \" + str(y_test_new[i]))\n",
    "                 \n",
    "            #print([(np.sort(y1_prob))[-5:]])\n",
    "            for i in (y1_prob[:,1].argsort())[-self.p_:]:\n",
    "                #if y1_prob[i,1] > 0.5:\n",
    "                    p.append(i)\n",
    "#                     if y_test_new[i] == 1:\n",
    "#                         accurate_guesses_h1 += 1\n",
    "#                         print(\"h1 correct class 1\")\n",
    "#                     else:\n",
    "#                         wrong_guesses_h1 += 1\n",
    "#                         print(\"h1 guessed 1 actually \" + str(y_test_new[i]))\n",
    "\n",
    "            #print([(np.sort(y2_prob))[:5]])\n",
    "            for i in (y2_prob[:,0].argsort())[-self.n_:]:\n",
    "                #if y2_prob[i,0] > 0.5:\n",
    "                    n.append(i)\n",
    "#                     if y_test_new[i] == 0:\n",
    "#                         accurate_guesses_h2 += 1\n",
    "#                         print(\"h2 correct class 0\")\n",
    "#                     else:\n",
    "#                         wrong_guesses_h2 += 1\n",
    "#                         print(\"h2 guessed 0 actually \" + str(y_test_new[i]))\n",
    "                    \n",
    "            #print([(np.sort(y2_prob))[-5:]])\n",
    "            for i in (y2_prob[:,1].argsort())[-self.p_:]:\n",
    "                #if y2_prob[i,1] > 0.5:\n",
    "                    p.append(i)\n",
    "#                     if y_test_new[i] == 1:\n",
    "#                         accurate_guesses_h2 += 1\n",
    "#                         print(\"h2 correct class 1\")\n",
    "#                     else:\n",
    "#                         wrong_guesses_h2 += 1\n",
    "#                         print(\"h2 guessed 1 actually \" + str(y_test_new[i]))\n",
    "\n",
    "\n",
    "                        \n",
    "#             print(\"accurate guesses h1 \" + str(accurate_guesses_h1))\n",
    "#             print(\"wrong guesses h1\" + str(wrong_guesses_h1))\n",
    "#             print(\"accurate guesses h2 \" + str(accurate_guesses_h2))\n",
    "#             print(\"wrong guesses h2\" + str(wrong_guesses_h2))\n",
    "            \n",
    "\n",
    "            #label the samples and remove the newly added samples from U_\n",
    "            y[[U_[x] for x in p]] = 1\n",
    "            y[[U_[x] for x in n]] = 0\n",
    "\n",
    "            L.extend([U_[x] for x in p])\n",
    "            L.extend([U_[x] for x in n])\n",
    "\n",
    "            U_ = [elem for elem in U_ if not (elem in p or elem in n)]\n",
    "\n",
    "            #add new elements to U_\n",
    "            add_counter = 0 #number we have added from U to U_\n",
    "            num_to_add = len(p) + len(n)\n",
    "            while add_counter != num_to_add and U:\n",
    "                add_counter += 1\n",
    "                U_.append(U.pop())\n",
    "                \n",
    "            \n",
    "            # if input testing data as well, find the incrememtal update on accuracy\n",
    "            if X1_test is not None and X2_test is not None and y_test is not None:\n",
    "                y_pred = self.predict(X1_test, X2_test)\n",
    "                self.partial_error_.append(1-accuracy_score(y_test, y_pred))\n",
    "                y_pred = self.predict(X1, X2)\n",
    "                self.partial_train_error_.append(1-accuracy_score(y_train_full, y_pred))\n",
    "\n",
    "\n",
    "            #TODO: Handle the case where the classifiers fail to agree on any of the samples (i.e. both n and p are empty)\n",
    "\n",
    "\n",
    "        #fit the final model\n",
    "        self.clf1_.fit(X1[L], y[L])\n",
    "        self.clf2_.fit(X2[L], y[L])\n",
    "        \n",
    "        return (self.partial_train_error_, self.partial_error_)\n",
    "\n",
    "\n",
    "    #TODO: Move this outside of the class into a util file.\n",
    "    def supports_proba(self, clf, x):\n",
    "        \"\"\"Checks if a given classifier supports the 'predict_proba' method, given a single vector x\"\"\"\n",
    "        try:\n",
    "            clf.predict_proba([x])\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def predict(self, X1, X2):\n",
    "        \"\"\"\n",
    "        Predict the classes of the samples represented by the features in X1 and X2.\n",
    "\n",
    "        Parameters:\n",
    "        X1 - array-like (n_samples, n_features1)\n",
    "        X2 - array-like (n_samples, n_features2)\n",
    "\n",
    "        \n",
    "        Output:\n",
    "        y - array-like (n_samples)\n",
    "            These are the predicted classes of each of the samples.  If the two classifiers, don't agree, we try\n",
    "            to use predict_proba and take the classifier with the highest confidence and if predict_proba is not implemented, then we randomly\n",
    "            assign either 0 or 1.  We hope to improve this in future releases.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        y1 = self.clf1_.predict(X1)\n",
    "        y2 = self.clf2_.predict(X2)\n",
    "\n",
    "        proba_supported = self.supports_proba(self.clf1_, X1[0]) and self.supports_proba(self.clf2_, X2[0])\n",
    "\n",
    "        #fill y_pred with -1 so we can identify the samples in which the classifiers failed to agree\n",
    "        y_pred = np.asarray([-1] * X1.shape[0])\n",
    "        num_disagree = 0\n",
    "        num_agree = 0\n",
    "\n",
    "        for i, (y1_i, y2_i) in enumerate(zip(y1, y2)):\n",
    "            if y1_i == y2_i:\n",
    "                y_pred[i] = y1_i\n",
    "                num_agree += 1\n",
    "            elif proba_supported:\n",
    "                y1_probs = self.clf1_.predict_proba([X1[i]])[0]\n",
    "                y2_probs = self.clf2_.predict_proba([X2[i]])[0]\n",
    "                sum_y_probs = [prob1 + prob2 for (prob1, prob2) in zip(y1_probs, y2_probs)]\n",
    "                max_sum_prob = max(sum_y_probs)\n",
    "                y_pred[i] = sum_y_probs.index(max_sum_prob)\n",
    "                num_disagree += 1\n",
    "            else:\n",
    "                #the classifiers disagree and don't support probability, so we guess\n",
    "                y_pred[i] = random.randint(0, 1)\n",
    "                \n",
    "        print(\"agree: \" + str(num_agree))\n",
    "        print(\"disagree: \" + str(num_disagree))\n",
    "\n",
    "\n",
    "        #check that we did everything right\n",
    "        assert not (-1 in y_pred)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "    def predict_proba(self, X1, X2):\n",
    "        \"\"\"Predict the probability of the samples belonging to each class.\"\"\"\n",
    "        y_proba = np.full((X1.shape[0], 2), -1)\n",
    "\n",
    "        y1_proba = self.clf1_.predict_proba(X1)\n",
    "        y2_proba = self.clf2_.predict_proba(X2)\n",
    "\n",
    "        for i, (y1_i_dist, y2_i_dist) in enumerate(zip(y1_proba, y2_proba)):\n",
    "            y_proba[i][0] = (y1_i_dist[0] + y2_i_dist[0]) / 2\n",
    "            y_proba[i][1] = (y1_i_dist[1] + y2_i_dist[1]) / 2\n",
    "\n",
    "        _epsilon = 0.0001\n",
    "        assert all(abs(sum(y_dist) - 1) <= _epsilon for y_dist in y_proba)\n",
    "        return y_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
