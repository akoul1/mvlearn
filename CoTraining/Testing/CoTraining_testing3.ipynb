{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.datasets as datasets\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mfeat_fac = pd.read_csv(\"../../../multi-feature-dataset/mfeat-fac.csv\")\n",
    "mfeat_fou = pd.read_csv(\"../../../multi-feature-dataset/mfeat-fou.csv\")\n",
    "mfeat_kar = pd.read_csv(\"../../../multi-feature-dataset/mfeat-kar.csv\")\n",
    "mfeat_mor = pd.read_csv(\"../../../multi-feature-dataset/mfeat-mor.csv\")\n",
    "mfeat_pix = pd.read_csv(\"../../../multi-feature-dataset/mfeat-pix.csv\")\n",
    "mfeat_zer = pd.read_csv(\"../../../multi-feature-dataset/mfeat-zer.csv\")\n",
    "labels_pd = mfeat_zer.label\n",
    "labels_pd.drop(np.arange(400,labels_pd.shape[0]), inplace=True)\n",
    "dataset_names = ['mfeat_fac','mfeat_fou','mfeat_kar','mfeat_mor','mfeat_pix','mfeat_zer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop all other than 0's and 1's\n",
    "for dataset in [mfeat_fac, mfeat_fou, mfeat_kar, mfeat_mor, mfeat_pix, mfeat_zer]:\n",
    "    dataset.drop(np.arange(400,dataset.shape[0]), inplace=True)\n",
    "    dataset.drop(['label'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>98</td>\n",
       "      <td>236</td>\n",
       "      <td>531</td>\n",
       "      <td>673</td>\n",
       "      <td>607</td>\n",
       "      <td>647</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>474</td>\n",
       "      <td>536</td>\n",
       "      <td>628</td>\n",
       "      <td>632</td>\n",
       "      <td>18</td>\n",
       "      <td>36</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>121</td>\n",
       "      <td>193</td>\n",
       "      <td>607</td>\n",
       "      <td>611</td>\n",
       "      <td>585</td>\n",
       "      <td>665</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>520</td>\n",
       "      <td>458</td>\n",
       "      <td>570</td>\n",
       "      <td>634</td>\n",
       "      <td>15</td>\n",
       "      <td>32</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>115</td>\n",
       "      <td>141</td>\n",
       "      <td>590</td>\n",
       "      <td>605</td>\n",
       "      <td>557</td>\n",
       "      <td>627</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>535</td>\n",
       "      <td>498</td>\n",
       "      <td>572</td>\n",
       "      <td>656</td>\n",
       "      <td>20</td>\n",
       "      <td>35</td>\n",
       "      <td>16</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90</td>\n",
       "      <td>122</td>\n",
       "      <td>627</td>\n",
       "      <td>692</td>\n",
       "      <td>607</td>\n",
       "      <td>642</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>576</td>\n",
       "      <td>549</td>\n",
       "      <td>628</td>\n",
       "      <td>621</td>\n",
       "      <td>16</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>157</td>\n",
       "      <td>167</td>\n",
       "      <td>681</td>\n",
       "      <td>666</td>\n",
       "      <td>587</td>\n",
       "      <td>666</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>594</td>\n",
       "      <td>525</td>\n",
       "      <td>568</td>\n",
       "      <td>653</td>\n",
       "      <td>16</td>\n",
       "      <td>35</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 216 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5   6  7  8  9 ...   206  207  208  209  210  \\\n",
       "0   98  236  531  673  607  647   2  9  3  6 ...   474  536  628  632   18   \n",
       "1  121  193  607  611  585  665   7  9  2  4 ...   520  458  570  634   15   \n",
       "2  115  141  590  605  557  627  12  6  3  3 ...   535  498  572  656   20   \n",
       "3   90  122  627  692  607  642   0  6  4  5 ...   576  549  628  621   16   \n",
       "4  157  167  681  666  587  666   8  6  1  4 ...   594  525  568  653   16   \n",
       "\n",
       "   211  212  213  214  215  \n",
       "0   36    8   15   12   13  \n",
       "1   32   11   13   15   11  \n",
       "2   35   16   14   13    6  \n",
       "3   35    7   12   15    9  \n",
       "4   35   10   15   13   13  \n",
       "\n",
       "[5 rows x 216 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfeat_fac.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataArrays = []\n",
    "for dataset in [mfeat_fac, mfeat_fou, mfeat_kar, mfeat_mor, mfeat_pix, mfeat_zer]:\n",
    "    dataArrays.append(dataset.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "[[0 3 4 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 1 ..., 2 1 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 4 3 0]\n",
      " [0 0 0 ..., 4 1 0]]\n"
     ]
    }
   ],
   "source": [
    "labels = labels_pd.values\n",
    "print(labels)\n",
    "print(dataArrays[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 216)\n",
      "(100, 216)\n",
      "[1 1 1 0 0 1 0 0 1 1 0 0 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 0 0 0 0 1 1 1 0\n",
      " 1 1 0 0 1 1 0 0 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1\n",
      " 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 1 1 0 0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "trainData = []\n",
    "testData = []\n",
    "for dataset in dataArrays:\n",
    "    temp_train, temp_test, y_train, y_test = train_test_split(dataset, labels, test_size = 0.25, random_state = 12)\n",
    "    trainData.append(temp_train)\n",
    "    testData.append(temp_test)\n",
    "    \n",
    "print(trainData[0].shape)\n",
    "print(testData[0].shape)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test of Sufficiency\n",
      "mfeat_fac accuracy: 1.0\n",
      "mfeat_fou accuracy: 0.99\n",
      "mfeat_kar accuracy: 1.0\n",
      "mfeat_mor accuracy: 0.99\n",
      "mfeat_pix accuracy: 0.99\n",
      "mfeat_zer accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "# Make sure each view is sufficient\n",
    "print(\"Test of Sufficiency\")\n",
    "for i in range(len(trainData)): \n",
    "    hh = GaussianNB()\n",
    "    hh.fit(trainData[i], y_train)\n",
    "    y_pred = hh.predict(testData[i])\n",
    "    print(dataset_names[i] + \" accuracy: \" + str(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]]\n"
     ]
    }
   ],
   "source": [
    "# set certain labels to -1\n",
    "# seed 8 Lsize 6\n",
    "# for last figure, seed 5, Lsize 6, test_size=0.25, random_state=12\n",
    "random.seed(5)\n",
    "Lsize = 6\n",
    "labels_train = y_train.copy()\n",
    "minus1 = np.arange(0,len(labels_train))\n",
    "random.shuffle(minus1)\n",
    "minus1 = minus1[:-Lsize]\n",
    "labels_train = labels_train.astype(float)\n",
    "labels_train[minus1] = np.nan\n",
    "\n",
    "print(labels_train[np.argwhere(~np.isnan(labels_train))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 143  149  570 ...,   14   15    9]\n",
      " [ 196  476  924 ...,    9   15   13]\n",
      " [ 264  334  981 ...,    8   16   14]\n",
      " [ 304  434 1093 ...,   14   14   14]\n",
      " [ 166  168  524 ...,   14   12   10]\n",
      " [ 139  209  600 ...,   14   17   10]]\n",
      "[[ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(trainData[i][np.argwhere(~np.isnan(labels_train)),:].squeeze())\n",
    "print(labels_train[np.argwhere(~np.isnan(labels_train))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mfeat_fac semi-supervised accuracy: 0.58\n",
      "mfeat_fou semi-supervised accuracy: 0.73\n",
      "mfeat_kar semi-supervised accuracy: 0.69\n",
      "mfeat_mor semi-supervised accuracy: 0.99\n",
      "mfeat_pix semi-supervised accuracy: 0.99\n",
      "mfeat_zer semi-supervised accuracy: 0.77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gavin\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# Test Accuracy in Semi-supervised setting\n",
    "for i in range(len(trainData)):\n",
    "    hh = GaussianNB()\n",
    "    hh.fit(trainData[i][np.argwhere(~np.isnan(labels_train)),:].squeeze(), labels_train[np.argwhere(~np.isnan(labels_train))])\n",
    "    y_pred = hh.predict(testData[i])\n",
    "    print(dataset_names[i] + \" semi-supervised accuracy: \" + str(accuracy_score(y_test, y_pred)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "6\n",
      "agree: 54\n",
      "disagree: 46\n",
      "agree: 159\n",
      "disagree: 141\n",
      "10\n",
      "agree: 78\n",
      "disagree: 22\n",
      "agree: 237\n",
      "disagree: 63\n",
      "14\n",
      "agree: 90\n",
      "disagree: 10\n",
      "agree: 276\n",
      "disagree: 24\n",
      "18\n",
      "agree: 94\n",
      "disagree: 6\n",
      "agree: 286\n",
      "disagree: 14\n",
      "22\n",
      "agree: 95\n",
      "disagree: 5\n",
      "agree: 284\n",
      "disagree: 16\n",
      "26\n",
      "agree: 95\n",
      "disagree: 5\n",
      "agree: 284\n",
      "disagree: 16\n",
      "30\n",
      "agree: 96\n",
      "disagree: 4\n",
      "agree: 287\n",
      "disagree: 13\n",
      "34\n",
      "agree: 93\n",
      "disagree: 7\n",
      "agree: 287\n",
      "disagree: 13\n",
      "38\n",
      "agree: 93\n",
      "disagree: 7\n",
      "agree: 286\n",
      "disagree: 14\n",
      "42\n",
      "agree: 93\n",
      "disagree: 7\n",
      "agree: 285\n",
      "disagree: 15\n",
      "46\n",
      "agree: 94\n",
      "disagree: 6\n",
      "agree: 290\n",
      "disagree: 10\n",
      "50\n",
      "agree: 97\n",
      "disagree: 3\n",
      "agree: 290\n",
      "disagree: 10\n",
      "54\n",
      "agree: 97\n",
      "disagree: 3\n",
      "agree: 289\n",
      "disagree: 11\n",
      "58\n",
      "agree: 96\n",
      "disagree: 4\n",
      "agree: 287\n",
      "disagree: 13\n",
      "62\n",
      "agree: 94\n",
      "disagree: 6\n",
      "agree: 287\n",
      "disagree: 13\n",
      "66\n",
      "agree: 96\n",
      "disagree: 4\n",
      "agree: 288\n",
      "disagree: 12\n",
      "70\n",
      "agree: 95\n",
      "disagree: 5\n",
      "agree: 288\n",
      "disagree: 12\n",
      "74\n",
      "agree: 97\n",
      "disagree: 3\n",
      "agree: 286\n",
      "disagree: 14\n",
      "78\n",
      "agree: 96\n",
      "disagree: 4\n",
      "agree: 290\n",
      "disagree: 10\n",
      "82\n",
      "agree: 98\n",
      "disagree: 2\n",
      "agree: 293\n",
      "disagree: 7\n",
      "86\n",
      "agree: 98\n",
      "disagree: 2\n",
      "agree: 291\n",
      "disagree: 9\n",
      "90\n",
      "agree: 99\n",
      "disagree: 1\n",
      "agree: 293\n",
      "disagree: 7\n",
      "94\n",
      "agree: 99\n",
      "disagree: 1\n",
      "agree: 293\n",
      "disagree: 7\n",
      "98\n",
      "agree: 99\n",
      "disagree: 1\n",
      "agree: 292\n",
      "disagree: 8\n",
      "102\n",
      "agree: 98\n",
      "disagree: 2\n",
      "agree: 292\n",
      "disagree: 8\n",
      "106\n",
      "agree: 98\n",
      "disagree: 2\n",
      "agree: 291\n",
      "disagree: 9\n",
      "110\n",
      "agree: 98\n",
      "disagree: 2\n",
      "agree: 290\n",
      "disagree: 10\n",
      "114\n",
      "agree: 98\n",
      "disagree: 2\n",
      "agree: 290\n",
      "disagree: 10\n",
      "118\n",
      "agree: 98\n",
      "disagree: 2\n",
      "agree: 289\n",
      "disagree: 11\n",
      "122\n",
      "agree: 98\n",
      "disagree: 2\n",
      "agree: 290\n",
      "disagree: 10\n",
      "126\n",
      "agree: 98\n",
      "disagree: 2\n",
      "agree: 288\n",
      "disagree: 12\n",
      "130\n",
      "agree: 98\n",
      "disagree: 2\n",
      "agree: 289\n",
      "disagree: 11\n",
      "134\n",
      "agree: 98\n",
      "disagree: 2\n",
      "agree: 288\n",
      "disagree: 12\n",
      "138\n",
      "agree: 98\n",
      "disagree: 2\n",
      "agree: 289\n",
      "disagree: 11\n",
      "142\n",
      "agree: 98\n",
      "disagree: 2\n",
      "agree: 288\n",
      "disagree: 12\n",
      "146\n",
      "agree: 98\n",
      "disagree: 2\n",
      "agree: 289\n",
      "disagree: 11\n",
      "150\n",
      "agree: 98\n",
      "disagree: 2\n",
      "agree: 289\n",
      "disagree: 11\n",
      "154\n",
      "agree: 98\n",
      "disagree: 2\n",
      "agree: 289\n",
      "disagree: 11\n",
      "158\n",
      "agree: 98\n",
      "disagree: 2\n",
      "agree: 290\n",
      "disagree: 10\n",
      "162\n",
      "agree: 98\n",
      "disagree: 2\n",
      "agree: 290\n",
      "disagree: 10\n",
      "166\n",
      "agree: 98\n",
      "disagree: 2\n",
      "agree: 289\n",
      "disagree: 11\n",
      "170\n",
      "agree: 98\n",
      "disagree: 2\n",
      "agree: 290\n",
      "disagree: 10\n",
      "174\n",
      "agree: 98\n",
      "disagree: 2\n",
      "agree: 290\n",
      "disagree: 10\n",
      "178\n",
      "agree: 98\n",
      "disagree: 2\n",
      "agree: 290\n",
      "disagree: 10\n",
      "182\n",
      "agree: 98\n",
      "disagree: 2\n",
      "agree: 289\n",
      "disagree: 11\n",
      "186\n",
      "agree: 98\n",
      "disagree: 2\n",
      "agree: 290\n",
      "disagree: 10\n",
      "190\n",
      "agree: 98\n",
      "disagree: 2\n",
      "agree: 290\n",
      "disagree: 10\n",
      "194\n",
      "agree: 98\n",
      "disagree: 2\n",
      "agree: 290\n",
      "disagree: 10\n",
      "198\n",
      "agree: 98\n",
      "disagree: 2\n",
      "agree: 290\n",
      "disagree: 10\n",
      "202\n",
      "agree: 98\n",
      "disagree: 2\n",
      "agree: 290\n",
      "disagree: 10\n",
      "206\n",
      "agree: 98\n",
      "disagree: 2\n",
      "agree: 290\n",
      "disagree: 10\n",
      "210\n",
      "agree: 98\n",
      "disagree: 2\n",
      "agree: 290\n",
      "disagree: 10\n",
      "214\n",
      "agree: 98\n",
      "disagree: 2\n",
      "agree: 289\n",
      "disagree: 11\n",
      "218\n",
      "agree: 98\n",
      "disagree: 2\n",
      "agree: 289\n",
      "disagree: 11\n",
      "222\n",
      "agree: 98\n",
      "disagree: 2\n",
      "agree: 291\n",
      "disagree: 9\n"
     ]
    }
   ],
   "source": [
    "view1 = 1\n",
    "view2 = 5\n",
    "gnb1 = GaussianNB()\n",
    "gnb2 = GaussianNB()\n",
    "clf = CoTrainingClassifier(gnb1, gnb2, n=1, p=1, k=100, u=75)\n",
    "errors_train, errors = clf.fit(trainData[view1], trainData[view2], labels_train, y_train, testData[view1], testData[view2], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlDlLBMYYU+YsERhjTJmzRGCMMWXu/wMHZjO+rZfj5AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d749f07a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datetime\n",
    "time = str(datetime.datetime.now())\n",
    "time = time.replace(\":\",\"_\")\n",
    "time = time.replace(\".\",\"_\")\n",
    "\n",
    "fig, ax = plt.subplots(1,2, sharey=True)\n",
    "ax[0].plot(100*np.array(errors_train))\n",
    "ax[0].set_title(\"Error on Training Data\")\n",
    "ax[1].plot(100*np.array(errors))\n",
    "ax[1].set_title(\"Error on Testing Data\")\n",
    "ax[0].set_xlabel(\"Co-training iterations\")\n",
    "ax[0].set_ylabel(\"Classification Error (%)\")\n",
    "ax[1].set_xlabel(\"Co-training iterations\")\n",
    "ax[1].set_ylabel(\"Classification Error (%)\")\n",
    "plt.savefig(\"Multiview data error \" + dataset_names[view1] + \"_and_\" + dataset_names[view2] + time + \".png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Update documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#import copy\n",
    "\n",
    "class CoTrainingClassifier(object):\n",
    "    \"\"\"\n",
    "    Co-Training Classifier\n",
    "    \n",
    "    This class implements the co-training classifier similar to as described in [1]\n",
    "    This is meant to be used on 2 views of the input data which satisfy the 2 conditions\n",
    "    \n",
    "    Organization from https://github.com/jjrob13/sklearn_cotraining\n",
    "    Algorithm based on \"Combining Labeled and Unlabeled Data with Co-Training\", Blum and Mitchell, 1998 \n",
    "    \n",
    "    Parameters:\n",
    "    clf - The classifier that will be used in the cotraining algorithm on the view 1 feature set\n",
    "        (If clf2 is not specified, then the same type of classifier will be used on the second view).\n",
    "\n",
    "    clf2 - (Optional) A different classifier type can be specified to be used on the X2 feature set\n",
    "         if desired.\n",
    "\n",
    "    p - (Optional) The number of positive examples that will be 'labeled' by each classifier during each iteration\n",
    "        The default is the is determined by the smallest integer ratio of positive to negative samples in L (from paper)\n",
    "\n",
    "    n - (Optional) The number of negative examples that will be 'labeled' by each classifier during each iteration\n",
    "    The default is the is determined by the smallest integer ratio of positive to negative samples in L (from paper)\n",
    "\n",
    "    k - (Optional) The number of iterations\n",
    "        The default is 30 (from paper)\n",
    "\n",
    "    u - (Optional) The size of the pool of unlabeled samples from which the classifier can choose\n",
    "    Default - 75 (from paper)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, clf, clf2, p=-1, n=-1, k=30, u=75):\n",
    "        \n",
    "        self.clf1_ = clf\n",
    "\n",
    "#         if clf2 == None:\n",
    "#             self.clf2_ = copy.copy(clf)\n",
    "#         else:\n",
    "        self.clf2_ = clf2\n",
    "\n",
    "        #if user only specifies one of n or p, raise an exception\n",
    "        if (p == -1 and n != -1) or (p != -1 and n == -1):\n",
    "            raise ValueError('Must supply either both p and n, or neither')\n",
    "\n",
    "        self.p_ = p\n",
    "        self.n_ = n\n",
    "        self.k_ = k\n",
    "        self.u_ = u\n",
    "\n",
    "        random.seed(10)\n",
    "        \n",
    "        # for testing\n",
    "        self.partial_error_ = []\n",
    "        # for testing with training data\n",
    "        self.partial_train_error_ = []\n",
    "\n",
    "\n",
    "    def fit(self, X1, X2, y, y_train_full=None, X1_test=None, X2_test=None, y_test=None):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        fits the classifiers on the partially labeled data, y.\n",
    "\n",
    "        Parameters:\n",
    "        X1 - array-like (n_samples, n_features_1): first set of features for samples\n",
    "        X2 - array-like (n_samples, n_features_2): second set of features for samples\n",
    "        y - array-like (n_samples): labels for samples, -1 indicates unlabeled\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # convert to numpy array\n",
    "        y = np.asarray(y)\n",
    "\n",
    "        #set the n and p parameters if we need to\n",
    "        if self.p_ == -1 and self.n_ == -1:\n",
    "            num_pos = sum(1 for y_i in y if y_i == 1)\n",
    "            num_neg = sum(1 for y_i in y if y_i == 0)\n",
    "\n",
    "            n_p_ratio = num_neg / float(num_pos)\n",
    "\n",
    "            if n_p_ratio > 1:\n",
    "                self.p_ = 1\n",
    "                self.n_ = round(self.p_*n_p_ratio)\n",
    "\n",
    "            else:\n",
    "                self.n_ = 1\n",
    "                self.p_ = round(self.n_/n_p_ratio)\n",
    "        print(self.n_)\n",
    "        print(self.p_)\n",
    "\n",
    "        assert(self.p_ > 0 and self.n_ > 0 and self.k_ > 0 and self.u_ > 0)\n",
    "\n",
    "        #the set of unlabeled samples\n",
    "        U = [i for i, y_i in enumerate(y) if np.isnan(y_i)]\n",
    "\n",
    "        #we randomize here, and then just take from the back so we don't have to sample every time\n",
    "        np.random.seed(10)\n",
    "        np.random.shuffle(U)\n",
    "        \n",
    "        #this is U' in paper\n",
    "        U_ = U[-min(len(U), self.u_):]\n",
    "\n",
    "        #the samples that are initially labeled\n",
    "        L = [i for i, y_i in enumerate(y) if ~np.isnan(y_i)]\n",
    "\n",
    "        #remove the samples in U_ from U\n",
    "        U = U[:-len(U_)]\n",
    "\n",
    "\n",
    "        it = 0 #number of cotraining iterations we've done so far\n",
    "\n",
    "        #loop until we have assigned labels to everything in U or we hit our iteration break condition\n",
    "        while it != self.k_ and U:\n",
    "            it += 1\n",
    "\n",
    "            \n",
    "            self.clf1_.fit(X1[L], y[L])\n",
    "            self.clf2_.fit(X2[L], y[L])\n",
    "            print(len(L))\n",
    "            ###y_test_new = y_test[U_]\n",
    "\n",
    "            y1_prob = self.clf1_.predict_log_proba(X1[U_])\n",
    "            y2_prob = self.clf2_.predict_log_proba(X2[U_])\n",
    "            \n",
    "            \n",
    "#             print(y1_prob)\n",
    "#             print(y2_prob)\n",
    "            \n",
    "            n, p = [], []\n",
    "            accurate_guesses_h1 = 0\n",
    "            accurate_guesses_h2 = 0\n",
    "            wrong_guesses_h1 = 0\n",
    "            wrong_guesses_h2 = 0\n",
    "            \n",
    "            \n",
    "            #print([np.sort(y1_prob)[:5]])\n",
    "            for i in (y1_prob[:,0].argsort())[-self.n_:]:\n",
    "                #if y1_prob[i,0] > 0.5:\n",
    "                    n.append(i)\n",
    "#                     if y_test_new[i] == 0:\n",
    "#                         accurate_guesses_h1 += 1\n",
    "#                         print(\"h1 correct class 0\")\n",
    "#                     else:\n",
    "#                         wrong_guesses_h1 += 1\n",
    "#                         print(\"h1 guessed 0 actually \" + str(y_test_new[i]))\n",
    "                 \n",
    "            #print([(np.sort(y1_prob))[-5:]])\n",
    "            for i in (y1_prob[:,1].argsort())[-self.p_:]:\n",
    "                #if y1_prob[i,1] > 0.5:\n",
    "                    p.append(i)\n",
    "#                     if y_test_new[i] == 1:\n",
    "#                         accurate_guesses_h1 += 1\n",
    "#                         print(\"h1 correct class 1\")\n",
    "#                     else:\n",
    "#                         wrong_guesses_h1 += 1\n",
    "#                         print(\"h1 guessed 1 actually \" + str(y_test_new[i]))\n",
    "\n",
    "            #print([(np.sort(y2_prob))[:5]])\n",
    "            for i in (y2_prob[:,0].argsort())[-self.n_:]:\n",
    "                #if y2_prob[i,0] > 0.5:\n",
    "                    n.append(i)\n",
    "#                     if y_test_new[i] == 0:\n",
    "#                         accurate_guesses_h2 += 1\n",
    "#                         print(\"h2 correct class 0\")\n",
    "#                     else:\n",
    "#                         wrong_guesses_h2 += 1\n",
    "#                         print(\"h2 guessed 0 actually \" + str(y_test_new[i]))\n",
    "                    \n",
    "            #print([(np.sort(y2_prob))[-5:]])\n",
    "            for i in (y2_prob[:,1].argsort())[-self.p_:]:\n",
    "                #if y2_prob[i,1] > 0.5:\n",
    "                    p.append(i)\n",
    "#                     if y_test_new[i] == 1:\n",
    "#                         accurate_guesses_h2 += 1\n",
    "#                         print(\"h2 correct class 1\")\n",
    "#                     else:\n",
    "#                         wrong_guesses_h2 += 1\n",
    "#                         print(\"h2 guessed 1 actually \" + str(y_test_new[i]))\n",
    "\n",
    "\n",
    "                        \n",
    "#             print(\"accurate guesses h1 \" + str(accurate_guesses_h1))\n",
    "#             print(\"wrong guesses h1\" + str(wrong_guesses_h1))\n",
    "#             print(\"accurate guesses h2 \" + str(accurate_guesses_h2))\n",
    "#             print(\"wrong guesses h2\" + str(wrong_guesses_h2))\n",
    "            \n",
    "\n",
    "            #label the samples and remove the newly added samples from U_\n",
    "            y[[U_[x] for x in p]] = 1\n",
    "            y[[U_[x] for x in n]] = 0\n",
    "\n",
    "            L.extend([U_[x] for x in p])\n",
    "            L.extend([U_[x] for x in n])\n",
    "\n",
    "            U_ = [elem for elem in U_ if not (elem in p or elem in n)]\n",
    "\n",
    "            #add new elements to U_\n",
    "            add_counter = 0 #number we have added from U to U_\n",
    "            num_to_add = len(p) + len(n)\n",
    "            while add_counter != num_to_add and U:\n",
    "                add_counter += 1\n",
    "                U_.append(U.pop())\n",
    "                \n",
    "            \n",
    "            # if input testing data as well, find the incrememtal update on accuracy\n",
    "            if X1_test is not None and X2_test is not None and y_test is not None:\n",
    "                y_pred = self.predict(X1_test, X2_test)\n",
    "                self.partial_error_.append(1-accuracy_score(y_test, y_pred))\n",
    "                y_pred = self.predict(X1, X2)\n",
    "                self.partial_train_error_.append(1-accuracy_score(y_train_full, y_pred))\n",
    "\n",
    "\n",
    "            #TODO: Handle the case where the classifiers fail to agree on any of the samples (i.e. both n and p are empty)\n",
    "\n",
    "\n",
    "        #fit the final model\n",
    "        self.clf1_.fit(X1[L], y[L])\n",
    "        self.clf2_.fit(X2[L], y[L])\n",
    "        \n",
    "        return (self.partial_train_error_, self.partial_error_)\n",
    "\n",
    "\n",
    "    #TODO: Move this outside of the class into a util file.\n",
    "    def supports_proba(self, clf, x):\n",
    "        \"\"\"Checks if a given classifier supports the 'predict_proba' method, given a single vector x\"\"\"\n",
    "        try:\n",
    "            clf.predict_proba([x])\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def predict(self, X1, X2):\n",
    "        \"\"\"\n",
    "        Predict the classes of the samples represented by the features in X1 and X2.\n",
    "\n",
    "        Parameters:\n",
    "        X1 - array-like (n_samples, n_features1)\n",
    "        X2 - array-like (n_samples, n_features2)\n",
    "\n",
    "        \n",
    "        Output:\n",
    "        y - array-like (n_samples)\n",
    "            These are the predicted classes of each of the samples.  If the two classifiers, don't agree, we try\n",
    "            to use predict_proba and take the classifier with the highest confidence and if predict_proba is not implemented, then we randomly\n",
    "            assign either 0 or 1.  We hope to improve this in future releases.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        y1 = self.clf1_.predict(X1)\n",
    "        y2 = self.clf2_.predict(X2)\n",
    "\n",
    "        proba_supported = self.supports_proba(self.clf1_, X1[0]) and self.supports_proba(self.clf2_, X2[0])\n",
    "\n",
    "        #fill y_pred with -1 so we can identify the samples in which the classifiers failed to agree\n",
    "        y_pred = np.asarray([-1] * X1.shape[0])\n",
    "        num_disagree = 0\n",
    "        num_agree = 0\n",
    "\n",
    "        for i, (y1_i, y2_i) in enumerate(zip(y1, y2)):\n",
    "            if y1_i == y2_i:\n",
    "                y_pred[i] = y1_i\n",
    "                num_agree += 1\n",
    "            elif proba_supported:\n",
    "                y1_probs = self.clf1_.predict_proba([X1[i]])[0]\n",
    "                y2_probs = self.clf2_.predict_proba([X2[i]])[0]\n",
    "                sum_y_probs = [prob1 + prob2 for (prob1, prob2) in zip(y1_probs, y2_probs)]\n",
    "                max_sum_prob = max(sum_y_probs)\n",
    "                y_pred[i] = sum_y_probs.index(max_sum_prob)\n",
    "                num_disagree += 1\n",
    "            else:\n",
    "                #the classifiers disagree and don't support probability, so we guess\n",
    "                y_pred[i] = random.randint(0, 1)\n",
    "                \n",
    "        print(\"agree: \" + str(num_agree))\n",
    "        print(\"disagree: \" + str(num_disagree))\n",
    "\n",
    "\n",
    "        #check that we did everything right\n",
    "        assert not (-1 in y_pred)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "    def predict_proba(self, X1, X2):\n",
    "        \"\"\"Predict the probability of the samples belonging to each class.\"\"\"\n",
    "        y_proba = np.full((X1.shape[0], 2), -1)\n",
    "\n",
    "        y1_proba = self.clf1_.predict_proba(X1)\n",
    "        y2_proba = self.clf2_.predict_proba(X2)\n",
    "\n",
    "        for i, (y1_i_dist, y2_i_dist) in enumerate(zip(y1_proba, y2_proba)):\n",
    "            y_proba[i][0] = (y1_i_dist[0] + y2_i_dist[0]) / 2\n",
    "            y_proba[i][1] = (y1_i_dist[1] + y2_i_dist[1]) / 2\n",
    "\n",
    "        _epsilon = 0.0001\n",
    "        assert all(abs(sum(y_dist) - 1) <= _epsilon for y_dist in y_proba)\n",
    "        return y_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "class CoTrainingClassifier(object):\n",
    "    \"\"\"\n",
    "    Organization from https://github.com/jjrob13/sklearn_cotraining\n",
    "    Algorithm based on \"Combining Labeled and Unlabeled Data with Co-Training\", Blum and Mitchell, 1998 \n",
    "    \n",
    "    Parameters:\n",
    "    clf - The classifier that will be used in the cotraining algorithm on the view 1 feature set\n",
    "        (If clf2 is not specified, then the same type of classifier will be used on the second view).\n",
    "\n",
    "    clf2 - (Optional) A different classifier type can be specified to be used on the X2 feature set\n",
    "         if desired.\n",
    "\n",
    "    p - (Optional) The number of positive examples that will be 'labeled' by each classifier during each iteration\n",
    "        The default is the is determined by the smallest integer ratio of positive to negative samples in L (from paper)\n",
    "\n",
    "    n - (Optional) The number of negative examples that will be 'labeled' by each classifier during each iteration\n",
    "    The default is the is determined by the smallest integer ratio of positive to negative samples in L (from paper)\n",
    "\n",
    "    k - (Optional) The number of iterations\n",
    "        The default is 30 (from paper)\n",
    "\n",
    "    u - (Optional) The size of the pool of unlabeled samples from which the classifier can choose\n",
    "    Default - 75 (from paper)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, clf, clf2=None, p=-1, n=-1, k=30, u=75):\n",
    "        \n",
    "        self.clf1_ = clf\n",
    "\n",
    "        if clf2 == None:\n",
    "            self.clf2_ = copy.copy(clf)\n",
    "        else:\n",
    "            self.clf2_ = clf2\n",
    "\n",
    "        #if user only specifies one of n or p, raise an exception\n",
    "        if (p == -1 and n != -1) or (p != -1 and n == -1):\n",
    "            raise ValueError('Must supply either both p and n, or neither')\n",
    "\n",
    "        self.p_ = p\n",
    "        self.n_ = n\n",
    "        self.k_ = k\n",
    "        self.u_ = u\n",
    "\n",
    "        random.seed(10)\n",
    "        \n",
    "        # for testing\n",
    "        self.partial_error_ = []\n",
    "        # for testing with training data\n",
    "        self.partial_train_error_ = []\n",
    "\n",
    "\n",
    "    def fit(self, X1, X2, y, y_train_full=None, X1_test=None, X2_test=None, y_test=None):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        fits the classifiers on the partially labeled data, y.\n",
    "\n",
    "        Parameters:\n",
    "        X1 - array-like (n_samples, n_features_1): first set of features for samples\n",
    "        X2 - array-like (n_samples, n_features_2): second set of features for samples\n",
    "        y - array-like (n_samples): labels for samples, -1 indicates unlabeled\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # convert to numpy array\n",
    "        y = np.asarray(y)\n",
    "\n",
    "        #set the n and p parameters if we need to\n",
    "        if self.p_ == -1 and self.n_ == -1:\n",
    "            num_pos = sum(1 for y_i in y if y_i == 1)\n",
    "            num_neg = sum(1 for y_i in y if y_i == 0)\n",
    "\n",
    "            n_p_ratio = num_neg / float(num_pos)\n",
    "\n",
    "            if n_p_ratio > 1:\n",
    "                self.p_ = 1\n",
    "                self.n_ = round(self.p_*n_p_ratio)\n",
    "\n",
    "            else:\n",
    "                self.n_ = 1\n",
    "                self.p_ = round(self.n_/n_p_ratio)\n",
    "        print(self.n_)\n",
    "        print(self.p_)\n",
    "\n",
    "        assert(self.p_ > 0 and self.n_ > 0 and self.k_ > 0 and self.u_ > 0)\n",
    "\n",
    "        #the set of unlabeled samples\n",
    "        U = [i for i, y_i in enumerate(y) if y_i == -1]\n",
    "\n",
    "        #we randomize here, and then just take from the back so we don't have to sample every time\n",
    "        np.random.seed(10)\n",
    "        np.random.shuffle(U)\n",
    "        \n",
    "        #this is U' in paper\n",
    "        U_ = U[-min(len(U), self.u_):]\n",
    "\n",
    "        #the samples that are initially labeled\n",
    "        L = [i for i, y_i in enumerate(y) if y_i != -1]\n",
    "\n",
    "        #remove the samples in U_ from U\n",
    "        U = U[:-len(U_)]\n",
    "\n",
    "\n",
    "        it = 0 #number of cotraining iterations we've done so far\n",
    "\n",
    "        #loop until we have assigned labels to everything in U or we hit our iteration break condition\n",
    "        while it != self.k_ and U:\n",
    "            it += 1\n",
    "\n",
    "            \n",
    "            self.clf1_.fit(X1[L], y[L])\n",
    "            self.clf2_.fit(X2[L], y[L])\n",
    "            print(len(L))\n",
    "            ###y_test_new = y_test[U_]\n",
    "\n",
    "            y1_prob = self.clf1_.predict_log_proba(X1[U_])\n",
    "            y2_prob = self.clf2_.predict_log_proba(X2[U_])\n",
    "            \n",
    "            \n",
    "#             print(y1_prob)\n",
    "#             print(y2_prob)\n",
    "            \n",
    "            n, p = [], []\n",
    "            accurate_guesses_h1 = 0\n",
    "            accurate_guesses_h2 = 0\n",
    "            wrong_guesses_h1 = 0\n",
    "            wrong_guesses_h2 = 0\n",
    "            \n",
    "            \n",
    "            #print([np.sort(y1_prob)[:5]])\n",
    "            for i in (y1_prob[:,0].argsort())[-self.n_:]:\n",
    "                #if y1_prob[i,0] > 0.5:\n",
    "                    n.append(i)\n",
    "#                     if y_test_new[i] == 0:\n",
    "#                         accurate_guesses_h1 += 1\n",
    "#                         print(\"h1 correct class 0\")\n",
    "#                     else:\n",
    "#                         wrong_guesses_h1 += 1\n",
    "#                         print(\"h1 guessed 0 actually \" + str(y_test_new[i]))\n",
    "                 \n",
    "            #print([(np.sort(y1_prob))[-5:]])\n",
    "            for i in (y1_prob[:,1].argsort())[-self.p_:]:\n",
    "                #if y1_prob[i,1] > 0.5:\n",
    "                    p.append(i)\n",
    "#                     if y_test_new[i] == 1:\n",
    "#                         accurate_guesses_h1 += 1\n",
    "#                         print(\"h1 correct class 1\")\n",
    "#                     else:\n",
    "#                         wrong_guesses_h1 += 1\n",
    "#                         print(\"h1 guessed 1 actually \" + str(y_test_new[i]))\n",
    "\n",
    "            #print([(np.sort(y2_prob))[:5]])\n",
    "            for i in (y2_prob[:,0].argsort())[-self.n_:]:\n",
    "                #if y2_prob[i,0] > 0.5:\n",
    "                    n.append(i)\n",
    "#                     if y_test_new[i] == 0:\n",
    "#                         accurate_guesses_h2 += 1\n",
    "#                         print(\"h2 correct class 0\")\n",
    "#                     else:\n",
    "#                         wrong_guesses_h2 += 1\n",
    "#                         print(\"h2 guessed 0 actually \" + str(y_test_new[i]))\n",
    "                    \n",
    "            #print([(np.sort(y2_prob))[-5:]])\n",
    "            for i in (y2_prob[:,1].argsort())[-self.p_:]:\n",
    "                #if y2_prob[i,1] > 0.5:\n",
    "                    p.append(i)\n",
    "#                     if y_test_new[i] == 1:\n",
    "#                         accurate_guesses_h2 += 1\n",
    "#                         print(\"h2 correct class 1\")\n",
    "#                     else:\n",
    "#                         wrong_guesses_h2 += 1\n",
    "#                         print(\"h2 guessed 1 actually \" + str(y_test_new[i]))\n",
    "\n",
    "\n",
    "                        \n",
    "#             print(\"accurate guesses h1 \" + str(accurate_guesses_h1))\n",
    "#             print(\"wrong guesses h1\" + str(wrong_guesses_h1))\n",
    "#             print(\"accurate guesses h2 \" + str(accurate_guesses_h2))\n",
    "#             print(\"wrong guesses h2\" + str(wrong_guesses_h2))\n",
    "            \n",
    "\n",
    "            #label the samples and remove the newly added samples from U_\n",
    "            y[[U_[x] for x in p]] = 1\n",
    "            y[[U_[x] for x in n]] = 0\n",
    "\n",
    "            L.extend([U_[x] for x in p])\n",
    "            L.extend([U_[x] for x in n])\n",
    "\n",
    "            U_ = [elem for elem in U_ if not (elem in p or elem in n)]\n",
    "\n",
    "            #add new elements to U_\n",
    "            add_counter = 0 #number we have added from U to U_\n",
    "            num_to_add = len(p) + len(n)\n",
    "            while add_counter != num_to_add and U:\n",
    "                add_counter += 1\n",
    "                U_.append(U.pop())\n",
    "                \n",
    "            \n",
    "            # if input testing data as well, find the incrememtal update on accuracy\n",
    "            if X1_test is not None and X2_test is not None and y_test is not None:\n",
    "                y_pred = self.predict(X1_test, X2_test)\n",
    "                self.partial_error_.append(1-accuracy_score(y_test, y_pred))\n",
    "                y_pred = self.predict(X1, X2)\n",
    "                self.partial_train_error_.append(1-accuracy_score(y_train_full, y_pred))\n",
    "\n",
    "\n",
    "            #TODO: Handle the case where the classifiers fail to agree on any of the samples (i.e. both n and p are empty)\n",
    "\n",
    "\n",
    "        #fit the final model\n",
    "        self.clf1_.fit(X1[L], y[L])\n",
    "        self.clf2_.fit(X2[L], y[L])\n",
    "        \n",
    "        return (self.partial_train_error_, self.partial_error_)\n",
    "\n",
    "\n",
    "    #TODO: Move this outside of the class into a util file.\n",
    "    def supports_proba(self, clf, x):\n",
    "        \"\"\"Checks if a given classifier supports the 'predict_proba' method, given a single vector x\"\"\"\n",
    "        try:\n",
    "            clf.predict_proba([x])\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def predict(self, X1, X2):\n",
    "        \"\"\"\n",
    "        Predict the classes of the samples represented by the features in X1 and X2.\n",
    "\n",
    "        Parameters:\n",
    "        X1 - array-like (n_samples, n_features1)\n",
    "        X2 - array-like (n_samples, n_features2)\n",
    "\n",
    "        \n",
    "        Output:\n",
    "        y - array-like (n_samples)\n",
    "            These are the predicted classes of each of the samples.  If the two classifiers, don't agree, we try\n",
    "            to use predict_proba and take the classifier with the highest confidence and if predict_proba is not implemented, then we randomly\n",
    "            assign either 0 or 1.  We hope to improve this in future releases.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        y1 = self.clf1_.predict(X1)\n",
    "        y2 = self.clf2_.predict(X2)\n",
    "\n",
    "        proba_supported = self.supports_proba(self.clf1_, X1[0]) and self.supports_proba(self.clf2_, X2[0])\n",
    "\n",
    "        #fill y_pred with -1 so we can identify the samples in which the classifiers failed to agree\n",
    "        y_pred = np.asarray([-1] * X1.shape[0])\n",
    "        num_disagree = 0\n",
    "        num_agree = 0\n",
    "\n",
    "        for i, (y1_i, y2_i) in enumerate(zip(y1, y2)):\n",
    "            if y1_i == y2_i:\n",
    "                y_pred[i] = y1_i\n",
    "                num_agree += 1\n",
    "            elif proba_supported:\n",
    "                y1_probs = self.clf1_.predict_proba([X1[i]])[0]\n",
    "                y2_probs = self.clf2_.predict_proba([X2[i]])[0]\n",
    "                sum_y_probs = [prob1 + prob2 for (prob1, prob2) in zip(y1_probs, y2_probs)]\n",
    "                max_sum_prob = max(sum_y_probs)\n",
    "                y_pred[i] = sum_y_probs.index(max_sum_prob)\n",
    "                num_disagree += 1\n",
    "            else:\n",
    "                #the classifiers disagree and don't support probability, so we guess\n",
    "                y_pred[i] = random.randint(0, 1)\n",
    "                \n",
    "        print(\"agree: \" + str(num_agree))\n",
    "        print(\"disagree: \" + str(num_disagree))\n",
    "\n",
    "\n",
    "        #check that we did everything right\n",
    "        assert not (-1 in y_pred)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "    def predict_proba(self, X1, X2):\n",
    "        \"\"\"Predict the probability of the samples belonging to each class.\"\"\"\n",
    "        y_proba = np.full((X1.shape[0], 2), -1)\n",
    "\n",
    "        y1_proba = self.clf1_.predict_proba(X1)\n",
    "        y2_proba = self.clf2_.predict_proba(X2)\n",
    "\n",
    "        for i, (y1_i_dist, y2_i_dist) in enumerate(zip(y1_proba, y2_proba)):\n",
    "            y_proba[i][0] = (y1_i_dist[0] + y2_i_dist[0]) / 2\n",
    "            y_proba[i][1] = (y1_i_dist[1] + y2_i_dist[1]) / 2\n",
    "\n",
    "        _epsilon = 0.0001\n",
    "        assert all(abs(sum(y_dist) - 1) <= _epsilon for y_dist in y_proba)\n",
    "        return y_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
