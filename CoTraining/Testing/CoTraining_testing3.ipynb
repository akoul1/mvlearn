{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.datasets as datasets\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mfeat_fac = pd.read_csv(\"../../../multi-feature-dataset/mfeat-fac.csv\")\n",
    "mfeat_fou = pd.read_csv(\"../../../multi-feature-dataset/mfeat-fou.csv\")\n",
    "mfeat_kar = pd.read_csv(\"../../../multi-feature-dataset/mfeat-kar.csv\")\n",
    "mfeat_mor = pd.read_csv(\"../../../multi-feature-dataset/mfeat-mor.csv\")\n",
    "mfeat_pix = pd.read_csv(\"../../../multi-feature-dataset/mfeat-pix.csv\")\n",
    "mfeat_zer = pd.read_csv(\"../../../multi-feature-dataset/mfeat-zer.csv\")\n",
    "labels_pd = mfeat_zer.label\n",
    "labels_pd.drop(np.arange(400,labels_pd.shape[0]), inplace=True)\n",
    "dataset_names = ['mfeat_fac','mfeat_fou','mfeat_kar','mfeat_mor','mfeat_pix','mfeat_zer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop all other than 0's and 1's\n",
    "for dataset in [mfeat_fac, mfeat_fou, mfeat_kar, mfeat_mor, mfeat_pix, mfeat_zer]:\n",
    "    dataset.drop(np.arange(400,dataset.shape[0]), inplace=True)\n",
    "    dataset.drop(['label'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>98</td>\n",
       "      <td>236</td>\n",
       "      <td>531</td>\n",
       "      <td>673</td>\n",
       "      <td>607</td>\n",
       "      <td>647</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>474</td>\n",
       "      <td>536</td>\n",
       "      <td>628</td>\n",
       "      <td>632</td>\n",
       "      <td>18</td>\n",
       "      <td>36</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>121</td>\n",
       "      <td>193</td>\n",
       "      <td>607</td>\n",
       "      <td>611</td>\n",
       "      <td>585</td>\n",
       "      <td>665</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>520</td>\n",
       "      <td>458</td>\n",
       "      <td>570</td>\n",
       "      <td>634</td>\n",
       "      <td>15</td>\n",
       "      <td>32</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>115</td>\n",
       "      <td>141</td>\n",
       "      <td>590</td>\n",
       "      <td>605</td>\n",
       "      <td>557</td>\n",
       "      <td>627</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>535</td>\n",
       "      <td>498</td>\n",
       "      <td>572</td>\n",
       "      <td>656</td>\n",
       "      <td>20</td>\n",
       "      <td>35</td>\n",
       "      <td>16</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90</td>\n",
       "      <td>122</td>\n",
       "      <td>627</td>\n",
       "      <td>692</td>\n",
       "      <td>607</td>\n",
       "      <td>642</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>576</td>\n",
       "      <td>549</td>\n",
       "      <td>628</td>\n",
       "      <td>621</td>\n",
       "      <td>16</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>157</td>\n",
       "      <td>167</td>\n",
       "      <td>681</td>\n",
       "      <td>666</td>\n",
       "      <td>587</td>\n",
       "      <td>666</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>594</td>\n",
       "      <td>525</td>\n",
       "      <td>568</td>\n",
       "      <td>653</td>\n",
       "      <td>16</td>\n",
       "      <td>35</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 216 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5   6  7  8  9 ...   206  207  208  209  210  \\\n",
       "0   98  236  531  673  607  647   2  9  3  6 ...   474  536  628  632   18   \n",
       "1  121  193  607  611  585  665   7  9  2  4 ...   520  458  570  634   15   \n",
       "2  115  141  590  605  557  627  12  6  3  3 ...   535  498  572  656   20   \n",
       "3   90  122  627  692  607  642   0  6  4  5 ...   576  549  628  621   16   \n",
       "4  157  167  681  666  587  666   8  6  1  4 ...   594  525  568  653   16   \n",
       "\n",
       "   211  212  213  214  215  \n",
       "0   36    8   15   12   13  \n",
       "1   32   11   13   15   11  \n",
       "2   35   16   14   13    6  \n",
       "3   35    7   12   15    9  \n",
       "4   35   10   15   13   13  \n",
       "\n",
       "[5 rows x 216 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfeat_fac.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataArrays = []\n",
    "for dataset in [mfeat_fac, mfeat_fou, mfeat_kar, mfeat_mor, mfeat_pix, mfeat_zer]:\n",
    "    dataArrays.append(dataset.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "[[0 3 4 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 1 ..., 2 1 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 4 3 0]\n",
      " [0 0 0 ..., 4 1 0]]\n"
     ]
    }
   ],
   "source": [
    "labels = labels_pd.values\n",
    "print(labels)\n",
    "print(dataArrays[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 216)\n",
      "(100, 216)\n",
      "[1 1 1 0 0 1 0 0 1 1 0 0 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 0 0 0 0 1 1 1 0\n",
      " 1 1 0 0 1 1 0 0 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1\n",
      " 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 1 1 0 0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "trainData = []\n",
    "testData = []\n",
    "for dataset in dataArrays:\n",
    "    temp_train, temp_test, y_train, y_test = train_test_split(dataset, labels, test_size = 0.25, random_state = 12)\n",
    "    trainData.append(temp_train)\n",
    "    testData.append(temp_test)\n",
    "    \n",
    "print(trainData[0].shape)\n",
    "print(testData[0].shape)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test of Sufficiency\n",
      "mfeat_fac accuracy: 1.0\n",
      "mfeat_fou accuracy: 0.99\n",
      "mfeat_kar accuracy: 1.0\n",
      "mfeat_mor accuracy: 0.99\n",
      "mfeat_pix accuracy: 0.99\n",
      "mfeat_zer accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "# Make sure each view is sufficient\n",
    "print(\"Test of Sufficiency\")\n",
    "for i in range(len(trainData)): \n",
    "    hh = GaussianNB()\n",
    "    hh.fit(trainData[i], y_train)\n",
    "    y_pred = hh.predict(testData[i])\n",
    "    print(dataset_names[i] + \" accuracy: \" + str(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]]\n"
     ]
    }
   ],
   "source": [
    "# set certain labels to -1\n",
    "# seed 8 Lsize 6\n",
    "# for last figure, seed 5, Lsize 6, test_size=0.25, random_state=12\n",
    "random.seed(5)\n",
    "Lsize = 6\n",
    "labels_train = y_train.copy()\n",
    "minus1 = np.arange(0,len(labels_train))\n",
    "random.shuffle(minus1)\n",
    "minus1 = minus1[:-Lsize]\n",
    "labels_train = labels_train.astype(float)\n",
    "labels_train[minus1] = np.nan\n",
    "\n",
    "print(labels_train[np.argwhere(~np.isnan(labels_train))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  4.64940000e-02   1.25060000e+00   1.44860000e+01   6.18350000e+01\n",
      "    9.47000000e+01   3.86450000e+02   2.84000000e+02   2.25340000e-02\n",
      "    6.39980000e-01   5.37530000e+00   2.32260000e+01   4.59380000e+01\n",
      "    9.20320000e+01   1.45020000e-01   3.99200000e+00   3.25960000e+01\n",
      "    5.57780000e+01   1.47380000e+02   4.15590000e+02   7.47940000e-02\n",
      "    1.47700000e+00   1.03710000e+01   2.20540000e+01   5.88240000e+01\n",
      "    5.36810000e-01   1.11190000e+01   5.74220000e+01   1.12560000e+02\n",
      "    3.36150000e+02   2.02680000e-01   3.33490000e+00   1.37960000e+01\n",
      "    2.59140000e+01   1.83340000e+00   3.00000000e+01   2.13910000e+02\n",
      "    6.17240000e+00   5.55310000e-01   7.28990000e+00   2.16730000e+01\n",
      "    6.74840000e+00   1.51040000e+02   3.97650000e+02   1.80490000e+00\n",
      "    1.85210000e+01   4.43930000e+01   5.16640000e+02]\n",
      " [  1.96440000e-01   7.47890000e-01   3.02210000e+01   9.84300000e+01\n",
      "    7.06790000e+01   1.73450000e+02   7.15460000e+01   2.48860000e-01\n",
      "    5.98300000e+00   1.04970000e+01   1.91880000e+02   1.83230000e+02\n",
      "    7.12260000e+01   9.52430000e-02   9.64360000e+00   6.67440000e+01\n",
      "    4.87890000e+01   9.36430000e+01   9.09630000e+01   8.34320000e-01\n",
      "    3.04690000e+00   1.30700000e+02   2.97310000e+02   7.38460000e+01\n",
      "    1.43630000e+00   2.61930000e+01   2.26840000e+01   1.94900000e+02\n",
      "    1.27800000e+02   4.42360000e-01   5.33220000e+01   2.77260000e+02\n",
      "    8.93710000e+01   4.72470000e+00   7.14770000e+00   2.55900000e+02\n",
      "    3.17710000e+02   1.01830000e+01   1.50720000e+02   1.40120000e+02\n",
      "    1.30240000e+00   1.73760000e+02   5.77510000e+02   3.74070000e+01\n",
      "    1.22840000e+02   5.12070000e+01   6.19140000e+02]\n",
      " [  8.88690000e-02   2.16910000e+00   2.17160000e+01   7.41520000e+01\n",
      "    2.14520000e+01   2.20260000e+02   1.21620000e+02   1.19940000e-01\n",
      "    2.56920000e+00   2.56470000e+01   1.64760000e+02   2.00030000e+02\n",
      "    1.04390000e+02   2.66570000e-01   6.35010000e+00   3.96050000e+01\n",
      "    2.44800000e+01   1.77140000e+02   1.19480000e+02   3.38750000e-01\n",
      "    8.39920000e+00   1.02110000e+02   2.64760000e+02   1.50270000e+02\n",
      "    8.91630000e-01   1.34890000e+01   2.14880000e+01   2.14870000e+02\n",
      "    2.13640000e+01   1.30390000e+00   3.92220000e+01   2.17220000e+02\n",
      "    1.80560000e+02   2.22440000e+00   1.09580000e+01   2.22960000e+02\n",
      "    2.24320000e+02   7.19410000e+00   1.08570000e+02   1.73820000e+02\n",
      "    2.51020000e+00   1.38720000e+02   4.82950000e+02   2.55390000e+01\n",
      "    1.16480000e+02   3.91940000e+01   5.16880000e+02]\n",
      " [  1.14390000e-01   2.56180000e+00   3.28650000e+01   8.25320000e+01\n",
      "    1.31860000e+01   2.29300000e+02   3.15210000e+01   2.80230000e-01\n",
      "    3.94270000e+00   2.68570000e+01   1.66570000e+02   2.76900000e+02\n",
      "    1.06890000e+02   3.25780000e-01   1.01550000e+01   5.20800000e+01\n",
      "    1.12950000e+01   8.74510000e+01   2.97480000e+01   5.52130000e-01\n",
      "    8.83310000e+00   1.12640000e+02   3.28990000e+02   8.33490000e+01\n",
      "    1.48130000e+00   1.97240000e+01   1.86150000e+01   8.78780000e+01\n",
      "    2.05930000e+01   1.39190000e+00   4.57170000e+01   2.65730000e+02\n",
      "    1.38200000e+02   3.48720000e+00   1.16160000e+01   1.69550000e+02\n",
      "    1.54210000e+02   8.69960000e+00   1.33920000e+02   1.76800000e+02\n",
      "    2.93110000e+00   1.24240000e+02   3.95360000e+02   3.18860000e+01\n",
      "    1.34870000e+02   3.75530000e+01   4.56640000e+02]\n",
      " [  3.55180000e-02   6.29230000e-01   1.87120000e+01   7.36980000e+01\n",
      "    6.88180000e+01   4.29500000e+02   1.94850000e+02   1.23220000e-01\n",
      "    3.60040000e+00   2.27240000e+01   2.62790000e+01   7.97300000e+01\n",
      "    1.76090000e+02   7.22050000e-02   5.24080000e+00   3.79470000e+01\n",
      "    6.75470000e+00   1.90280000e+02   3.96940000e+02   4.47100000e-01\n",
      "    6.53440000e+00   1.77240000e+01   1.99350000e+01   1.52080000e+02\n",
      "    7.12060000e-01   1.25740000e+01   1.41890000e+01   9.23090000e+01\n",
      "    3.99910000e+02   9.27440000e-01   8.59840000e+00   5.68570000e+01\n",
      "    7.86510000e+01   2.03050000e+00   9.58680000e+00   2.18490000e+02\n",
      "    8.46700000e+01   1.81460000e+00   3.83000000e+01   9.56410000e+00\n",
      "    2.30450000e+00   1.60760000e+02   3.43860000e+02   1.02310000e+01\n",
      "    2.34360000e+01   4.80440000e+01   5.10960000e+02]\n",
      " [  1.52190000e-02   6.10290000e-01   8.37530000e+00   3.98950000e+01\n",
      "    2.85280000e+01   3.41840000e+02   2.78600000e+02   2.23290000e-02\n",
      "    6.17260000e-01   6.24500000e+00   3.57890000e+01   4.17090000e+01\n",
      "    1.20680000e+02   6.89390000e-02   2.27070000e+00   2.24130000e+01\n",
      "    8.34520000e+01   1.01950000e+02   3.84950000e+02   8.23700000e-02\n",
      "    2.28270000e+00   2.78730000e+01   1.00110000e+02   3.48390000e+01\n",
      "    3.01200000e-01   7.78970000e+00   7.37220000e+01   1.39650000e+02\n",
      "    2.86070000e+02   3.72010000e-01   1.16730000e+01   9.22790000e+01\n",
      "    7.27870000e+01   1.29010000e+00   3.47530000e+01   2.26210000e+02\n",
      "    5.67560000e+01   2.22290000e+00   4.76540000e+01   1.17380000e+02\n",
      "    7.41700000e+00   1.54140000e+02   4.32390000e+02   1.13060000e+01\n",
      "    9.27490000e+01   4.46730000e+01   5.32530000e+02]]\n",
      "[[ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(trainData[i][np.argwhere(~np.isnan(labels_train)),:].squeeze())\n",
    "print(labels_train[np.argwhere(~np.isnan(labels_train))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mfeat_fac semi-supervised accuracy: 0.58\n",
      "mfeat_fou semi-supervised accuracy: 0.73\n",
      "mfeat_kar semi-supervised accuracy: 0.69\n",
      "mfeat_mor semi-supervised accuracy: 0.99\n",
      "mfeat_pix semi-supervised accuracy: 0.99\n",
      "mfeat_zer semi-supervised accuracy: 0.77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gavin\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# Test Accuracy in Semi-supervised setting\n",
    "for i in range(len(trainData)):\n",
    "    hh = GaussianNB()\n",
    "    hh.fit(trainData[i][np.argwhere(~np.isnan(labels_train)),:].squeeze(), labels_train[np.argwhere(~np.isnan(labels_train))])\n",
    "    y_pred = hh.predict(testData[i])\n",
    "    print(dataset_names[i] + \" semi-supervised accuracy: \" + str(accuracy_score(y_test, y_pred)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of set\n",
      "2\n",
      "1\n",
      "1\n",
      "U is\n",
      "L is\n",
      "103\n",
      "agree: 98\n",
      "disagree: 2\n",
      "agree: 293\n",
      "disagree: 7\n",
      "107\n",
      "agree: 98\n",
      "disagree: 2\n",
      "agree: 293\n",
      "disagree: 7\n",
      "111\n",
      "agree: 98\n",
      "disagree: 2\n",
      "agree: 293\n",
      "disagree: 7\n",
      "115\n",
      "agree: 98\n",
      "disagree: 2\n",
      "agree: 293\n",
      "disagree: 7\n",
      "119\n",
      "agree: 98\n",
      "disagree: 2\n",
      "agree: 293\n",
      "disagree: 7\n",
      "123\n",
      "agree: 98\n",
      "disagree: 2\n",
      "agree: 294\n",
      "disagree: 6\n",
      "127\n",
      "agree: 98\n",
      "disagree: 2\n",
      "agree: 293\n",
      "disagree: 7\n",
      "131\n",
      "agree: 99\n",
      "disagree: 1\n",
      "agree: 293\n",
      "disagree: 7\n",
      "135\n",
      "agree: 99\n",
      "disagree: 1\n",
      "agree: 293\n",
      "disagree: 7\n",
      "139\n",
      "agree: 99\n",
      "disagree: 1\n",
      "agree: 293\n",
      "disagree: 7\n",
      "143\n",
      "agree: 99\n",
      "disagree: 1\n",
      "agree: 292\n",
      "disagree: 8\n",
      "147\n",
      "agree: 99\n",
      "disagree: 1\n",
      "agree: 292\n",
      "disagree: 8\n",
      "151\n",
      "agree: 99\n",
      "disagree: 1\n",
      "agree: 293\n",
      "disagree: 7\n",
      "155\n",
      "agree: 99\n",
      "disagree: 1\n",
      "agree: 293\n",
      "disagree: 7\n",
      "159\n",
      "agree: 99\n",
      "disagree: 1\n",
      "agree: 293\n",
      "disagree: 7\n",
      "163\n",
      "agree: 99\n",
      "disagree: 1\n",
      "agree: 293\n",
      "disagree: 7\n",
      "167\n",
      "agree: 99\n",
      "disagree: 1\n",
      "agree: 292\n",
      "disagree: 8\n",
      "171\n",
      "agree: 99\n",
      "disagree: 1\n",
      "agree: 292\n",
      "disagree: 8\n",
      "175\n",
      "agree: 99\n",
      "disagree: 1\n",
      "agree: 292\n",
      "disagree: 8\n",
      "179\n",
      "agree: 99\n",
      "disagree: 1\n",
      "agree: 292\n",
      "disagree: 8\n",
      "183\n",
      "agree: 99\n",
      "disagree: 1\n",
      "agree: 292\n",
      "disagree: 8\n",
      "187\n",
      "agree: 99\n",
      "disagree: 1\n",
      "agree: 292\n",
      "disagree: 8\n",
      "191\n",
      "agree: 99\n",
      "disagree: 1\n",
      "agree: 293\n",
      "disagree: 7\n",
      "195\n",
      "agree: 99\n",
      "disagree: 1\n",
      "agree: 293\n",
      "disagree: 7\n",
      "199\n",
      "agree: 99\n",
      "disagree: 1\n",
      "agree: 293\n",
      "disagree: 7\n",
      "203\n",
      "agree: 99\n",
      "disagree: 1\n",
      "agree: 293\n",
      "disagree: 7\n",
      "207\n",
      "agree: 99\n",
      "disagree: 1\n",
      "agree: 293\n",
      "disagree: 7\n",
      "211\n",
      "agree: 99\n",
      "disagree: 1\n",
      "agree: 293\n",
      "disagree: 7\n",
      "215\n",
      "agree: 99\n",
      "disagree: 1\n",
      "agree: 293\n",
      "disagree: 7\n",
      "219\n",
      "agree: 99\n",
      "disagree: 1\n",
      "agree: 293\n",
      "disagree: 7\n",
      "223\n",
      "agree: 99\n",
      "disagree: 1\n",
      "agree: 293\n",
      "disagree: 7\n"
     ]
    }
   ],
   "source": [
    "view1 = 1\n",
    "view2 = 5\n",
    "gnb1 = GaussianNB()\n",
    "gnb2 = GaussianNB()\n",
    "clf = CoTrainingClassifier(gnb1, gnb2, n=1, p=1, k=100, u=75)\n",
    "errors_train, errors = clf.fit(trainData[view1], trainData[view2], labels_train, y_train, testData[view1], testData[view2], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCCGq/H8FrSChdtNaKwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d0b5513588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datetime\n",
    "time = str(datetime.datetime.now())\n",
    "time = time.replace(\":\",\"_\")\n",
    "time = time.replace(\".\",\"_\")\n",
    "\n",
    "fig, ax = plt.subplots(1,2, sharey=True)\n",
    "ax[0].plot(100*np.array(errors_train))\n",
    "ax[0].set_title(\"Error on Training Data\")\n",
    "ax[1].plot(100*np.array(errors))\n",
    "ax[1].set_title(\"Error on Testing Data\")\n",
    "ax[0].set_xlabel(\"Co-training iterations\")\n",
    "ax[0].set_ylabel(\"Classification Error (%)\")\n",
    "ax[1].set_xlabel(\"Co-training iterations\")\n",
    "ax[1].set_ylabel(\"Classification Error (%)\")\n",
    "plt.savefig(\"Multiview data error \" + dataset_names[view1] + \"_and_\" + dataset_names[view2] + time + \".png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Update documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#import copy\n",
    "\n",
    "class CoTrainingClassifier(object):\n",
    "    \"\"\"\n",
    "    Co-Training Classifier\n",
    "    \n",
    "    This class implements the co-training classifier similar to as described in [1]\n",
    "    This is meant to be used on 2 views of the input data which satisfy the 2 conditions\n",
    "    \n",
    "    Organization from https://github.com/jjrob13/sklearn_cotraining\n",
    "    Algorithm based on \"Combining Labeled and Unlabeled Data with Co-Training\", Blum and Mitchell, 1998 \n",
    "    \n",
    "    Parameters:\n",
    "    clf - The classifier that will be used in the cotraining algorithm on the view 1 feature set\n",
    "        (If clf2 is not specified, then the same type of classifier will be used on the second view).\n",
    "\n",
    "    clf2 - (Optional) A different classifier type can be specified to be used on the X2 feature set\n",
    "         if desired.\n",
    "\n",
    "    p - (Optional) The number of positive examples that will be 'labeled' by each classifier during each iteration\n",
    "        The default is the is determined by the smallest integer ratio of positive to negative samples in L (from paper)\n",
    "\n",
    "    n - (Optional) The number of negative examples that will be 'labeled' by each classifier during each iteration\n",
    "    The default is the is determined by the smallest integer ratio of positive to negative samples in L (from paper)\n",
    "\n",
    "    k - (Optional) The number of iterations\n",
    "        The default is 30 (from paper)\n",
    "\n",
    "    u - (Optional) The size of the pool of unlabeled samples from which the classifier can choose\n",
    "    Default - 75 (from paper)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, clf, clf2, p=-1, n=-1, k=30, u=75):\n",
    "        \n",
    "        self.clf1_ = clf\n",
    "\n",
    "#         if clf2 == None:\n",
    "#             self.clf2_ = copy.copy(clf)\n",
    "#         else:\n",
    "        self.clf2_ = clf2\n",
    "\n",
    "        #if user only specifies one of n or p, raise an exception\n",
    "        if (p == -1 and n != -1) or (p != -1 and n == -1):\n",
    "            raise ValueError('Must supply either both p and n, or neither')\n",
    "\n",
    "        self.p_ = p\n",
    "        self.n_ = n\n",
    "        self.k_ = k\n",
    "        self.u_ = u\n",
    "\n",
    "        random.seed(10)\n",
    "        \n",
    "        # for testing\n",
    "        self.partial_error_ = []\n",
    "        # for testing with training data\n",
    "        self.partial_train_error_ = []\n",
    "\n",
    "\n",
    "    def fit(self, X1, X2, y, y_train_full=None, X1_test=None, X2_test=None, y_test=None):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        fits the classifiers on the partially labeled data, y.\n",
    "\n",
    "        Parameters:\n",
    "        X1 - array-like (n_samples, n_features_1): first set of features for samples\n",
    "        X2 - array-like (n_samples, n_features_2): second set of features for samples\n",
    "        y - array-like (n_samples): labels for samples, -1 indicates unlabeled\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # convert to numpy array\n",
    "        y = np.asarray(y)\n",
    "        print(\"length of set\")\n",
    "        print(len(set(y[~np.isnan(y)])))\n",
    "\n",
    "        #set the n and p parameters if we need to\n",
    "        if self.p_ == -1 and self.n_ == -1:\n",
    "            num_pos = sum(1 for y_i in y if y_i == 1)\n",
    "            num_neg = sum(1 for y_i in y if y_i == 0)\n",
    "\n",
    "            n_p_ratio = num_neg / float(num_pos)\n",
    "\n",
    "            if n_p_ratio > 1:\n",
    "                self.p_ = 1\n",
    "                self.n_ = round(self.p_*n_p_ratio)\n",
    "\n",
    "            else:\n",
    "                self.n_ = 1\n",
    "                self.p_ = round(self.n_/n_p_ratio)\n",
    "        print(self.n_)\n",
    "        print(self.p_)\n",
    "\n",
    "        assert(self.p_ > 0 and self.n_ > 0 and self.k_ > 0 and self.u_ > 0)\n",
    "\n",
    "        #the set of unlabeled samples\n",
    "        U = [i for i, y_i in enumerate(y) if np.isnan(y_i)]\n",
    "        print(\"U is\")\n",
    "\n",
    "        #we randomize here, and then just take from the back so we don't have to sample every time\n",
    "        np.random.seed(10)\n",
    "        np.random.shuffle(U)\n",
    "        \n",
    "        #this is U' in paper\n",
    "        U_ = U[-min(len(U), self.u_):]\n",
    "\n",
    "        #the samples that are initially labeled\n",
    "        L = [i for i, y_i in enumerate(y) if ~np.isnan(y_i)]\n",
    "        print(\"L is\")\n",
    "\n",
    "        #remove the samples in U_ from U\n",
    "        U = U[:-len(U_)]\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        it = 0 #number of cotraining iterations we've done so far\n",
    "\n",
    "        #loop until we have assigned labels to everything in U or we hit our iteration break condition\n",
    "        while it != self.k_ and len(U) > 0:\n",
    "            it += 1\n",
    "\n",
    "            \n",
    "            self.clf1_.fit(X1[L], y[L])\n",
    "            self.clf2_.fit(X2[L], y[L])\n",
    "            print(len(L))\n",
    "            ###y_test_new = y_test[U_]\n",
    "\n",
    "            y1_prob = self.clf1_.predict_log_proba(X1[U_])\n",
    "            y2_prob = self.clf2_.predict_log_proba(X2[U_])\n",
    "            \n",
    "            \n",
    "#             print(y1_prob)\n",
    "#             print(y2_prob)\n",
    "            \n",
    "            n, p = [], []\n",
    "            accurate_guesses_h1 = 0\n",
    "            accurate_guesses_h2 = 0\n",
    "            wrong_guesses_h1 = 0\n",
    "            wrong_guesses_h2 = 0\n",
    "            \n",
    "            \n",
    "            #print([np.sort(y1_prob)[:5]])\n",
    "            for i in (y1_prob[:,0].argsort())[-self.n_:]:\n",
    "                if y1_prob[i,0] > np.log(0.5):\n",
    "                    n.append(i)\n",
    "#                     if y_test_new[i] == 0:\n",
    "#                         accurate_guesses_h1 += 1\n",
    "#                         print(\"h1 correct class 0\")\n",
    "#                     else:\n",
    "#                         wrong_guesses_h1 += 1\n",
    "#                         print(\"h1 guessed 0 actually \" + str(y_test_new[i]))\n",
    "                 \n",
    "            #print([(np.sort(y1_prob))[-5:]])\n",
    "            for i in (y1_prob[:,1].argsort())[-self.p_:]:\n",
    "                if y1_prob[i,1] > np.log(0.5):\n",
    "                    p.append(i)\n",
    "#                     if y_test_new[i] == 1:\n",
    "#                         accurate_guesses_h1 += 1\n",
    "#                         print(\"h1 correct class 1\")\n",
    "#                     else:\n",
    "#                         wrong_guesses_h1 += 1\n",
    "#                         print(\"h1 guessed 1 actually \" + str(y_test_new[i]))\n",
    "\n",
    "            #print([(np.sort(y2_prob))[:5]])\n",
    "            for i in (y2_prob[:,0].argsort())[-self.n_:]:\n",
    "                if y2_prob[i,0] > np.log(0.5):\n",
    "                    n.append(i)\n",
    "#                     if y_test_new[i] == 0:\n",
    "#                         accurate_guesses_h2 += 1\n",
    "#                         print(\"h2 correct class 0\")\n",
    "#                     else:\n",
    "#                         wrong_guesses_h2 += 1\n",
    "#                         print(\"h2 guessed 0 actually \" + str(y_test_new[i]))\n",
    "                    \n",
    "            #print([(np.sort(y2_prob))[-5:]])\n",
    "            for i in (y2_prob[:,1].argsort())[-self.p_:]:\n",
    "                if y2_prob[i,1] > np.log(0.5):\n",
    "                    p.append(i)\n",
    "#                     if y_test_new[i] == 1:\n",
    "#                         accurate_guesses_h2 += 1\n",
    "#                         print(\"h2 correct class 1\")\n",
    "#                     else:\n",
    "#                         wrong_guesses_h2 += 1\n",
    "#                         print(\"h2 guessed 1 actually \" + str(y_test_new[i]))\n",
    "\n",
    "\n",
    "                        \n",
    "#             print(\"accurate guesses h1 \" + str(accurate_guesses_h1))\n",
    "#             print(\"wrong guesses h1\" + str(wrong_guesses_h1))\n",
    "#             print(\"accurate guesses h2 \" + str(accurate_guesses_h2))\n",
    "#             print(\"wrong guesses h2\" + str(wrong_guesses_h2))\n",
    "            \n",
    "\n",
    "            #label the samples and remove the newly added samples from U_\n",
    "            y[[U_[x] for x in p]] = 1\n",
    "            y[[U_[x] for x in n]] = 0\n",
    "\n",
    "            L.extend([U_[x] for x in p])\n",
    "            L.extend([U_[x] for x in n])\n",
    "\n",
    "            U_ = [elem for elem in U_ if not (elem in p or elem in n)]\n",
    "\n",
    "            #add new elements to U_\n",
    "            add_counter = 0 #number we have added from U to U_\n",
    "            num_to_add = len(p) + len(n)\n",
    "            while add_counter != num_to_add and U:\n",
    "                add_counter += 1\n",
    "                U_.append(U.pop())\n",
    "                \n",
    "            \n",
    "            # if input testing data as well, find the incrememtal update on accuracy\n",
    "            if X1_test is not None and X2_test is not None and y_test is not None:\n",
    "                y_pred = self.predict(X1_test, X2_test)\n",
    "                self.partial_error_.append(1-accuracy_score(y_test, y_pred))\n",
    "                y_pred = self.predict(X1, X2)\n",
    "                self.partial_train_error_.append(1-accuracy_score(y_train_full, y_pred))\n",
    "\n",
    "\n",
    "            #TODO: Handle the case where the classifiers fail to agree on any of the samples (i.e. both n and p are empty)\n",
    "\n",
    "\n",
    "        #fit the final model\n",
    "        self.clf1_.fit(X1[L], y[L])\n",
    "        self.clf2_.fit(X2[L], y[L])\n",
    "        \n",
    "        return (self.partial_train_error_, self.partial_error_)\n",
    "\n",
    "\n",
    "    #TODO: Move this outside of the class into a util file.\n",
    "    def supports_proba(self, clf, x):\n",
    "        \"\"\"Checks if a given classifier supports the 'predict_proba' method, given a single vector x\"\"\"\n",
    "        try:\n",
    "            clf.predict_proba([x])\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def predict(self, X1, X2):\n",
    "        \"\"\"\n",
    "        Predict the classes of the samples represented by the features in X1 and X2.\n",
    "\n",
    "        Parameters:\n",
    "        X1 - array-like (n_samples, n_features1)\n",
    "        X2 - array-like (n_samples, n_features2)\n",
    "\n",
    "        \n",
    "        Output:\n",
    "        y - array-like (n_samples)\n",
    "            These are the predicted classes of each of the samples.  If the two classifiers, don't agree, we try\n",
    "            to use predict_proba and take the classifier with the highest confidence and if predict_proba is not implemented, then we randomly\n",
    "            assign either 0 or 1.  We hope to improve this in future releases.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        y1 = self.clf1_.predict(X1)\n",
    "        y2 = self.clf2_.predict(X2)\n",
    "\n",
    "        proba_supported = self.supports_proba(self.clf1_, X1[0]) and self.supports_proba(self.clf2_, X2[0])\n",
    "\n",
    "        #fill y_pred with -1 so we can identify the samples in which the classifiers failed to agree\n",
    "        y_pred = np.asarray([-1] * X1.shape[0])\n",
    "        num_disagree = 0\n",
    "        num_agree = 0\n",
    "\n",
    "        for i, (y1_i, y2_i) in enumerate(zip(y1, y2)):\n",
    "            if y1_i == y2_i:\n",
    "                y_pred[i] = y1_i\n",
    "                num_agree += 1\n",
    "            elif proba_supported:\n",
    "                y1_probs = self.clf1_.predict_proba([X1[i]])[0]\n",
    "                y2_probs = self.clf2_.predict_proba([X2[i]])[0]\n",
    "                sum_y_probs = [prob1 + prob2 for (prob1, prob2) in zip(y1_probs, y2_probs)]\n",
    "                max_sum_prob = max(sum_y_probs)\n",
    "                y_pred[i] = sum_y_probs.index(max_sum_prob)\n",
    "                num_disagree += 1\n",
    "            else:\n",
    "                #the classifiers disagree and don't support probability, so we guess\n",
    "                y_pred[i] = random.randint(0, 1)\n",
    "                \n",
    "        print(\"agree: \" + str(num_agree))\n",
    "        print(\"disagree: \" + str(num_disagree))\n",
    "\n",
    "\n",
    "        #check that we did everything right\n",
    "        assert not (-1 in y_pred)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "    def predict_proba(self, X1, X2):\n",
    "        \"\"\"Predict the probability of the samples belonging to each class.\"\"\"\n",
    "        y_proba = np.full((X1.shape[0], 2), -1)\n",
    "\n",
    "        y1_proba = self.clf1_.predict_proba(X1)\n",
    "        y2_proba = self.clf2_.predict_proba(X2)\n",
    "\n",
    "        for i, (y1_i_dist, y2_i_dist) in enumerate(zip(y1_proba, y2_proba)):\n",
    "            y_proba[i][0] = (y1_i_dist[0] + y2_i_dist[0]) / 2\n",
    "            y_proba[i][1] = (y1_i_dist[1] + y2_i_dist[1]) / 2\n",
    "\n",
    "        _epsilon = 0.0001\n",
    "        assert all(abs(sum(y_dist) - 1) <= _epsilon for y_dist in y_proba)\n",
    "        return y_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "class CoTrainingClassifier(object):\n",
    "    \"\"\"\n",
    "    Organization from https://github.com/jjrob13/sklearn_cotraining\n",
    "    Algorithm based on \"Combining Labeled and Unlabeled Data with Co-Training\", Blum and Mitchell, 1998 \n",
    "    \n",
    "    Parameters:\n",
    "    clf - The classifier that will be used in the cotraining algorithm on the view 1 feature set\n",
    "        (If clf2 is not specified, then the same type of classifier will be used on the second view).\n",
    "\n",
    "    clf2 - (Optional) A different classifier type can be specified to be used on the X2 feature set\n",
    "         if desired.\n",
    "\n",
    "    p - (Optional) The number of positive examples that will be 'labeled' by each classifier during each iteration\n",
    "        The default is the is determined by the smallest integer ratio of positive to negative samples in L (from paper)\n",
    "\n",
    "    n - (Optional) The number of negative examples that will be 'labeled' by each classifier during each iteration\n",
    "    The default is the is determined by the smallest integer ratio of positive to negative samples in L (from paper)\n",
    "\n",
    "    k - (Optional) The number of iterations\n",
    "        The default is 30 (from paper)\n",
    "\n",
    "    u - (Optional) The size of the pool of unlabeled samples from which the classifier can choose\n",
    "    Default - 75 (from paper)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, clf, clf2=None, p=-1, n=-1, k=30, u=75):\n",
    "        \n",
    "        self.clf1_ = clf\n",
    "\n",
    "        if clf2 == None:\n",
    "            self.clf2_ = copy.copy(clf)\n",
    "        else:\n",
    "            self.clf2_ = clf2\n",
    "\n",
    "        #if user only specifies one of n or p, raise an exception\n",
    "        if (p == -1 and n != -1) or (p != -1 and n == -1):\n",
    "            raise ValueError('Must supply either both p and n, or neither')\n",
    "\n",
    "        self.p_ = p\n",
    "        self.n_ = n\n",
    "        self.k_ = k\n",
    "        self.u_ = u\n",
    "\n",
    "        random.seed(10)\n",
    "        \n",
    "        # for testing\n",
    "        self.partial_error_ = []\n",
    "        # for testing with training data\n",
    "        self.partial_train_error_ = []\n",
    "\n",
    "\n",
    "    def fit(self, X1, X2, y, y_train_full=None, X1_test=None, X2_test=None, y_test=None):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        fits the classifiers on the partially labeled data, y.\n",
    "\n",
    "        Parameters:\n",
    "        X1 - array-like (n_samples, n_features_1): first set of features for samples\n",
    "        X2 - array-like (n_samples, n_features_2): second set of features for samples\n",
    "        y - array-like (n_samples): labels for samples, -1 indicates unlabeled\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # convert to numpy array\n",
    "        y = np.asarray(y)\n",
    "\n",
    "        #set the n and p parameters if we need to\n",
    "        if self.p_ == -1 and self.n_ == -1:\n",
    "            num_pos = sum(1 for y_i in y if y_i == 1)\n",
    "            num_neg = sum(1 for y_i in y if y_i == 0)\n",
    "\n",
    "            n_p_ratio = num_neg / float(num_pos)\n",
    "\n",
    "            if n_p_ratio > 1:\n",
    "                self.p_ = 1\n",
    "                self.n_ = round(self.p_*n_p_ratio)\n",
    "\n",
    "            else:\n",
    "                self.n_ = 1\n",
    "                self.p_ = round(self.n_/n_p_ratio)\n",
    "        print(self.n_)\n",
    "        print(self.p_)\n",
    "\n",
    "        assert(self.p_ > 0 and self.n_ > 0 and self.k_ > 0 and self.u_ > 0)\n",
    "\n",
    "        #the set of unlabeled samples\n",
    "        U = [i for i, y_i in enumerate(y) if y_i == -1]\n",
    "\n",
    "        #we randomize here, and then just take from the back so we don't have to sample every time\n",
    "        np.random.seed(10)\n",
    "        np.random.shuffle(U)\n",
    "        \n",
    "        #this is U' in paper\n",
    "        U_ = U[-min(len(U), self.u_):]\n",
    "\n",
    "        #the samples that are initially labeled\n",
    "        L = [i for i, y_i in enumerate(y) if y_i != -1]\n",
    "\n",
    "        #remove the samples in U_ from U\n",
    "        U = U[:-len(U_)]\n",
    "\n",
    "\n",
    "        it = 0 #number of cotraining iterations we've done so far\n",
    "\n",
    "        #loop until we have assigned labels to everything in U or we hit our iteration break condition\n",
    "        while it != self.k_ and U:\n",
    "            it += 1\n",
    "\n",
    "            \n",
    "            self.clf1_.fit(X1[L], y[L])\n",
    "            self.clf2_.fit(X2[L], y[L])\n",
    "            print(len(L))\n",
    "            ###y_test_new = y_test[U_]\n",
    "\n",
    "            y1_prob = self.clf1_.predict_log_proba(X1[U_])\n",
    "            y2_prob = self.clf2_.predict_log_proba(X2[U_])\n",
    "            \n",
    "            \n",
    "#             print(y1_prob)\n",
    "#             print(y2_prob)\n",
    "            \n",
    "            n, p = [], []\n",
    "            accurate_guesses_h1 = 0\n",
    "            accurate_guesses_h2 = 0\n",
    "            wrong_guesses_h1 = 0\n",
    "            wrong_guesses_h2 = 0\n",
    "            \n",
    "            \n",
    "            #print([np.sort(y1_prob)[:5]])\n",
    "            for i in (y1_prob[:,0].argsort())[-self.n_:]:\n",
    "                #if y1_prob[i,0] > 0.5:\n",
    "                    n.append(i)\n",
    "#                     if y_test_new[i] == 0:\n",
    "#                         accurate_guesses_h1 += 1\n",
    "#                         print(\"h1 correct class 0\")\n",
    "#                     else:\n",
    "#                         wrong_guesses_h1 += 1\n",
    "#                         print(\"h1 guessed 0 actually \" + str(y_test_new[i]))\n",
    "                 \n",
    "            #print([(np.sort(y1_prob))[-5:]])\n",
    "            for i in (y1_prob[:,1].argsort())[-self.p_:]:\n",
    "                #if y1_prob[i,1] > 0.5:\n",
    "                    p.append(i)\n",
    "#                     if y_test_new[i] == 1:\n",
    "#                         accurate_guesses_h1 += 1\n",
    "#                         print(\"h1 correct class 1\")\n",
    "#                     else:\n",
    "#                         wrong_guesses_h1 += 1\n",
    "#                         print(\"h1 guessed 1 actually \" + str(y_test_new[i]))\n",
    "\n",
    "            #print([(np.sort(y2_prob))[:5]])\n",
    "            for i in (y2_prob[:,0].argsort())[-self.n_:]:\n",
    "                #if y2_prob[i,0] > 0.5:\n",
    "                    n.append(i)\n",
    "#                     if y_test_new[i] == 0:\n",
    "#                         accurate_guesses_h2 += 1\n",
    "#                         print(\"h2 correct class 0\")\n",
    "#                     else:\n",
    "#                         wrong_guesses_h2 += 1\n",
    "#                         print(\"h2 guessed 0 actually \" + str(y_test_new[i]))\n",
    "                    \n",
    "            #print([(np.sort(y2_prob))[-5:]])\n",
    "            for i in (y2_prob[:,1].argsort())[-self.p_:]:\n",
    "                #if y2_prob[i,1] > 0.5:\n",
    "                    p.append(i)\n",
    "#                     if y_test_new[i] == 1:\n",
    "#                         accurate_guesses_h2 += 1\n",
    "#                         print(\"h2 correct class 1\")\n",
    "#                     else:\n",
    "#                         wrong_guesses_h2 += 1\n",
    "#                         print(\"h2 guessed 1 actually \" + str(y_test_new[i]))\n",
    "\n",
    "\n",
    "                        \n",
    "#             print(\"accurate guesses h1 \" + str(accurate_guesses_h1))\n",
    "#             print(\"wrong guesses h1\" + str(wrong_guesses_h1))\n",
    "#             print(\"accurate guesses h2 \" + str(accurate_guesses_h2))\n",
    "#             print(\"wrong guesses h2\" + str(wrong_guesses_h2))\n",
    "            \n",
    "\n",
    "            #label the samples and remove the newly added samples from U_\n",
    "            y[[U_[x] for x in p]] = 1\n",
    "            y[[U_[x] for x in n]] = 0\n",
    "\n",
    "            L.extend([U_[x] for x in p])\n",
    "            L.extend([U_[x] for x in n])\n",
    "\n",
    "            U_ = [elem for elem in U_ if not (elem in p or elem in n)]\n",
    "\n",
    "            #add new elements to U_\n",
    "            add_counter = 0 #number we have added from U to U_\n",
    "            num_to_add = len(p) + len(n)\n",
    "            while add_counter != num_to_add and U:\n",
    "                add_counter += 1\n",
    "                U_.append(U.pop())\n",
    "                \n",
    "            \n",
    "            # if input testing data as well, find the incrememtal update on accuracy\n",
    "            if X1_test is not None and X2_test is not None and y_test is not None:\n",
    "                y_pred = self.predict(X1_test, X2_test)\n",
    "                self.partial_error_.append(1-accuracy_score(y_test, y_pred))\n",
    "                y_pred = self.predict(X1, X2)\n",
    "                self.partial_train_error_.append(1-accuracy_score(y_train_full, y_pred))\n",
    "\n",
    "\n",
    "            #TODO: Handle the case where the classifiers fail to agree on any of the samples (i.e. both n and p are empty)\n",
    "\n",
    "\n",
    "        #fit the final model\n",
    "        self.clf1_.fit(X1[L], y[L])\n",
    "        self.clf2_.fit(X2[L], y[L])\n",
    "        \n",
    "        return (self.partial_train_error_, self.partial_error_)\n",
    "\n",
    "\n",
    "    #TODO: Move this outside of the class into a util file.\n",
    "    def supports_proba(self, clf, x):\n",
    "        \"\"\"Checks if a given classifier supports the 'predict_proba' method, given a single vector x\"\"\"\n",
    "        try:\n",
    "            clf.predict_proba([x])\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def predict(self, X1, X2):\n",
    "        \"\"\"\n",
    "        Predict the classes of the samples represented by the features in X1 and X2.\n",
    "\n",
    "        Parameters:\n",
    "        X1 - array-like (n_samples, n_features1)\n",
    "        X2 - array-like (n_samples, n_features2)\n",
    "\n",
    "        \n",
    "        Output:\n",
    "        y - array-like (n_samples)\n",
    "            These are the predicted classes of each of the samples.  If the two classifiers, don't agree, we try\n",
    "            to use predict_proba and take the classifier with the highest confidence and if predict_proba is not implemented, then we randomly\n",
    "            assign either 0 or 1.  We hope to improve this in future releases.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        y1 = self.clf1_.predict(X1)\n",
    "        y2 = self.clf2_.predict(X2)\n",
    "\n",
    "        proba_supported = self.supports_proba(self.clf1_, X1[0]) and self.supports_proba(self.clf2_, X2[0])\n",
    "\n",
    "        #fill y_pred with -1 so we can identify the samples in which the classifiers failed to agree\n",
    "        y_pred = np.asarray([-1] * X1.shape[0])\n",
    "        num_disagree = 0\n",
    "        num_agree = 0\n",
    "\n",
    "        for i, (y1_i, y2_i) in enumerate(zip(y1, y2)):\n",
    "            if y1_i == y2_i:\n",
    "                y_pred[i] = y1_i\n",
    "                num_agree += 1\n",
    "            elif proba_supported:\n",
    "                y1_probs = self.clf1_.predict_proba([X1[i]])[0]\n",
    "                y2_probs = self.clf2_.predict_proba([X2[i]])[0]\n",
    "                sum_y_probs = [prob1 + prob2 for (prob1, prob2) in zip(y1_probs, y2_probs)]\n",
    "                max_sum_prob = max(sum_y_probs)\n",
    "                y_pred[i] = sum_y_probs.index(max_sum_prob)\n",
    "                num_disagree += 1\n",
    "            else:\n",
    "                #the classifiers disagree and don't support probability, so we guess\n",
    "                y_pred[i] = random.randint(0, 1)\n",
    "                \n",
    "        print(\"agree: \" + str(num_agree))\n",
    "        print(\"disagree: \" + str(num_disagree))\n",
    "\n",
    "\n",
    "        #check that we did everything right\n",
    "        assert not (-1 in y_pred)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "    def predict_proba(self, X1, X2):\n",
    "        \"\"\"Predict the probability of the samples belonging to each class.\"\"\"\n",
    "        y_proba = np.full((X1.shape[0], 2), -1)\n",
    "\n",
    "        y1_proba = self.clf1_.predict_proba(X1)\n",
    "        y2_proba = self.clf2_.predict_proba(X2)\n",
    "\n",
    "        for i, (y1_i_dist, y2_i_dist) in enumerate(zip(y1_proba, y2_proba)):\n",
    "            y_proba[i][0] = (y1_i_dist[0] + y2_i_dist[0]) / 2\n",
    "            y_proba[i][1] = (y1_i_dist[1] + y2_i_dist[1]) / 2\n",
    "\n",
    "        _epsilon = 0.0001\n",
    "        assert all(abs(sum(y_dist) - 1) <= _epsilon for y_dist in y_proba)\n",
    "        return y_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
