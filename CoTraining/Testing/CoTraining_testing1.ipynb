{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Co-Training Implementation\n",
    "**Based on the Blum and Mitchell, 1998 paper**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#setA = ['rec.sport.baseball', 'rec.sport.hockey']\n",
    "setA = ['comp.os.ms-windows.misc','comp.sys.ibm.pc.hardware']\n",
    "setB = ['talk.politics.misc','talk.politics.guns']\n",
    "\n",
    "newsA_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'), categories=setA)\n",
    "newsB_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'), categories=setB)\n",
    "\n",
    "newsA_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'), categories=setA)\n",
    "newsB_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'), categories=setB)\n",
    "\n",
    "newsA_y_train = newsA_train.target\n",
    "newsB_y_train = newsB_train.target\n",
    "\n",
    "newsA_y_test = newsA_test.target\n",
    "newsB_y_test = newsB_test.target\n",
    "\n",
    "vectorizerA = TfidfVectorizer()\n",
    "newsA_X_train = vectorizerA.fit_transform(newsA_train.data)\n",
    "newsA_X_test = vectorizerA.transform(newsA_test.data)\n",
    "\n",
    "vectorizerB = TfidfVectorizer()\n",
    "newsB_X_train = vectorizerB.fit_transform(newsB_train.data)\n",
    "newsB_X_test = vectorizerB.transform(newsB_test.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1181,)\n",
      "(1181, 39223)\n",
      "(1011,)\n",
      "(1011, 19825)\n",
      "(786, 39223)\n",
      "(786,)\n",
      "(674, 19825)\n",
      "(674,)\n"
     ]
    }
   ],
   "source": [
    "print(newsA_y_train.shape)\n",
    "print(newsA_X_train.shape)\n",
    "print(newsB_y_train.shape)\n",
    "print(newsB_X_train.shape)\n",
    "\n",
    "print(newsA_X_test.shape)\n",
    "print(newsA_y_test.shape)\n",
    "print(newsB_X_test.shape)\n",
    "print(newsB_y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1181,)\n",
      "(1011,)\n",
      "(1011,)\n"
     ]
    }
   ],
   "source": [
    "#print(np.argsort(newsA_y_train))\n",
    "orderA = np.argsort(newsA_y_train)\n",
    "newsA_X_train = newsA_X_train[orderA]\n",
    "newsA_y_train = newsA_y_train[orderA]\n",
    "print(newsA_y_train.shape)\n",
    "\n",
    "orderB = np.argsort(newsB_y_train)\n",
    "newsB_X_train = newsB_X_train[orderB]\n",
    "newsB_y_train = newsB_y_train[orderB]\n",
    "print(newsB_y_train.shape)\n",
    "\n",
    "# cut off the extras so both views same size\n",
    "newsA_X_train = newsA_X_train[:min(len(newsA_y_train),len(newsB_y_train))]\n",
    "newsA_y_train = newsA_y_train[:min(len(newsA_y_train),len(newsB_y_train))]\n",
    "\n",
    "print(newsA_y_train.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR', 'description'])\n"
     ]
    }
   ],
   "source": [
    "print((newsA_train.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1]\n",
      "(16, 39223)\n",
      "(995, 39223)\n"
     ]
    }
   ],
   "source": [
    "# split into labeled and unlabeled\n",
    "Lsize = 12\n",
    "\n",
    "np.random.seed(8)\n",
    "mask = np.random.choice([False, True], (newsA_X_train.shape[0]), p=[((newsA_X_train.shape[0]) - Lsize)/(newsA_X_train.shape[0]), Lsize/(newsA_X_train.shape[0])])\n",
    "\n",
    "\n",
    "newsA_X_train_L = newsA_X_train[mask,:]\n",
    "newsA_y_train_L = newsA_y_train[mask]\n",
    "newsA_X_train_U = newsA_X_train[mask==False,:]\n",
    "\n",
    "newsB_X_train_L = newsB_X_train[mask,:]\n",
    "newsB_y_train_L = newsB_y_train[mask]\n",
    "newsB_X_train_U = newsB_X_train[mask==False,:]\n",
    "\n",
    "print(newsA_y_train_L)\n",
    "print(newsA_X_train_L.shape)\n",
    "print(newsA_X_train_U.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import random\n",
    "class MVCoTrain:\n",
    "    \n",
    "    def __init__(self, model_type=\"Naive_Bayes\"):\n",
    "        if model_type == \"Naive_Bayes\":\n",
    "            self.h1 = GaussianNB()\n",
    "            self.h2 = GaussianNB()\n",
    "        else:\n",
    "            raise Exception(\"Bad model_type\")\n",
    "        self.model_type = model_type\n",
    "        \n",
    "    # return the most confident row indices for positive and negative\n",
    "    def pred_self(self, modelnum, Z_X_UU, n, p):\n",
    "        if modelnum==\"h1\":\n",
    "            probs = self.h1.predict_proba(Z_X_UU)\n",
    "            y_pred = self.h1.predict(Z_X_UU)\n",
    "        elif modelnum==\"h2\":\n",
    "            probs = self.h2.predict_proba(Z_X_UU)\n",
    "            y_pred = self.h2.predict(Z_X_UU)\n",
    "                    \n",
    "        probs_class0 = probs[:,0]\n",
    "        probs_class1 = probs[:,1]\n",
    "        topN_class0 = np.argsort(probs_class0)[-n:]\n",
    "        print(\"bad labels\")\n",
    "        print(y_pred[topN_class0]==1)\n",
    "        print(topN_class0)\n",
    "        topN_class0 = np.delete(topN_class0, y_pred[topN_class0]==1)\n",
    "        print(\"after delete\")\n",
    "        print(topN_class0)\n",
    "        topN_pred0 = y_pred[topN_class0]\n",
    "#         print(\" hhhh \")\n",
    "#         print(y_pred)\n",
    "#         print(probs)\n",
    "        \n",
    "#         print(\"here\")\n",
    "#         print(probs_class0[topN_class0])\n",
    "#         print(topN_pred0)\n",
    "        topP_class1 = np.argsort(probs_class1)[-p:]\n",
    "        print(\"bad labels P\")\n",
    "        print(y_pred[topP_class1]==0)\n",
    "        print(\"BEFORE DELETE\")\n",
    "        print(topP_class1)\n",
    "        topP_class1 = np.delete(topP_class1, y_pred[topP_class1]==0)\n",
    "        print(\"aFTER DELETE\")\n",
    "        print(topP_class1)\n",
    "        topP_pred1 = y_pred[topP_class1]\n",
    "        print([topN_class0])\n",
    "        print([topP_class1])\n",
    "#         print(probs_class1[topP_class1])\n",
    "#         print(topP_pred1)\n",
    "        best_locs = np.transpose(np.hstack([topN_class0, topP_class1]))\n",
    "        preds = np.transpose(np.hstack([topN_pred0, topP_pred1]))\n",
    "        if len(set(best_locs)) is not len(best_locs):\n",
    "            print(\"Error\")\n",
    "            raise Exception\n",
    "#         print(best_locs)\n",
    "#         print(preds)\n",
    "        return (best_locs, preds)\n",
    "\n",
    "    # replenish UU subset with examples from U set\n",
    "    def replenish(self, A_X_UU, A_X_U, B_X_UU, B_X_U, num_removed):\n",
    "        # not enough examples left to fully replenish and have leftover\n",
    "        if num_removed >= len(A_X_U):\n",
    "            A_X_UU = np.vstack((A_X_UU, A_X_U))\n",
    "            B_X_UU = np.vstack((B_X_UU, B_X_U))\n",
    "            return (A_X_UU, np.array([]), B_X_UU, np.array([]))\n",
    "        \n",
    "        # choose a random sample of the examples in the unlabeled set U\n",
    "        # to put into the UU subset\n",
    "        random.seed(11)\n",
    "        selector = np.arange(0,len(A_X_U))\n",
    "        random.shuffle(selector)\n",
    "        selector = selector[:num_removed]\n",
    "        print(selector)\n",
    "        A_X_UU = np.vstack((A_X_UU, A_X_U[selector,:]))\n",
    "        B_X_UU = np.vstack((B_X_UU, B_X_U[selector,:]))\n",
    "        # delete these from the unlabeled set U\n",
    "        A_X_U = np.delete(A_X_U, selector, axis=0)\n",
    "        B_X_U = np.delete(B_X_U, selector, axis=0)\n",
    "        return (A_X_UU, A_X_U, B_X_UU, B_X_U)\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    def fit_full(self, A_X_L, A_y_L, B_X_L, B_y_L, A_X_U, B_X_U, n=10, p=10, max_iters=20, seed=10):\n",
    "        # randomly get subgroup of the unlabeled data\n",
    "        UUsize = 4 * n + 4 * p\n",
    "        np.random.seed(seed)\n",
    "        mask = np.random.choice([False, True], (A_X_U.shape[0]), p=[((A_X_U.shape[0]) - UUsize)/(A_X_U.shape[0]), UUsize/(A_X_U.shape[0])])\n",
    "        notMask = np.invert(mask)\n",
    "#         print(notMask[:100])\n",
    "#         print(mask[:100])\n",
    "        notMask = np.flatnonzero(notMask)\n",
    "        mask = np.flatnonzero(mask)\n",
    "#         print(mask)\n",
    "#         print(len(mask))\n",
    "#         print(len(notMask))\n",
    "#         print(A_X_U.shape)\n",
    "        A_X_UU = A_X_U[mask,:]\n",
    "        B_X_UU = B_X_U[mask,:]\n",
    "        A_X_U = A_X_U[notMask,:]\n",
    "        B_X_U = B_X_U[notMask,:]\n",
    "#         print(A_X_U.shape)\n",
    "#         print(A_X_UU.shape)\n",
    "        \n",
    "        \n",
    "        A_X_L = A_X_L.toarray()\n",
    "        B_X_L = B_X_L.toarray()\n",
    "        A_X_U = A_X_U.toarray()\n",
    "        B_X_U = B_X_U.toarray()\n",
    "        A_X_UU = A_X_UU.toarray()\n",
    "        B_X_UU = B_X_UU.toarray()\n",
    "           \n",
    "        # iterate and fit\n",
    "        for n_iter in range(2):\n",
    "            print(\"iter \" + str(n_iter))\n",
    "            # fit h1 on the labeled data A_X_L\n",
    "            self.h1.fit(A_X_L, A_y_L)\n",
    "            \n",
    "            # fit h2 on labeled data B_X_L\n",
    "            self.h2.fit(B_X_L, B_y_L)\n",
    "            \n",
    "            # get h1 predictions of A_X_UU\n",
    "            best_rows1, best_preds1 = self.pred_self(\"h1\", A_X_UU, n, p)\n",
    "            print(best_rows1.shape)\n",
    "                    \n",
    "            # get the h2 predictions of B_X_UU\n",
    "            best_rows2, best_preds2 = self.pred_self(\"h2\", B_X_UU, n, p)\n",
    "            print(best_rows2.shape)\n",
    "            \n",
    "            # union these and put these best predictions into B_X_L, then replenish both UU\n",
    "            best_rows = np.array(list(set(np.hstack([best_rows1, best_rows2]))))\n",
    "            print(best_rows.shape)\n",
    "            num_removed = len(best_rows)\n",
    "            \n",
    "            y_L_new = np.zeros_like(best_rows)\n",
    "            remove_mask = np.zeros_like(best_rows)\n",
    "            for i,row in enumerate(best_rows):\n",
    "                remove_mask[i] = row\n",
    "                if row in best_rows1:\n",
    "                    print(\"best row\")\n",
    "                    print(row)\n",
    "                    print(best_rows1)\n",
    "                    print(np.where(best_rows1==row))\n",
    "                    y_L_new[i] = best_preds1[np.where(best_rows1==row)]\n",
    "                else:\n",
    "                    y_L_new[i] = best_preds2[np.where(best_rows2==row)]\n",
    "                    \n",
    "            \n",
    "            print(y_L_new)\n",
    "            \n",
    "            # add to labeled sets\n",
    "            print(A_X_L.shape)\n",
    "            print(A_X_UU[best_rows,:].shape)\n",
    "            \n",
    "            A_X_L = np.vstack((A_X_L, A_X_UU[best_rows,:]))\n",
    "            print(A_X_L.shape)\n",
    "            B_X_L = np.vstack((B_X_L, B_X_UU[best_rows,:]))\n",
    "            \n",
    "            # Add labels########################\n",
    "#             print(\"size\")\n",
    "#             print(y_L_new.shape)\n",
    "#             print(A_y_L.shape)\n",
    "#             print(B_y_L.shape)\n",
    "            A_y_L = np.transpose(np.hstack((A_y_L, y_L_new)))\n",
    "            B_y_L = np.transpose(np.hstack((B_y_L, y_L_new)))\n",
    "#             print(A_y_L.shape)\n",
    "#             print(B_y_L.shape)\n",
    "            \n",
    "            # remove from unlabeled set\n",
    "            mask = np.ones((A_X_UU.shape[0],))\n",
    "            mask[remove_mask] = 0\n",
    "            print(\"UU shape\")\n",
    "            print(A_X_UU.shape)\n",
    "            print(B_X_UU.shape)\n",
    "            A_X_UU = A_X_UU[mask==1,:]\n",
    "            B_X_UU = B_X_UU[mask==1,:]\n",
    "            print(A_X_UU.shape)\n",
    "            print(B_X_UU.shape)\n",
    "            \n",
    "            # replenish UU sets\n",
    "            print(\"num removed\")\n",
    "            print(num_removed)\n",
    "            print(\"before replenish\")\n",
    "            print(A_X_U.shape)\n",
    "            print(A_X_UU.shape)\n",
    "            (A_X_UU, A_X_U, B_X_UU, B_X_U) = self.replenish(A_X_UU, A_X_U, B_X_UU, B_X_U, num_removed)\n",
    "            print(\"shape after replenish\")\n",
    "            print(A_X_U.shape)\n",
    "            print(A_X_UU.shape)\n",
    "            \n",
    "            print(\"new shapes\")\n",
    "            print(A_X_L.shape)\n",
    "            print(A_y_L.shape)\n",
    "            print(B_X_L.shape)\n",
    "            print(B_y_L.shape)\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1,2,2,3,4,5,5,5,5,5,6])\n",
    "y = np.array(list(set(x)))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0\n",
      "bad labels\n",
      "[False False False False False False False False False False]\n",
      "[30  5 20 28 27 44 45 12 29 17]\n",
      "after delete\n",
      "[ 5 20 28 27 44 45 12 29 17]\n",
      "bad labels P\n",
      "[False False False False False False False False False False]\n",
      "BEFORE DELETE\n",
      "[26 31 33 34 35 71 37 38 19 72]\n",
      "aFTER DELETE\n",
      "[31 33 34 35 71 37 38 19 72]\n",
      "[array([ 5, 20, 28, 27, 44, 45, 12, 29, 17], dtype=int64)]\n",
      "[array([31, 33, 34, 35, 71, 37, 38, 19, 72], dtype=int64)]\n",
      "(18,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gavin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:30: FutureWarning: in the future insert will treat boolean arrays and array-likes as boolean index instead of casting it to integer\n",
      "C:\\Users\\gavin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:46: FutureWarning: in the future insert will treat boolean arrays and array-likes as boolean index instead of casting it to integer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad labels\n",
      "[False False False False False False False False False False]\n",
      "[17 20 22 27 31 32 38 45 53 72]\n",
      "after delete\n",
      "[20 22 27 31 32 38 45 53 72]\n",
      "bad labels P\n",
      "[False False False False False False False False False False]\n",
      "BEFORE DELETE\n",
      "[35 71 37 39 40 41 42 43  8 36]\n",
      "aFTER DELETE\n",
      "[71 37 39 40 41 42 43  8 36]\n",
      "[array([20, 22, 27, 31, 32, 38, 45, 53, 72], dtype=int64)]\n",
      "[array([71, 37, 39, 40, 41, 42, 43,  8, 36], dtype=int64)]\n",
      "(18,)\n",
      "(28,)\n",
      "best row\n",
      "5\n",
      "[ 5 20 28 27 44 45 12 29 17 31 33 34 35 71 37 38 19 72]\n",
      "(array([0], dtype=int64),)\n",
      "best row\n",
      "12\n",
      "[ 5 20 28 27 44 45 12 29 17 31 33 34 35 71 37 38 19 72]\n",
      "(array([6], dtype=int64),)\n",
      "best row\n",
      "17\n",
      "[ 5 20 28 27 44 45 12 29 17 31 33 34 35 71 37 38 19 72]\n",
      "(array([8], dtype=int64),)\n",
      "best row\n",
      "19\n",
      "[ 5 20 28 27 44 45 12 29 17 31 33 34 35 71 37 38 19 72]\n",
      "(array([16], dtype=int64),)\n",
      "best row\n",
      "20\n",
      "[ 5 20 28 27 44 45 12 29 17 31 33 34 35 71 37 38 19 72]\n",
      "(array([1], dtype=int64),)\n",
      "best row\n",
      "27\n",
      "[ 5 20 28 27 44 45 12 29 17 31 33 34 35 71 37 38 19 72]\n",
      "(array([3], dtype=int64),)\n",
      "best row\n",
      "28\n",
      "[ 5 20 28 27 44 45 12 29 17 31 33 34 35 71 37 38 19 72]\n",
      "(array([2], dtype=int64),)\n",
      "best row\n",
      "29\n",
      "[ 5 20 28 27 44 45 12 29 17 31 33 34 35 71 37 38 19 72]\n",
      "(array([7], dtype=int64),)\n",
      "best row\n",
      "31\n",
      "[ 5 20 28 27 44 45 12 29 17 31 33 34 35 71 37 38 19 72]\n",
      "(array([9], dtype=int64),)\n",
      "best row\n",
      "33\n",
      "[ 5 20 28 27 44 45 12 29 17 31 33 34 35 71 37 38 19 72]\n",
      "(array([10], dtype=int64),)\n",
      "best row\n",
      "34\n",
      "[ 5 20 28 27 44 45 12 29 17 31 33 34 35 71 37 38 19 72]\n",
      "(array([11], dtype=int64),)\n",
      "best row\n",
      "35\n",
      "[ 5 20 28 27 44 45 12 29 17 31 33 34 35 71 37 38 19 72]\n",
      "(array([12], dtype=int64),)\n",
      "best row\n",
      "37\n",
      "[ 5 20 28 27 44 45 12 29 17 31 33 34 35 71 37 38 19 72]\n",
      "(array([14], dtype=int64),)\n",
      "best row\n",
      "38\n",
      "[ 5 20 28 27 44 45 12 29 17 31 33 34 35 71 37 38 19 72]\n",
      "(array([15], dtype=int64),)\n",
      "best row\n",
      "44\n",
      "[ 5 20 28 27 44 45 12 29 17 31 33 34 35 71 37 38 19 72]\n",
      "(array([4], dtype=int64),)\n",
      "best row\n",
      "45\n",
      "[ 5 20 28 27 44 45 12 29 17 31 33 34 35 71 37 38 19 72]\n",
      "(array([5], dtype=int64),)\n",
      "best row\n",
      "71\n",
      "[ 5 20 28 27 44 45 12 29 17 31 33 34 35 71 37 38 19 72]\n",
      "(array([13], dtype=int64),)\n",
      "best row\n",
      "72\n",
      "[ 5 20 28 27 44 45 12 29 17 31 33 34 35 71 37 38 19 72]\n",
      "(array([17], dtype=int64),)\n",
      "[0 1 0 0 1 0 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1]\n",
      "(16, 39223)\n",
      "(28, 39223)\n",
      "(44, 39223)\n",
      "UU shape\n",
      "(73, 39223)\n",
      "(73, 19825)\n",
      "(45, 39223)\n",
      "(45, 19825)\n",
      "num removed\n",
      "28\n",
      "before replenish\n",
      "(922, 39223)\n",
      "(45, 39223)\n",
      "[581 306 361  51  83  57 608 668 449 279 280  78 337 188 227 571 142 645\n",
      " 747 550 166 453 528 411 549 435 292 368]\n",
      "shape after replenish\n",
      "(894, 39223)\n",
      "(73, 39223)\n",
      "new shapes\n",
      "(44, 39223)\n",
      "(44,)\n",
      "(44, 19825)\n",
      "(44,)\n",
      "iter 1\n",
      "bad labels\n",
      "[ True  True  True False False False False False False False]\n",
      "[26 27 28 37 17 58 20 54 59 53]\n",
      "after delete\n",
      "[28 37 17 58 20 54 59 53]\n",
      "bad labels P\n",
      "[False False False False False False False False False False]\n",
      "BEFORE DELETE\n",
      "[18 28 26 25 24 23 22 21 27 72]\n",
      "aFTER DELETE\n",
      "[28 26 25 24 23 22 21 27 72]\n",
      "[array([28, 37, 17, 58, 20, 54, 59, 53], dtype=int64)]\n",
      "[array([28, 26, 25, 24, 23, 22, 21, 27, 72], dtype=int64)]\n",
      "Error\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-162-8f368c8e1485>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mMVtest1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMVCoTrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mMVtest1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_full\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnewsA_X_train_L\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewsA_y_train_L\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewsB_X_train_L\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewsB_y_train_L\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewsA_X_train_U\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewsB_X_train_U\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-160-c76d13faa0b1>\u001b[0m in \u001b[0;36mfit_full\u001b[1;34m(self, A_X_L, A_y_L, B_X_L, B_y_L, A_X_U, B_X_U, n, p, max_iters, seed)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m             \u001b[1;31m# get h1 predictions of A_X_UU\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m             \u001b[0mbest_rows1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_preds1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpred_self\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"h1\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA_X_UU\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_rows1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-160-c76d13faa0b1>\u001b[0m in \u001b[0;36mpred_self\u001b[1;34m(self, modelnum, Z_X_UU, n, p)\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_locs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_locs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Error\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;31m#         print(best_locs)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;31m#         print(preds)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: "
     ]
    }
   ],
   "source": [
    "MVtest1 = MVCoTrain()\n",
    "MVtest1.fit_full(newsA_X_train_L, newsA_y_train_L, newsB_X_train_L, newsB_y_train_L, newsA_X_train_U, newsB_X_train_U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from https://github.com/jjrob13/sklearn_cotraining/blob/master/sklearn_cotraining/classifiers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "class CoTrainingClassifier(object):\n",
    "    \"\"\"\n",
    "    Organization from https://github.com/jjrob13/sklearn_cotraining\n",
    "    Algorithm based on \"Combining Labeled and Unlabeled Data with Co-Training\", Blum and Mitchell, 1998 \n",
    "    \n",
    "    Parameters:\n",
    "    clf - The classifier that will be used in the cotraining algorithm on the view 1 feature set\n",
    "        (If clf2 is not specified, then the same type of classifier will be used on the second view).\n",
    "\n",
    "    clf2 - (Optional) A different classifier type can be specified to be used on the X2 feature set\n",
    "         if desired.\n",
    "\n",
    "    p - (Optional) The number of positive examples that will be 'labeled' by each classifier during each iteration\n",
    "        The default is the is determined by the smallest integer ratio of positive to negative samples in L (from paper)\n",
    "\n",
    "    n - (Optional) The number of negative examples that will be 'labeled' by each classifier during each iteration\n",
    "    The default is the is determined by the smallest integer ratio of positive to negative samples in L (from paper)\n",
    "\n",
    "    k - (Optional) The number of iterations\n",
    "        The default is 30 (from paper)\n",
    "\n",
    "    u - (Optional) The size of the pool of unlabeled samples from which the classifier can choose\n",
    "    Default - 75 (from paper)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, clf, clf2=None, p=-1, n=-1, k=30, u=75):\n",
    "        \n",
    "        self.clf1_ = clf\n",
    "\n",
    "        if clf2 == None:\n",
    "            self.clf2_ = copy.copy(clf)\n",
    "        else:\n",
    "            self.clf2_ = clf2\n",
    "\n",
    "        #if user only specifies one of n or p, raise an exception\n",
    "        if (p == -1 and n != -1) or (p != -1 and n == -1):\n",
    "            raise ValueError('Must supply either both p and n, or neither')\n",
    "\n",
    "        self.p_ = p\n",
    "        self.n_ = n\n",
    "        self.k_ = k\n",
    "        self.u_ = u\n",
    "\n",
    "        random.seed(10)\n",
    "        \n",
    "        # for testing\n",
    "        self.partial_error_ = []\n",
    "\n",
    "\n",
    "    def fit(self, X1, X2, y, X1_test=None, X2_test=None, y_test=None):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        fits the classifiers on the partially labeled data, y.\n",
    "\n",
    "        Parameters:\n",
    "        X1 - array-like (n_samples, n_features_1): first set of features for samples\n",
    "        X2 - array-like (n_samples, n_features_2): second set of features for samples\n",
    "        y - array-like (n_samples): labels for samples, -1 indicates unlabeled\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # convert to numpy array\n",
    "        y = np.asarray(y)\n",
    "\n",
    "        #set the n and p parameters if we need to\n",
    "        if self.p_ == -1 and self.n_ == -1:\n",
    "            num_pos = sum(1 for y_i in y if y_i == 1)\n",
    "            num_neg = sum(1 for y_i in y if y_i == 0)\n",
    "\n",
    "            n_p_ratio = num_neg / float(num_pos)\n",
    "\n",
    "            if n_p_ratio > 1:\n",
    "                self.p_ = 1\n",
    "                self.n_ = round(self.p_*n_p_ratio)\n",
    "\n",
    "            else:\n",
    "                self.n_ = 1\n",
    "                self.p_ = round(self.n_/n_p_ratio)\n",
    "        print(self.n_)\n",
    "        print(self.p_)\n",
    "\n",
    "        assert(self.p_ > 0 and self.n_ > 0 and self.k_ > 0 and self.u_ > 0)\n",
    "\n",
    "        #the set of unlabeled samples\n",
    "        U = [i for i, y_i in enumerate(y) if y_i == -1]\n",
    "\n",
    "        #we randomize here, and then just take from the back so we don't have to sample every time\n",
    "        np.random.seed(10)\n",
    "        np.random.shuffle(U)\n",
    "        \n",
    "        #this is U' in paper\n",
    "        U_ = U[-min(len(U), self.u_):]\n",
    "        print(U_)\n",
    "\n",
    "        #the samples that are initially labeled\n",
    "        L = [i for i, y_i in enumerate(y) if y_i != -1]\n",
    "        print(L[:200])\n",
    "\n",
    "        #remove the samples in U_ from U\n",
    "        U = U[:-len(U_)]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        print(y_test[U_])\n",
    "\n",
    "\n",
    "        it = 0 #number of cotraining iterations we've done so far\n",
    "\n",
    "        #loop until we have assigned labels to everything in U or we hit our iteration break condition\n",
    "        while it != self.k_ and U:\n",
    "            it += 1\n",
    "\n",
    "            \n",
    "            self.clf1_.fit(X1[L], y[L])\n",
    "            self.clf2_.fit(X2[L], y[L])\n",
    "            y_test_new = y_test[U_]\n",
    "\n",
    "            y1_prob = self.clf1_.predict_log_proba(X1[U_])\n",
    "            y2_prob = self.clf2_.predict_log_proba(X2[U_])\n",
    "            \n",
    "            \n",
    "#             print(y1_prob)\n",
    "#             print(y2_prob)\n",
    "            \n",
    "            n, p = [], []\n",
    "            accurate_guesses_h1 = 0\n",
    "            accurate_guesses_h2 = 0\n",
    "            wrong_guesses_h1 = 0\n",
    "            wrong_guesses_h2 = 0\n",
    "            \n",
    "            #print([np.sort(y1_prob)[:5]])\n",
    "            for i in (y1_prob[:,0].argsort())[-self.n_:]:\n",
    "                #if y1_prob[i,0] > 0.5:\n",
    "                    n.append(i)\n",
    "                    if y_test_new[i] == 0:\n",
    "                        accurate_guesses_h1 += 1\n",
    "                        print(\"h1 correct class 0\")\n",
    "                    else:\n",
    "                        wrong_guesses_h1 += 1\n",
    "                        print(\"h1 guessed 0 actually \" + str(y_test_new[i]))\n",
    "                 \n",
    "            #print([(np.sort(y1_prob))[-5:]])\n",
    "            for i in (y1_prob[:,0].argsort())[:self.p_]:\n",
    "                #if y1_prob[i,1] > 0.5:\n",
    "                    p.append(i)\n",
    "                    if y_test_new[i] == 1:\n",
    "                        accurate_guesses_h1 += 1\n",
    "                        print(\"h1 correct class 1\")\n",
    "                    else:\n",
    "                        wrong_guesses_h1 += 1\n",
    "                        print(\"h1 guessed 1 actually \" + str(y_test_new[i]))\n",
    "\n",
    "            #print([(np.sort(y2_prob))[:5]])\n",
    "            for i in (y2_prob[:,0].argsort())[-self.n_:]:\n",
    "                #if y2_prob[i,0] > 0.5:\n",
    "                    n.append(i)\n",
    "                    if y_test_new[i] == 0:\n",
    "                        accurate_guesses_h2 += 1\n",
    "                        print(\"h2 correct class 0\")\n",
    "                    else:\n",
    "                        wrong_guesses_h2 += 1\n",
    "                        print(\"h2 guessed 0 actually \" + str(y_test_new[i]))\n",
    "                    \n",
    "            #print([(np.sort(y2_prob))[-5:]])\n",
    "            for i in (y2_prob[:,0].argsort())[:self.p_]:\n",
    "                if y2_prob[i,0] < 0.5:\n",
    "                    p.append(i)\n",
    "                    if y_test_new[i] == 1:\n",
    "                        accurate_guesses_h2 += 1\n",
    "                        print(\"h2 correct class 1\")\n",
    "                    else:\n",
    "                        wrong_guesses_h2 += 1\n",
    "                        print(\"h2 guessed 1 actually \" + str(y_test_new[i]))\n",
    "\n",
    "\n",
    "                        \n",
    "#             print(\"accurate guesses h1 \" + str(accurate_guesses_h1))\n",
    "#             print(\"wrong guesses h1\" + str(wrong_guesses_h1))\n",
    "#             print(\"accurate guesses h2 \" + str(accurate_guesses_h2))\n",
    "#             print(\"wrong guesses h2\" + str(wrong_guesses_h2))\n",
    "            \n",
    "\n",
    "            #label the samples and remove the newly added samples from U_\n",
    "            y[[U_[x] for x in p]] = 1\n",
    "            y[[U_[x] for x in n]] = 0\n",
    "\n",
    "            L.extend([U_[x] for x in p])\n",
    "            L.extend([U_[x] for x in n])\n",
    "\n",
    "            U_ = [elem for elem in U_ if not (elem in p or elem in n)]\n",
    "\n",
    "            #add new elements to U_\n",
    "            add_counter = 0 #number we have added from U to U_\n",
    "            num_to_add = len(p) + len(n)\n",
    "            while add_counter != num_to_add and U:\n",
    "                add_counter += 1\n",
    "                U_.append(U.pop())\n",
    "                \n",
    "            \n",
    "            # if input testing data as well, find the incrememtal update on accuracy\n",
    "            if X1_test is not None and X2_test is not None and y_test is not None:\n",
    "                y_pred = self.predict(X1_test, X2_test)\n",
    "                self.partial_error_.append(1-accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "            #TODO: Handle the case where the classifiers fail to agree on any of the samples (i.e. both n and p are empty)\n",
    "\n",
    "\n",
    "        #fit the final model\n",
    "        self.clf1_.fit(X1[L], y[L])\n",
    "        self.clf2_.fit(X2[L], y[L])\n",
    "        \n",
    "        return self.partial_error_\n",
    "\n",
    "\n",
    "    #TODO: Move this outside of the class into a util file.\n",
    "    def supports_proba(self, clf, x):\n",
    "        \"\"\"Checks if a given classifier supports the 'predict_proba' method, given a single vector x\"\"\"\n",
    "        try:\n",
    "            clf.predict_proba([x])\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def predict(self, X1, X2):\n",
    "        \"\"\"\n",
    "        Predict the classes of the samples represented by the features in X1 and X2.\n",
    "\n",
    "        Parameters:\n",
    "        X1 - array-like (n_samples, n_features1)\n",
    "        X2 - array-like (n_samples, n_features2)\n",
    "\n",
    "        \n",
    "        Output:\n",
    "        y - array-like (n_samples)\n",
    "            These are the predicted classes of each of the samples.  If the two classifiers, don't agree, we try\n",
    "            to use predict_proba and take the classifier with the highest confidence and if predict_proba is not implemented, then we randomly\n",
    "            assign either 0 or 1.  We hope to improve this in future releases.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        y1 = self.clf1_.predict(X1)\n",
    "        y2 = self.clf2_.predict(X2)\n",
    "\n",
    "        proba_supported = self.supports_proba(self.clf1_, X1[0]) and self.supports_proba(self.clf2_, X2[0])\n",
    "\n",
    "        #fill y_pred with -1 so we can identify the samples in which the classifiers failed to agree\n",
    "        y_pred = np.asarray([-1] * X1.shape[0])\n",
    "        num_disagree = 0\n",
    "        num_agree = 0\n",
    "\n",
    "        for i, (y1_i, y2_i) in enumerate(zip(y1, y2)):\n",
    "            if y1_i == y2_i:\n",
    "                y_pred[i] = y1_i\n",
    "                num_agree += 1\n",
    "            elif proba_supported:\n",
    "                y1_probs = self.clf1_.predict_proba([X1[i]])[0]\n",
    "                y2_probs = self.clf2_.predict_proba([X2[i]])[0]\n",
    "                sum_y_probs = [prob1 + prob2 for (prob1, prob2) in zip(y1_probs, y2_probs)]\n",
    "                max_sum_prob = max(sum_y_probs)\n",
    "                y_pred[i] = sum_y_probs.index(max_sum_prob)\n",
    "                num_disagree += 1\n",
    "            else:\n",
    "                #the classifiers disagree and don't support probability, so we guess\n",
    "                y_pred[i] = random.randint(0, 1)\n",
    "                \n",
    "        print(\"agree\")\n",
    "        print(num_agree)\n",
    "        print(\"disagree\")\n",
    "        print(num_disagree)\n",
    "\n",
    "\n",
    "        #check that we did everything right\n",
    "        assert not (-1 in y_pred)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "    def predict_proba(self, X1, X2):\n",
    "        \"\"\"Predict the probability of the samples belonging to each class.\"\"\"\n",
    "        y_proba = np.full((X1.shape[0], 2), -1)\n",
    "\n",
    "        y1_proba = self.clf1_.predict_proba(X1)\n",
    "        y2_proba = self.clf2_.predict_proba(X2)\n",
    "\n",
    "        for i, (y1_i_dist, y2_i_dist) in enumerate(zip(y1_proba, y2_proba)):\n",
    "            y_proba[i][0] = (y1_i_dist[0] + y2_i_dist[0]) / 2\n",
    "            y_proba[i][1] = (y1_i_dist[1] + y2_i_dist[1]) / 2\n",
    "\n",
    "        _epsilon = 0.0001\n",
    "        assert all(abs(sum(y_dist) - 1) <= _epsilon for y_dist in y_proba)\n",
    "        return y_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1181,)\n",
      "(1181, 39223)\n",
      "(1011,)\n",
      "(1011, 19825)\n",
      "(786, 39223)\n",
      "(786,)\n",
      "(674, 19825)\n",
      "(674,)\n"
     ]
    }
   ],
   "source": [
    "# Import the data\n",
    "setA = ['comp.os.ms-windows.misc','comp.sys.ibm.pc.hardware']\n",
    "setB = ['talk.politics.misc','talk.politics.guns']\n",
    "\n",
    "newsA_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'), categories=setA)\n",
    "newsB_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'), categories=setB)\n",
    "\n",
    "newsA_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'), categories=setA)\n",
    "newsB_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'), categories=setB)\n",
    "\n",
    "newsA_y_train = newsA_train.target\n",
    "newsB_y_train = newsB_train.target\n",
    "\n",
    "newsA_y_test = newsA_test.target\n",
    "newsB_y_test = newsB_test.target\n",
    "\n",
    "vectorizerA = TfidfVectorizer()\n",
    "newsA_X_train = vectorizerA.fit_transform(newsA_train.data)\n",
    "newsA_X_test = vectorizerA.transform(newsA_test.data)\n",
    "\n",
    "vectorizerB = TfidfVectorizer()\n",
    "newsB_X_train = vectorizerB.fit_transform(newsB_train.data)\n",
    "newsB_X_test = vectorizerB.transform(newsB_test.data)\n",
    "# newsB_X_train = vectorizerA.transform(newsB_train.data)\n",
    "# newsB_X_test = vectorizerA.transform(newsB_test.data)\n",
    "\n",
    "\n",
    "print(newsA_y_train.shape)\n",
    "print(newsA_X_train.shape)\n",
    "print(newsB_y_train.shape)\n",
    "print(newsB_X_train.shape)\n",
    "\n",
    "print(newsA_X_test.shape)\n",
    "print(newsA_y_test.shape)\n",
    "print(newsB_X_test.shape)\n",
    "print(newsB_y_test.shape)\n",
    "\n",
    "#make numpy arrays\n",
    "\n",
    "newsA_X_train = newsA_X_train.toarray()\n",
    "newsB_X_train = newsB_X_train.toarray()\n",
    "\n",
    "newsA_X_test = newsA_X_test.toarray()\n",
    "newsB_X_test = newsB_X_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 1 1 1 1 1 1]\n",
      "[0 0 0 0 0 1 1 1 1 1 1]\n",
      "465\n",
      "546\n",
      "(1009, 39223)\n",
      "(1009,)\n",
      "(1009, 19825)\n",
      "(1009,)\n",
      "310\n",
      "364\n",
      "(672, 39223)\n",
      "(672,)\n",
      "(672, 19825)\n",
      "(672,)\n",
      "[0 0 0 1 1 1 1 1 1 1 1]\n",
      "[0 0 0 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# make same size\n",
    "\n",
    "newsA_X_train_old = newsA_X_train.copy()\n",
    "\n",
    "order_train = np.argsort(newsA_y_train)\n",
    "newsA_y_train = newsA_y_train[order_train]\n",
    "newsA_X_train = newsA_X_train[order_train]\n",
    "\n",
    "order_train = np.argsort(newsB_y_train)\n",
    "newsB_y_train = newsB_y_train[order_train]\n",
    "newsB_X_train = newsB_X_train[order_train]\n",
    "\n",
    "\n",
    "class1len = min((len(np.argwhere(newsA_y_train==1)), len(np.argwhere(newsB_y_train==1))))\n",
    "class0len = min((len(np.argwhere(newsA_y_train==0)), len(np.argwhere(newsB_y_train==0))))\n",
    "\n",
    "\n",
    "newsA_y_train = np.concatenate((newsA_y_train[:class0len-1], newsA_y_train[-(class1len)+1:]))\n",
    "newsA_X_train = np.vstack((newsA_X_train[:class0len-1], newsA_X_train[-(class1len)+1:]))\n",
    "\n",
    "newsB_y_train = np.concatenate((newsB_y_train[:class0len-1], newsB_y_train[-(class1len)+1:]))\n",
    "newsB_X_train = np.vstack((newsB_X_train[:class0len-1], newsB_X_train[-(class1len)+1:]))\n",
    "\n",
    "print(newsA_y_train[np.arange(540,551)])\n",
    "print(newsB_y_train[np.arange(540,551)])\n",
    "\n",
    "print(class1len)\n",
    "print(class0len)\n",
    "\n",
    "print(newsA_X_train.shape)\n",
    "print(newsA_y_train.shape)\n",
    "print(newsB_X_train.shape)\n",
    "print(newsB_y_train.shape)\n",
    "\n",
    "# reshape test data\n",
    "order_test = np.argsort(newsA_y_test)\n",
    "newsA_y_test = newsA_y_test[order_test]\n",
    "newsA_X_test = newsA_X_test[order_test]\n",
    "\n",
    "order_test = np.argsort(newsB_y_test)\n",
    "newsB_y_test = newsB_y_test[order_test]\n",
    "newsB_X_test = newsB_X_test[order_test]\n",
    "\n",
    "\n",
    "class1len = min((len(np.argwhere(newsA_y_test==1)), len(np.argwhere(newsB_y_test==1))))\n",
    "class0len = min((len(np.argwhere(newsA_y_test==0)), len(np.argwhere(newsB_y_test==0))))\n",
    "\n",
    "\n",
    "newsA_y_test = np.concatenate((newsA_y_test[:class0len-1], newsA_y_test[-(class1len)+1:]))\n",
    "newsA_X_test = np.vstack((newsA_X_test[:class0len-1], newsA_X_test[-(class1len)+1:]))\n",
    "\n",
    "newsB_y_test = np.concatenate((newsB_y_test[:class0len-1], newsB_y_test[-(class1len)+1:]))\n",
    "newsB_X_test = np.vstack((newsB_X_test[:class0len-1], newsB_X_test[-(class1len)+1:]))\n",
    "\n",
    "print(class1len)\n",
    "print(class0len)\n",
    "\n",
    "print(newsA_X_test.shape)\n",
    "print(newsA_y_test.shape)\n",
    "print(newsB_X_test.shape)\n",
    "print(newsB_y_test.shape)\n",
    "\n",
    "\n",
    "print(newsA_y_test[np.arange(360,371)])\n",
    "print(newsB_y_test[np.arange(360,371)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1  1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "# randomly pick some labels to set to -1\n",
    "Lsize = 20\n",
    "labels = newsA_y_train.copy()\n",
    "minus1 = np.arange(0,len(labels))\n",
    "random.seed(11)\n",
    "random.shuffle(minus1)\n",
    "minus1 = minus1[:-Lsize]\n",
    "labels[minus1] = -1\n",
    "\n",
    "print(labels[-200:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "[728, 135, 433, 670, 949, 984, 71, 642, 469, 956, 15, 396, 219, 74, 681, 986, 773, 617, 938, 409, 974, 93, 539, 286, 819, 841, 552, 324, 665, 89, 683, 813, 345, 878, 666, 385, 412, 768, 13, 456, 77, 985, 704, 702, 887, 203, 554, 584, 122, 366, 347, 54, 398, 242, 359, 537, 374, 423, 509, 40, 497, 259, 73, 8, 900, 944, 503, 746, 156, 123, 372, 323, 536, 125, 268]\n",
      "[189, 190, 194, 462, 463, 476, 487, 520, 524, 573, 601, 628, 644, 799, 812, 823, 875, 877, 886, 946]\n",
      "[1 0 0 1 1 1 0 1 0 1 0 0 0 0 1 1 1 1 1 0 1 0 0 0 1 1 1 0 1 0 1 1 0 1 1 0 0\n",
      " 1 0 0 0 1 1 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0\n",
      " 0]\n",
      "h1 guessed 0 actually 0\n",
      "h1 correct class 1\n",
      "h2 correct class 0\n",
      "h2 correct class 1\n",
      "agree\n",
      "674\n",
      "disagree\n",
      "335\n",
      "h1 guessed 0 actually 0\n",
      "h1 correct class 1\n",
      "h2 correct class 0\n",
      "h2 correct class 1\n",
      "agree\n",
      "661\n",
      "disagree\n",
      "348\n",
      "h1 guessed 0 actually 0\n",
      "h1 correct class 1\n",
      "h2 correct class 0\n",
      "h2 correct class 1\n",
      "agree\n",
      "630\n",
      "disagree\n",
      "379\n",
      "h1 guessed 0 actually 0\n",
      "h1 correct class 1\n",
      "h2 guessed 0 actually 0\n",
      "h2 correct class 1\n",
      "agree\n",
      "586\n",
      "disagree\n",
      "423\n",
      "h1 correct class 0\n",
      "h1 correct class 1\n",
      "h2 guessed 0 actually 0\n",
      "h2 correct class 1\n",
      "agree\n",
      "586\n",
      "disagree\n",
      "423\n",
      "h1 guessed 0 actually 0\n",
      "h1 correct class 1\n",
      "h2 guessed 0 actually 0\n",
      "h2 correct class 1\n",
      "agree\n",
      "569\n",
      "disagree\n",
      "440\n",
      "h1 correct class 0\n",
      "h1 correct class 1\n",
      "h2 guessed 0 actually 0\n",
      "h2 correct class 1\n",
      "agree\n",
      "548\n",
      "disagree\n",
      "461\n",
      "h1 guessed 0 actually 0\n",
      "h1 correct class 1\n",
      "h2 guessed 0 actually 0\n",
      "h2 correct class 1\n",
      "agree\n",
      "540\n",
      "disagree\n",
      "469\n",
      "h1 correct class 0\n",
      "h1 correct class 1\n",
      "h2 correct class 0\n",
      "h2 correct class 1\n",
      "agree\n",
      "537\n",
      "disagree\n",
      "472\n",
      "h1 correct class 0\n",
      "h1 correct class 1\n",
      "h2 correct class 0\n",
      "h2 correct class 1\n",
      "agree\n",
      "518\n",
      "disagree\n",
      "491\n",
      "h1 correct class 0\n",
      "h1 correct class 1\n",
      "h2 guessed 0 actually 0\n",
      "h2 correct class 1\n",
      "agree\n",
      "508\n",
      "disagree\n",
      "501\n",
      "h1 correct class 0\n",
      "h1 correct class 1\n",
      "h2 guessed 0 actually 0\n",
      "h2 correct class 1\n",
      "agree\n",
      "496\n",
      "disagree\n",
      "513\n",
      "h1 correct class 0\n",
      "h1 correct class 1\n",
      "h2 correct class 0\n",
      "h2 correct class 1\n",
      "agree\n",
      "512\n",
      "disagree\n",
      "497\n",
      "h1 guessed 0 actually 0\n",
      "h1 correct class 1\n",
      "h2 correct class 0\n",
      "h2 correct class 1\n",
      "agree\n",
      "512\n",
      "disagree\n",
      "497\n",
      "h1 correct class 0\n",
      "h1 correct class 1\n",
      "h2 correct class 0\n",
      "h2 correct class 1\n",
      "agree\n",
      "510\n",
      "disagree\n",
      "499\n",
      "h1 guessed 0 actually 0\n",
      "h1 correct class 1\n",
      "h2 correct class 0\n",
      "h2 correct class 1\n",
      "agree\n",
      "510\n",
      "disagree\n",
      "499\n",
      "h1 correct class 0\n",
      "h1 correct class 1\n",
      "h2 guessed 0 actually 0\n",
      "h2 correct class 1\n",
      "agree\n",
      "550\n",
      "disagree\n",
      "459\n",
      "h1 guessed 0 actually 0\n",
      "h1 correct class 1\n",
      "h2 guessed 0 actually 0\n",
      "h2 correct class 1\n",
      "agree\n",
      "569\n",
      "disagree\n",
      "440\n",
      "h1 correct class 0\n",
      "h1 correct class 1\n",
      "h2 correct class 0\n",
      "h2 correct class 1\n",
      "agree\n",
      "577\n",
      "disagree\n",
      "432\n",
      "h1 correct class 0\n",
      "h1 correct class 1\n",
      "h2 correct class 0\n",
      "h2 correct class 1\n",
      "agree\n",
      "586\n",
      "disagree\n",
      "423\n",
      "h1 correct class 0\n",
      "h1 correct class 1\n",
      "h2 correct class 0\n",
      "h2 correct class 1\n",
      "agree\n",
      "612\n",
      "disagree\n",
      "397\n",
      "h1 guessed 0 actually 0\n",
      "h1 correct class 1\n",
      "h2 guessed 0 actually 0\n",
      "h2 correct class 1\n",
      "agree\n",
      "622\n",
      "disagree\n",
      "387\n",
      "h1 guessed 0 actually 0\n",
      "h1 correct class 1\n",
      "h2 guessed 0 actually 0\n",
      "h2 correct class 1\n",
      "agree\n",
      "636\n",
      "disagree\n",
      "373\n",
      "h1 guessed 0 actually 0\n",
      "h1 correct class 1\n",
      "h2 guessed 0 actually 0\n",
      "h2 correct class 1\n",
      "agree\n",
      "680\n",
      "disagree\n",
      "329\n",
      "h1 guessed 0 actually 0\n",
      "h1 correct class 1\n",
      "h2 guessed 0 actually 0\n",
      "h2 correct class 1\n",
      "agree\n",
      "692\n",
      "disagree\n",
      "317\n",
      "h1 correct class 0\n",
      "h1 correct class 1\n",
      "h2 correct class 0\n",
      "h2 correct class 1\n",
      "agree\n",
      "700\n",
      "disagree\n",
      "309\n",
      "h1 guessed 0 actually 0\n",
      "h1 correct class 1\n",
      "h2 guessed 0 actually 0\n",
      "h2 correct class 1\n",
      "agree\n",
      "702\n",
      "disagree\n",
      "307\n",
      "h1 guessed 0 actually 0\n",
      "h1 correct class 1\n",
      "h2 guessed 0 actually 0\n",
      "h2 correct class 1\n",
      "agree\n",
      "715\n",
      "disagree\n",
      "294\n",
      "h1 correct class 0\n",
      "h1 correct class 1\n",
      "h2 correct class 0\n",
      "h2 correct class 1\n",
      "agree\n",
      "727\n",
      "disagree\n",
      "282\n",
      "h1 correct class 0\n",
      "h1 correct class 1\n",
      "h2 guessed 0 actually 0\n",
      "h2 correct class 1\n",
      "agree\n",
      "731\n",
      "disagree\n",
      "278\n"
     ]
    }
   ],
   "source": [
    "# Fit the training data, get errors w.r.t. the training data\n",
    "gnb1 = GaussianNB()\n",
    "gnb2 = GaussianNB()\n",
    "clf = CoTrainingClassifier(gnb1, gnb2, k=30)\n",
    "errors = clf.fit(newsA_X_train, newsB_X_train, labels, newsA_X_train, newsB_X_train, newsA_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GaussianNB' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-886754ebe5ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Fit the training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mgnb1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mgnb2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCoTrainingClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgnb1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgnb2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnewsA_X_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewsB_X_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewsA_X_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewsB_X_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewsA_y_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'GaussianNB' is not defined"
     ]
    }
   ],
   "source": [
    "# Fit the training data\n",
    "gnb1 = GaussianNB()\n",
    "gnb2 = GaussianNB()\n",
    "clf = CoTrainingClassifier(gnb1, gnb2, k=10)\n",
    "errors = clf.fit(newsA_X_train, newsB_X_train, labels, newsA_X_test, newsB_X_test, newsA_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agree\n",
      "362\n",
      "disagree\n",
      "310\n",
      "0.6875\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "y_pred = clf.predict(newsA_X_test, newsB_X_test)\n",
    "print(accuracy_score(newsA_y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSXH/H6R3tg+qMVRsAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x146ddb60b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(100*np.array(errors))\n",
    "plt.xlabel(\"Co-training iterations\")\n",
    "plt.ylabel(\"Classification Error (%)\")\n",
    "plt.savefig(\"Error Decrease on Training diffVectorizor 30.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:\n",
      "0.722772277228\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "(672, 39223)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(newsA_X_train, newsA_y_train, test_size=0.20, random_state=2)\n",
    "hh = GaussianNB()\n",
    "hh.fit(X_train, y_train)\n",
    "y_pred = hh.predict(X_test)\n",
    "print(\"accuracy score:\")\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(newsA_y_test[-100:])\n",
    "print(newsA_X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Diabetes Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>6</th>\n",
       "      <th>148</th>\n",
       "      <th>72</th>\n",
       "      <th>35</th>\n",
       "      <th>0</th>\n",
       "      <th>33.6</th>\n",
       "      <th>0.627</th>\n",
       "      <th>50</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>116</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.6</td>\n",
       "      <td>0.201</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   6  148  72  35    0  33.6  0.627  50  1\n",
       "0  1   85  66  29    0  26.6  0.351  31  0\n",
       "1  8  183  64   0    0  23.3  0.672  32  1\n",
       "2  1   89  66  23   94  28.1  0.167  21  0\n",
       "3  0  137  40  35  168  43.1  2.288  33  1\n",
       "4  5  116  74   0    0  25.6  0.201  30  0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data = pd.read_csv(\"Diabetes_Dataset.csv\")\n",
    "full_labels = full_data['1']\n",
    "full_data.head()\n",
    "full_data = full_data.drop('1',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>6</th>\n",
       "      <th>148</th>\n",
       "      <th>72</th>\n",
       "      <th>35</th>\n",
       "      <th>0</th>\n",
       "      <th>33.6</th>\n",
       "      <th>0.627</th>\n",
       "      <th>50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>116</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.6</td>\n",
       "      <td>0.201</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   6  148  72  35    0  33.6  0.627  50\n",
       "0  1   85  66  29    0  26.6  0.351  31\n",
       "1  8  183  64   0    0  23.3  0.672  32\n",
       "2  1   89  66  23   94  28.1  0.167  21\n",
       "3  0  137  40  35  168  43.1  2.288  33\n",
       "4  5  116  74   0    0  25.6  0.201  30"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = full_data.as_matrix()\n",
    "Y = full_labels.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(767, 8)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split into two groups (\"views\")\n",
    "Lsize = 20\n",
    "X_train_L = X_train[:Lsize,:]\n",
    "X_train_U = X_train[Lsize:,:]\n",
    "y_train_L = y_train[:Lsize]\n",
    "y_train_U = np.zeros_like(y_train[Lsize:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 8)\n",
      "(670, 8)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_L.shape)\n",
    "print(X_train_U.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy iter 1 = \n",
      "0.675324675325\n",
      "(60,)\n",
      "(60,)\n",
      "Accuracy iter 2 = \n",
      "0.701298701299\n",
      "(60,)\n",
      "(60,)\n",
      "(553, 8)\n",
      "(140, 8)\n",
      "(140,)\n",
      "Accuracy iter 3 = \n",
      "0.753246753247\n",
      "(60,)\n",
      "(60,)\n",
      "(496, 8)\n",
      "(200, 8)\n",
      "(200,)\n",
      "Accuracy iter 4 = \n",
      "0.727272727273\n",
      "(60,)\n",
      "(60,)\n",
      "(441, 8)\n",
      "(260, 8)\n",
      "(260,)\n",
      "Accuracy iter 5 = \n",
      "0.701298701299\n",
      "(60,)\n",
      "(60,)\n",
      "(383, 8)\n",
      "(320, 8)\n",
      "(320,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# test fitting to a single view\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train_L, y_train_L)\n",
    "probs = gnb.predict_proba(X_train_U)\n",
    "y_pred = gnb.predict(X_train_U)\n",
    "probs_class0 = probs[y_pred==0][:,0]\n",
    "probs_class1 = probs[y_pred==1][:,1]\n",
    "print(\"Accuracy iter 1 = \")\n",
    "print(accuracy_score(y_test,gnb.predict(X_test)))\n",
    "n = 30\n",
    "new_labels = np.transpose(np.hstack([np.zeros(n,),np.ones(n,)]))\n",
    "print(new_labels.shape)\n",
    "topN_class0 = np.argsort(probs_class0)[-n:]\n",
    "topN_class1 = np.argsort(probs_class1)[-n:]\n",
    "top2N_class = np.transpose(np.hstack([topN_class0, topN_class1]))\n",
    "print(top2N_class.shape)\n",
    "# add these to the labeled set\n",
    "# print(X_train_L.shape)\n",
    "# print(X_train_U[topN_class0,:].shape)\n",
    "# print(X_train_U[topN_class1,:].shape)\n",
    "X_train_L = np.vstack([X_train_L,X_train_U[topN_class0,:],X_train_U[topN_class1,:]])\n",
    "\n",
    "y_train_L = np.hstack([y_train_L,new_labels])\n",
    "\n",
    "X_train_U = np.delete(X_train_U,top2N_class.T,0)\n",
    "# print(X_train_U.shape)\n",
    "\n",
    "# Iter 2\n",
    "\n",
    "gnb.fit(X_train_L, y_train_L)\n",
    "probs = gnb.predict_proba(X_train_U)\n",
    "y_pred = gnb.predict(X_train_U)\n",
    "probs_class0 = probs[y_pred==0][:,0]\n",
    "probs_class1 = probs[y_pred==1][:,1]\n",
    "print(\"Accuracy iter 2 = \")\n",
    "print(accuracy_score(y_test,gnb.predict(X_test)))\n",
    "n = 30\n",
    "new_labels = np.transpose(np.hstack([np.zeros(n,),np.ones(n,)]))\n",
    "print(new_labels.shape)\n",
    "topN_class0 = np.argsort(probs_class0)[-n:]\n",
    "topN_class1 = np.argsort(probs_class1)[-n:]\n",
    "top2N_class = np.transpose(np.hstack([topN_class0, topN_class1]))\n",
    "print(top2N_class.shape)\n",
    "# add these to the labeled set\n",
    "# print(X_train_L.shape)\n",
    "# print(X_train_U[topN_class0,:].shape)\n",
    "# print(X_train_U[topN_class1,:].shape)\n",
    "X_train_L = np.vstack([X_train_L,X_train_U[topN_class0,:],X_train_U[topN_class1,:]])\n",
    "\n",
    "y_train_L = np.hstack([y_train_L,new_labels])\n",
    "\n",
    "X_train_U = np.delete(X_train_U,top2N_class.T,0)\n",
    "print(X_train_U.shape)\n",
    "print(X_train_L.shape)\n",
    "print(y_train_L.shape)\n",
    "\n",
    "# Iter 3\n",
    "\n",
    "gnb.fit(X_train_L, y_train_L)\n",
    "probs = gnb.predict_proba(X_train_U)\n",
    "y_pred = gnb.predict(X_train_U)\n",
    "probs_class0 = probs[y_pred==0][:,0]\n",
    "probs_class1 = probs[y_pred==1][:,1]\n",
    "print(\"Accuracy iter 3 = \")\n",
    "print(accuracy_score(y_test,gnb.predict(X_test)))\n",
    "n = 30\n",
    "new_labels = np.transpose(np.hstack([np.zeros(n,),np.ones(n,)]))\n",
    "print(new_labels.shape)\n",
    "topN_class0 = np.argsort(probs_class0)[-n:]\n",
    "topN_class1 = np.argsort(probs_class1)[-n:]\n",
    "top2N_class = np.transpose(np.hstack([topN_class0, topN_class1]))\n",
    "print(top2N_class.shape)\n",
    "# add these to the labeled set\n",
    "# print(X_train_L.shape)\n",
    "# print(X_train_U[topN_class0,:].shape)\n",
    "# print(X_train_U[topN_class1,:].shape)\n",
    "X_train_L = np.vstack([X_train_L,X_train_U[topN_class0,:],X_train_U[topN_class1,:]])\n",
    "\n",
    "y_train_L = np.hstack([y_train_L,new_labels])\n",
    "\n",
    "X_train_U = np.delete(X_train_U,top2N_class.T,0)\n",
    "print(X_train_U.shape)\n",
    "print(X_train_L.shape)\n",
    "print(y_train_L.shape)\n",
    "\n",
    "# Iter 4\n",
    "\n",
    "gnb.fit(X_train_L, y_train_L)\n",
    "probs = gnb.predict_proba(X_train_U)\n",
    "y_pred = gnb.predict(X_train_U)\n",
    "probs_class0 = probs[y_pred==0][:,0]\n",
    "probs_class1 = probs[y_pred==1][:,1]\n",
    "print(\"Accuracy iter 4 = \")\n",
    "print(accuracy_score(y_test,gnb.predict(X_test)))\n",
    "n = 30\n",
    "new_labels = np.transpose(np.hstack([np.zeros(n,),np.ones(n,)]))\n",
    "print(new_labels.shape)\n",
    "topN_class0 = np.argsort(probs_class0)[-n:]\n",
    "topN_class1 = np.argsort(probs_class1)[-n:]\n",
    "top2N_class = np.transpose(np.hstack([topN_class0, topN_class1]))\n",
    "print(top2N_class.shape)\n",
    "# add these to the labeled set\n",
    "# print(X_train_L.shape)\n",
    "# print(X_train_U[topN_class0,:].shape)\n",
    "# print(X_train_U[topN_class1,:].shape)\n",
    "X_train_L = np.vstack([X_train_L,X_train_U[topN_class0,:],X_train_U[topN_class1,:]])\n",
    "\n",
    "y_train_L = np.hstack([y_train_L,new_labels])\n",
    "\n",
    "X_train_U = np.delete(X_train_U,top2N_class.T,0)\n",
    "print(X_train_U.shape)\n",
    "print(X_train_L.shape)\n",
    "print(y_train_L.shape)\n",
    "\n",
    "# Iter 5\n",
    "\n",
    "gnb.fit(X_train_L, y_train_L)\n",
    "probs = gnb.predict_proba(X_train_U)\n",
    "y_pred = gnb.predict(X_train_U)\n",
    "probs_class0 = probs[y_pred==0][:,0]\n",
    "probs_class1 = probs[y_pred==1][:,1]\n",
    "print(\"Accuracy iter 5 = \")\n",
    "print(accuracy_score(y_test,gnb.predict(X_test)))\n",
    "n = 30\n",
    "new_labels = np.transpose(np.hstack([np.zeros(n,),np.ones(n,)]))\n",
    "print(new_labels.shape)\n",
    "topN_class0 = np.argsort(probs_class0)[-n:]\n",
    "topN_class1 = np.argsort(probs_class1)[-n:]\n",
    "top2N_class = np.transpose(np.hstack([topN_class0, topN_class1]))\n",
    "print(top2N_class.shape)\n",
    "# add these to the labeled set\n",
    "# print(X_train_L.shape)\n",
    "# print(X_train_U[topN_class0,:].shape)\n",
    "# print(X_train_U[topN_class1,:].shape)\n",
    "X_train_L = np.vstack([X_train_L,X_train_U[topN_class0,:],X_train_U[topN_class1,:]])\n",
    "\n",
    "y_train_L = np.hstack([y_train_L,new_labels])\n",
    "\n",
    "X_train_U = np.delete(X_train_U,top2N_class.T,0)\n",
    "print(X_train_U.shape)\n",
    "print(X_train_L.shape)\n",
    "print(y_train_L.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "class CoTrainSingleView:\n",
    "    def __init__(self):\n",
    "        gnb1 = GaussianNB()\n",
    "        \n",
    "    def fit_full(self,X_L,y_L,X_U):\n",
    "        # fit on the labeled data\n",
    "        gnb1.fit(X_L,y_L)\n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1]\n",
      " [2 2 2 2]\n",
      " [3 3 3 3]\n",
      " [4 4 4 4]\n",
      " [5 5 5 5]]\n",
      "[1 2 3 4 5]\n",
      "[[3 3 3 3]\n",
      " [4 4 4 4]\n",
      " [1 1 1 1]\n",
      " [5 5 5 5]\n",
      " [2 2 2 2]]\n",
      "[3 4 1 5 2]\n"
     ]
    }
   ],
   "source": [
    "xxx = np.array([[1, 1, 1, 1],[2, 2, 2, 2],[3, 3, 3, 3],[4, 4, 4, 4],[5, 5, 5, 5]])\n",
    "yyy = np.array([1,2,3,4,5])\n",
    "print(xxx)\n",
    "print(yyy)\n",
    "np.random.seed(10)\n",
    "np.random.shuffle(xxx)\n",
    "np.random.seed(10)\n",
    "np.random.shuffle(yyy)\n",
    "print(xxx)\n",
    "print(yyy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
