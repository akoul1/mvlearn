# Multi-View Clustering
### Steffen Bickel & Tobias Scheffer, 2004
link: https://www.cs.uni-potsdam.de/ml/publications/icdm-2004.pdf

## 1 Introduction
Wish to study whether co-training methods can be applied to clustering algorithms. Will discuss partitioning multiview clustering algorithms and hierarchical algorithms.

## 2 Related Work
- Co-EM algorithm is a MV version of the EM algorithm for semi-supervised learning.
Clustering algorithms can be divided into two categories:
1. Generative (model-based) approaches
	- Learn generative models from data, with each model representing one cluster. Often based on EM algorithm. 
2. Discriminative (similarity-based) approaches
	- Optimize an objective function that involves maximizing the average similarities within clusters and minimizing the average similarities between clusters. Most follow hierarchical agglomerative approach, where dendrogram is built up by iteratively merging closest examples/clusters.

## 3 Problem Setting and Evaluation
Assume data is generated by a mixture model. Want to estimate parameters of each component and thereby cluster. In a MV setting, attributes V of examples are split into V1 and V2. x = (x1,x2), where x1 is a vector of attributes in V1, and x2 is in V2. 

Quality of clustering is measured using average entropy over all clusters. 

## 4 Multi-View EM Clustering
### 4.1 General Multi-View EM Algorithm
In each iteration *i*, each view *v* finds the model parameters Theta_i(v), which maximizes the likelihood given the expected values for the hidden variables of the other view. In turns, M, E steps in one view and M, E steps in view 2 are executed. The single E and M steps are equivalent to the E and M steps of the original EM algorithm. This often does not converge. Stop after a special stopping criterion.

** Algorithm**

Input: Unlabeled data D = {(x_1(1),x_1(2)), ... , (x_n(1),x_n(2))}
1. Initialize Theta_0(2), T, t=0
2. E step view 2: Compute expectation for hidden variables given model parameters Theta_0(2)
3. Do until stopping criterion is met:
	* For *v* = 1...2:
		i. t = t+1
		ii. M step view *v*: Find model parameters Theta_t(*v*) that maximize the likelihood of the data given the expected values for the hidden variables of view *v* of iteration t-1
		iii. E step view *v-bar*: Compute expectation for hidden variables given the model parameters Theta_t(*v*)
	* End for *v*
4. Return combined Theta = Union{Theta_t-1(1), Theta_t(2)}

### 4.2 Mixture of Multinomials EM Algorithm
Used to model generating a document of length *n* from mixture component *j* because this can be viewed as drawing *n* words at random from a dictionary with replacement. Each word has an individual probability. So, probability of seeing a given word in the document is governed by a multinomial distribution. So overall document is a mixture of multinomials. Once maximized the log-likelihood of the data, final assignment is just the sub-distribution that has the highest conditional probability of the data coming from it given the parameters.

### 4.3 Multi-View Spherical k-Means
Normalize each example vector to unit length. Spherical k-Means deals with this type of normalized vector, which is the regular k-Means algorithm with cosine similarity as distance measure. 
The parameter Theta_v consists of the concept vectors c_j(*v*); j = 1,...,*k*; v = 1,2; that have unit length. *k* is the desired number of clusters. All example vectors are also unit length. Randomly initialize concept vectors c_j(2).
1. E step assigns the samples that are closest to its concept vector c_j(*v*) to the corresponding partition pi_j(*v*).
2. M step computes new concept vectors by averaging the concept vectors in a partition. 
After the E and M steps in one view, the partitions pi_j(*v*) get interchanged for an E and M step in the other view.
After each iteration, compute the objective function for each view. End the iterations if the objective function did not reach a new minimum for a fixed number of iterations in each view.
After ending, the cluster partitions pi_j(1) and pi_j(2) don't necessarily contain the same examples. To get a combined clustering result, assign each example to one distinct cluster that is determined through the closest concept vector. To do this, compute a consensus mean for each cluster and view. Only those examples are included that both views agree on. Assign each example to the final cluster that provides the most similar consensus vector, determined by averaging over the arcus cosine values in both views.

## 5 Multi-View Agglomerative Clustering
Based on iteratively merging nearest clusters. Natural extension to MV setting is splitting up the iterative merging procedure so that one iteration executes one merging step in one view and the next iteration step in the other view and so on.

### 5.1 Algorithm
Inspired by co-training. Co-training greedily augments the training set with the *p* positive and *n* negative highest confidence examples from the unlabeled data in each iteration. 
Agglomerative clustering is based on distance measure between clusters. In MV setting, two attribute sets V(1), V(2), and two distance measures d(1)(C_i,C_j) and d(2)(C_i,C_j). Start with each example having its own cluster. Iteratively merge the closest cluster to build up dendrogram. In MV, algorithm merges in turns the closest clusters in view 1 and view 2. All merges work on a combined dendrogram across views.

**Algorithm**

Input: Unlabeled data D = {(x_1(1),x_1(2)), ... , (x_n(1),x_n(2))}, distance measures d(1)(C_i,C_j) and d(2)(C_i,C_j)
1. Initialize C_i = x_i, i = 1...n
2. For t = 1...n: For v = 1...2
	- Find pair of closest clusters (C_i,C_j) = argmin d(*v*)(C_i,C_j) for i,j = 1...(n-t+1).
	- Merge C_i and C_j
3. Return dendrogram

This algorithm assumes low dependence between the views to get better quality clustering than on concatenated views. One view can benefit from a high confidence decision made in the other view. As a distance measure, this paper uses cosine similarity.

**NOTE** MV agglomerative techniques often result in higher entropy clusters than simply concatenating the multiple views and clustering. 
