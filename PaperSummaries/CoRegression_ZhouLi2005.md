# Semi-Supervised Regression with Co-Training
### Zhi-Hua Zhou & Ming Li, 2005
link: https://www.ijcai.org/Proceedings/05/Papers/0689.pdf

## 1 Introduction
Propose a co-training style semi-supervised regression algorithm, COREG. Uses two kNN regressors, each of which labels the unlabeled data for the other during the learning process. Final prediction is the average of the regression estimates generated by both regressors. Utilizes different distance metrics instead of requiring sufficient and redundant views, so more broadly applicable. 

## 2 COREG
Let L denote the labeled data set, where example x_i has label y_i (real-valued), and |L| is the number of labeled examples. Let U be the unlabeled set.

Two regressors, h1 and h2 are generated from L, each of which is refined with the help of unlabeled examples that are labeled by the latest version of the other regressor. 

Here, the kNN regressor is used as the base learner. This reduces the computational load by not needing to completely retrain each regressor at each iteration. 

Note: initial regressors should be diverse, because if they are identical, then each label generated by one may be the same as that generated by the other, which leads to self-training with a single learner. For MV learning, the use of different views prevents this. Here, diversity is achieved through using different distance metrics. Minkowsky distance is altered by selecting different value for parameter *p*, making it more or less sensitive to small data variations.

COREG contains a mechanism for estimating the labeling confidence as well, which is useful for deciding which labeled examples to give to the other regressor. The most confidently labeled example should have this property: the error of the regressor on the labeled example set should decrease the most if the most confidently labeled eample is utilized. i.e. the most confidently labeled example should be the one which makes the regressor most consistent with the labeled example set.
- First compute MSE of the regressor on the labeled set. Then, recompute utilizing the information provided by the (x_u, y_u). Delta_u is the result of subtracting these MSE calculations. The number of Delta_u's to calculate is the number of unlabeled samples. Then, can order them in terms of Delta_u.
- However, this is time consuming. So, COREG uses an approximation since kNN is assumed to mainly utilize local info. For each x_u, the most confidently labeled example is chosen through maximizing the value of Delta_x_u in equation in paper (Eq 2).

**COREG Algorithm**

Input: Labeled examples L, unlabeled set U, number of nearest neighbors k, maximum iterations T, distance order p1, p2

Process:
- L1 <- L; L2 <- L
- Create pool U' by randomly picking examples from U
- h1 <- kNN(L1,k,p1); h2 <- kNN(L2,k,p2)
- Repeat for T rounds:
	* for j in {1,2}:
		* for each x_u in U':
			* y_hat_u <- hj(x_u)
			* Omega <- Neighbors(x_u,k,Lj)
			* h'j <- kNN(Lj union {(x_u,y_hat_u)},k,pj)
			* Delta_x_u maximize formula
		* if there exists Delta_x_u > 0, then best x_j <- argmax(Delta_x_u); best y_j <- hj(best x_j)
			* pi_j <- {(best x_j, best y_j)}; U' <- U' - pi_j
		* else 
			* pi_j <- empty set
	* L1 <- L1 union pi_2; L2 <- L2 union pi_1
	* if neither L1 nor L2 changes, then exit
	* else
		* h1 <- kNN(L1,k,p1); h2 <- kNN(L2,k,p2)
		* Replenish U' by randomly picking examples from U

**Output:** regressor h(x) <- 0.5(h1(x) + h2(x))

**Overall** Co-regression can improve regression over other methods, such as *Self* and *ARTRE*. 
